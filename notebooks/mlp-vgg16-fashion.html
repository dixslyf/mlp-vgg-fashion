<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Dixon Sean Low Yan Feng">
<meta name="dcterms.date" content="2024-10-20">

<title>A Comparison of Multilayer Perceptron and VGG-16 Models for Fashion Image Classification</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-37eea08aefeeee20ff55810ff984fec1.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dark-2fef5ea3f8957b3e4ecc936fc74692ca.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-37eea08aefeeee20ff55810ff984fec1.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-fc06ccf06a818f1a3a7dcfd5dc6ce88f.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../site_libs/bootstrap/bootstrap-dark-82ad1e1dd44acef22b025fe37c206c8f.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="../site_libs/bootstrap/bootstrap-fc06ccf06a818f1a3a7dcfd5dc6ce88f.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>html{ scroll-behavior: smooth; }</style>


</head>

<body class="quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const queryPrefersDark = window.matchMedia('(prefers-color-scheme: dark)');
    const darkModeDefault = queryPrefersDark.matches;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    queryPrefersDark.addEventListener("change", e => {
      if(window.localStorage.getItem("quarto-color-scheme") !== null)
        return;
      const alternate = e.matches
      toggleColorMode(alternate);
      localAlternateSentinel = e.matches ? 'alternate' : 'default'; // this is used alongside local storage!
      toggleGiscusIfUsed(alternate, darkModeDefault);
    });
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction">Introduction</a></li>
  <li><a href="#preamble" id="toc-preamble" class="nav-link" data-scroll-target="#preamble">Preamble</a></li>
  <li><a href="#data" id="toc-data" class="nav-link" data-scroll-target="#data">Data</a>
  <ul>
  <li><a href="#downloading-the-dataset" id="toc-downloading-the-dataset" class="nav-link" data-scroll-target="#downloading-the-dataset">Downloading the Dataset</a></li>
  <li><a href="#custom-dataset" id="toc-custom-dataset" class="nav-link" data-scroll-target="#custom-dataset">Custom <code>Dataset</code></a></li>
  <li><a href="#preliminary-analysis" id="toc-preliminary-analysis" class="nav-link" data-scroll-target="#preliminary-analysis">Preliminary Analysis</a></li>
  <li><a href="#preprocessing" id="toc-preprocessing" class="nav-link" data-scroll-target="#preprocessing">Preprocessing</a>
  <ul>
  <li><a href="#normalisation-and-label-encoding" id="toc-normalisation-and-label-encoding" class="nav-link" data-scroll-target="#normalisation-and-label-encoding">Normalisation and Label Encoding</a></li>
  <li><a href="#splitting-the-dataset" id="toc-splitting-the-dataset" class="nav-link" data-scroll-target="#splitting-the-dataset">Splitting the Dataset</a></li>
  <li><a href="#undersampling" id="toc-undersampling" class="nav-link" data-scroll-target="#undersampling">Undersampling</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#model-definitions" id="toc-model-definitions" class="nav-link" data-scroll-target="#model-definitions">Model Definitions</a>
  <ul>
  <li><a href="#multilayer-perceptron" id="toc-multilayer-perceptron" class="nav-link" data-scroll-target="#multilayer-perceptron">Multilayer Perceptron</a></li>
  <li><a href="#vgg-16" id="toc-vgg-16" class="nav-link" data-scroll-target="#vgg-16">VGG-16</a></li>
  <li><a href="#vgg-16-pretrained" id="toc-vgg-16-pretrained" class="nav-link" data-scroll-target="#vgg-16-pretrained">VGG-16 Pretrained</a></li>
  </ul></li>
  <li><a href="#definitions-for-training-and-evaluation" id="toc-definitions-for-training-and-evaluation" class="nav-link" data-scroll-target="#definitions-for-training-and-evaluation">Definitions for Training and Evaluation</a></li>
  <li><a href="#hyperparameter-tuning" id="toc-hyperparameter-tuning" class="nav-link" data-scroll-target="#hyperparameter-tuning">Hyperparameter Tuning</a>
  <ul>
  <li><a href="#multilayer-perceptron-1" id="toc-multilayer-perceptron-1" class="nav-link" data-scroll-target="#multilayer-perceptron-1">Multilayer Perceptron</a></li>
  <li><a href="#vgg-16-1" id="toc-vgg-16-1" class="nav-link" data-scroll-target="#vgg-16-1">VGG-16</a></li>
  <li><a href="#vgg-16-pretrained-1" id="toc-vgg-16-pretrained-1" class="nav-link" data-scroll-target="#vgg-16-pretrained-1">VGG-16 Pretrained</a></li>
  </ul></li>
  <li><a href="#training-and-evaluation" id="toc-training-and-evaluation" class="nav-link" data-scroll-target="#training-and-evaluation">Training and Evaluation</a>
  <ul>
  <li><a href="#multilayer-perceptron-2" id="toc-multilayer-perceptron-2" class="nav-link" data-scroll-target="#multilayer-perceptron-2">Multilayer Perceptron</a></li>
  <li><a href="#vgg-16-2" id="toc-vgg-16-2" class="nav-link" data-scroll-target="#vgg-16-2">VGG-16</a></li>
  <li><a href="#vgg-16-pretrained-2" id="toc-vgg-16-pretrained-2" class="nav-link" data-scroll-target="#vgg-16-pretrained-2">VGG-16 Pretrained</a></li>
  <li><a href="#tensorboard" id="toc-tensorboard" class="nav-link" data-scroll-target="#tensorboard">Tensorboard</a></li>
  </ul></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion">Conclusion</a></li>
  </ul>
<div class="quarto-other-links"><h2>Other Links</h2><ul><li><a href="https://www.dixslyf.dev" target="_blank"><i class="bi bi-person"></i>dixslyf's Website</a></li></ul></div><div class="quarto-code-links"><h2>Code Links</h2><ul><li><a href="https://github.com/dixslyf/mlp-vgg16-fashion/" target="_blank"><i class="bi bi-github"></i>GitHub Repo</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">


<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">A Comparison of Multilayer Perceptron and VGG-16 Models for Fashion Image Classification</h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Dixon Sean Low Yan Feng </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">October 20, 2024</p>
    </div>
  </div>
  
    <div>
    <div class="quarto-title-meta-heading">Modified</div>
    <div class="quarto-title-meta-contents">
      <p class="date-modified">July 19, 2025</p>
    </div>
  </div>
    
  </div>
  


</header>


<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>This notebook develops three models for image classification on a dataset containing images of clothes and accessories. It was written as part of a university assignment on machine learning.</p>
<p>The three models are as follows:</p>
<ul>
<li><p>A multilayer perceptron (MLP)</p></li>
<li><p>A VGG-16 implemented from scratch</p></li>
<li><p>A fine-tuned VGG-16 from PyTorch pre-trained on the <a href="https://www.image-net.org/">ImageNet</a> dataset</p></li>
</ul>
<p>All models were implemented in PyTorch.</p>
<p>The development of the models followed a typical machine learning pipeline:</p>
<ol type="1">
<li><p><strong>Data analysis:</strong> Quick exploration of the dataset to understand its structure, class distribution and potential issues such as class imbalance.</p></li>
<li><p><strong>Data preprocessing:</strong> Preparation of the data by dividing the dataset into train, validation and test splits, normalising pixel values, label encoding the target feature and defining an undersampling procedure.</p></li>
<li><p><strong>Hyperparameter tuning:</strong> Tuning of hyperparameters like learning rate, batch size and number of layers to optimise each model’s performance. Hyperparameter tuning was performed using <a href="https://optuna.org/">Optuna</a>. Each model has different hyperparameters to tune — these will be discussed more extensively later.</p></li>
<li><p><strong>Training:</strong> Training of the models on the train split using the best set of hyperparameters determined by the hyperparameter tuning procedure.</p></li>
<li><p><strong>Evaluation:</strong> Evaluation of the models on the test split using confusion matrices and metrics like accuracy and F1-score. Additionally, we will analyse each model’s loss curves using Tensorboard and then compare the models’ performance.</p></li>
</ol>
<p>Note that this notebook takes <em>several hours</em> to run even with a GPU.</p>
</section>
<section id="preamble" class="level2">
<h2 class="anchored" data-anchor-id="preamble">Preamble</h2>
<p>We’ll start by installing additional libraries we need:</p>
<div id="278b58db" class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># For downloading the dataset from Google Drive.</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>pip install gdown</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-container">
<div class="cell-output cell-output-stdout">
<pre><code>Collecting gdown
  Downloading gdown-5.2.0-py3-none-any.whl.metadata (5.8 kB)
Requirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.10/site-packages (from gdown) (4.12.3)
Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from gdown) (3.15.1)
Requirement already satisfied: requests[socks] in /opt/conda/lib/python3.10/site-packages (from gdown) (2.32.3)
Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from gdown) (4.66.4)
Requirement already satisfied: soupsieve&gt;1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4-&gt;gdown) (2.5)
Requirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /opt/conda/lib/python3.10/site-packages (from requests[socks]-&gt;gdown) (3.3.2)
Requirement already satisfied: idna&lt;4,&gt;=2.5 in /opt/conda/lib/python3.10/site-packages (from requests[socks]-&gt;gdown) (3.7)
Requirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests[socks]-&gt;gdown) (1.26.18)
Requirement already satisfied: certifi&gt;=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests[socks]-&gt;gdown) (2024.8.30)
Requirement already satisfied: PySocks!=1.5.7,&gt;=1.5.6 in /opt/conda/lib/python3.10/site-packages (from requests[socks]-&gt;gdown) (1.7.1)
Downloading gdown-5.2.0-py3-none-any.whl (18 kB)
Installing collected packages: gdown
Successfully installed gdown-5.2.0</code></pre>
</div>
</div>
</div>
<p>Kaggle environments install Optuna by default. However, other environments may not, so we check and install Optuna if needed:</p>
<div id="c144f021" class="cell">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> importlib.util</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>spec <span class="op">=</span> importlib.util.find_spec(<span class="st">"optuna"</span>)</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> spec <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>    <span class="im">import</span> subprocess</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>    subprocess.run([<span class="st">"pip"</span>, <span class="st">"install"</span>, <span class="st">"optuna"</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now, we can import all the libraries and modules we need for the rest of the notebook:</p>
<div id="8e972d23" class="cell">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> itertools</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> gc</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> math</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> statistics</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> collections.abc <span class="im">import</span> Iterable</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dataclasses <span class="im">import</span> dataclass</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> typing <span class="im">import</span> Any, Callable, Iterable, Optional, Sequence</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> gdown</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> optuna</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> sklearn</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> sklearn.model_selection</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torchvision</span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torchvision.transforms.v2</span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.tensorboard <span class="im">import</span> SummaryWriter</span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> LabelEncoder</span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tensorboard</span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch <span class="im">import</span> Tensor</span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> DataLoader, Dataset, Subset</span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tqdm <span class="im">import</span> tqdm</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We’ll define some constants as configuration for this notebook. Feel free to adjust these constants to reduce the run time of the notebook:</p>
<div id="a5f6b7ed" class="cell">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Variables for downloading and locating the dataset.</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>DATASET_GDRIVE_URL <span class="op">=</span> <span class="st">"https://drive.google.com/file/d/1nWRm-Npq_QE0j_sHyVVxVEx2Rb0Lc1zU/view"</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>DATASET_OUT_PATH <span class="op">=</span> <span class="st">"data.zip"</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>DATASET_ROOT_PATH <span class="op">=</span> <span class="st">"data/"</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Automatically determine the device to use for PyTorch.</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a><span class="co"># On Google Colab and Kaggle, make sure to enable an accelerator to use CUDA.</span></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>DEVICE <span class="op">=</span> torch.device(</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>    <span class="st">"cuda"</span></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> torch.cuda.is_available()</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span> <span class="st">"mps"</span></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> torch.backends.mps.is_available()</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span> <span class="st">"cpu"</span></span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Default batch size for training and evaluation.</span></span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Note that the batch size is still a hyperparameter that will be tuned;</span></span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a><span class="co"># this is just the default.</span></span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>DEFAULT_BATCH_SIZE: <span class="bu">int</span> <span class="op">=</span> <span class="dv">32</span></span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Number of folds to use for hyperparameter tuning.</span></span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a>N_FOLDS: <span class="bu">int</span> <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Number of trials for hyperparameter tuning.</span></span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Note: Be careful not to reduce this too much as you can accidentally cause</span></span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a><span class="co">#       all trials to be pruned! If you want to set this to a low number,</span></span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a><span class="co">#       you should disable trial pruning by setting `PRUNE_TRIALS` (below) to `False`.</span></span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a>N_TRIALS: <span class="bu">int</span> <span class="op">=</span> <span class="dv">25</span></span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true" tabindex="-1"></a><span class="co"># Max number of epochs for hyperparameter tuning.</span></span>
<span id="cb5-31"><a href="#cb5-31" aria-hidden="true" tabindex="-1"></a>N_EPOCHS_TUNE: <span class="bu">int</span> <span class="op">=</span> <span class="dv">30</span></span>
<span id="cb5-32"><a href="#cb5-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-33"><a href="#cb5-33" aria-hidden="true" tabindex="-1"></a><span class="co"># Max number of epochs for the real training.</span></span>
<span id="cb5-34"><a href="#cb5-34" aria-hidden="true" tabindex="-1"></a>N_EPOCHS_TRAIN: <span class="bu">int</span> <span class="op">=</span> <span class="dv">30</span></span>
<span id="cb5-35"><a href="#cb5-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-36"><a href="#cb5-36" aria-hidden="true" tabindex="-1"></a><span class="co"># Whether to prune trials during hyperparameter tuning.</span></span>
<span id="cb5-37"><a href="#cb5-37" aria-hidden="true" tabindex="-1"></a>PRUNE_TRIALS: <span class="bu">bool</span> <span class="op">=</span> <span class="va">True</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="data" class="level2">
<h2 class="anchored" data-anchor-id="data">Data</h2>
<p>This section includes the downloading of the dataset, preliminary analysis and preprocessing steps.</p>
<p>The dataset contains various categories of fashion images. Unfortunately, the origin of the dataset remains unknown — students were only given a Google Drive link to the dataset without any source provided.</p>
<section id="downloading-the-dataset" class="level3">
<h3 class="anchored" data-anchor-id="downloading-the-dataset">Downloading the Dataset</h3>
<p>First, we need to download the dataset. We’ll download it directly from Google Drive using the <code>gdown</code> library:</p>
<div id="d8e4d6d1" class="cell">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>gdown.cached_download(DATASET_GDRIVE_URL, DATASET_OUT_PATH, fuzzy<span class="op">=</span><span class="va">True</span>, postprocess<span class="op">=</span>gdown.extractall)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-container">
<div class="cell-output cell-output-stderr">
<pre><code>Cached downloading...
From (original): https://drive.google.com/uc?id=1nWRm-Npq_QE0j_sHyVVxVEx2Rb0Lc1zU
From (redirected): https://drive.google.com/uc?id=1nWRm-Npq_QE0j_sHyVVxVEx2Rb0Lc1zU&amp;confirm=t&amp;uuid=92f80be4-e2d6-4b2e-b82d-94faf30a6bf7
To: data.zip
100%|██████████| 44.2M/44.2M [00:00&lt;00:00, 64.7MB/s]</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="5">
<pre><code>'data.zip'</code></pre>
</div>
</div>
</div>
<p>To verify that the dataset was downloaded and extracted, we’ll use the shell’s <code>ls</code> command to list the files in the <code>data</code> directory. The following command should show <code>test  train  valid</code> if the dataset was successfully downloaded and extracted:</p>
<div id="afc1cb5f" class="cell">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>ls data</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-container">
<div class="cell-output cell-output-stderr">
<pre><code>/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.
  pid, fd = os.forkpty()</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>test  train  valid</code></pre>
</div>
</div>
</div>
<p>If the download fails (e.g., because the Google Drive link is down), please manually download and extract the dataset and set the <code>DATASET_ROOT_PATH</code> constant to the root directory of the dataset. You can find a copy of the dataset from this notebook’s GitHub repository <a href="https://github.com/dixslyf/mlp-vgg16-fashion/releases/latest/download/data.zip">here</a>.</p>
</section>
<section id="custom-dataset" class="level3">
<h3 class="anchored" data-anchor-id="custom-dataset">Custom <code>Dataset</code></h3>
<p>To load the images from the dataset, we need to define a custom PyTorch <code>Dataset</code>:</p>
<div id="2dfa9c97" class="cell">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ClothesDataset(Dataset):</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="co">    A PyTorch `Dataset` class for loading the clothes and accessories dataset.</span></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Attributes:</span></span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a><span class="co">        input_transform (Optional[Callable[[Tensor], Tensor]]): Transformation applied to input images.</span></span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a><span class="co">        target_transform (Optional[Callable[[str], Any]]): Transformation applied to target labels.</span></span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a><span class="co">        categories (set[str]): The unique categories (subdirectory names) present in the dataset.</span></span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(</span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>,</span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a>        img_dirs: <span class="bu">str</span> <span class="op">|</span> Iterable[<span class="bu">str</span>],</span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a>        input_transform: Optional[Callable[[Tensor], Tensor]] <span class="op">=</span> <span class="va">None</span>,</span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a>        target_transform: Optional[Callable[[<span class="bu">str</span>], Any]] <span class="op">=</span> <span class="va">None</span>,</span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a>    ):</span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a><span class="co">        Initialises the dataset with one or more directories containing the images,</span></span>
<span id="cb12-19"><a href="#cb12-19" aria-hidden="true" tabindex="-1"></a><span class="co">        and with optional input and target transformations.</span></span>
<span id="cb12-20"><a href="#cb12-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-21"><a href="#cb12-21" aria-hidden="true" tabindex="-1"></a><span class="co">        The given directories are assumed to be organised such that images are put into subdirectories</span></span>
<span id="cb12-22"><a href="#cb12-22" aria-hidden="true" tabindex="-1"></a><span class="co">        that correspond to the categories.</span></span>
<span id="cb12-23"><a href="#cb12-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-24"><a href="#cb12-24" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb12-25"><a href="#cb12-25" aria-hidden="true" tabindex="-1"></a><span class="co">            img_dirs (str | Iterable[str]): Directory path(s) containing the images.</span></span>
<span id="cb12-26"><a href="#cb12-26" aria-hidden="true" tabindex="-1"></a><span class="co">            input_transform (Optional[Callable[[Tensor], Tensor]]): Transformation applied to input images.</span></span>
<span id="cb12-27"><a href="#cb12-27" aria-hidden="true" tabindex="-1"></a><span class="co">            target_transform (Optional[Callable[[str], Any]]): Transformation applied to target labels.</span></span>
<span id="cb12-28"><a href="#cb12-28" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb12-29"><a href="#cb12-29" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._img_dirs: <span class="bu">list</span>[<span class="bu">str</span>] <span class="op">=</span> (</span>
<span id="cb12-30"><a href="#cb12-30" aria-hidden="true" tabindex="-1"></a>            [img_dirs] <span class="cf">if</span> <span class="bu">type</span>(img_dirs) <span class="kw">is</span> <span class="bu">str</span> <span class="cf">else</span> <span class="bu">list</span>(img_dirs)</span>
<span id="cb12-31"><a href="#cb12-31" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb12-32"><a href="#cb12-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-33"><a href="#cb12-33" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.input_transform <span class="op">=</span> input_transform</span>
<span id="cb12-34"><a href="#cb12-34" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.target_transform <span class="op">=</span> target_transform</span>
<span id="cb12-35"><a href="#cb12-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-36"><a href="#cb12-36" aria-hidden="true" tabindex="-1"></a>        <span class="co"># A list of pairs that maps the path of an image (path includes each `img_dir`)</span></span>
<span id="cb12-37"><a href="#cb12-37" aria-hidden="true" tabindex="-1"></a>        <span class="co"># to its category. We're intentionally not using a dictionary because we need</span></span>
<span id="cb12-38"><a href="#cb12-38" aria-hidden="true" tabindex="-1"></a>        <span class="co"># to access elements by index, which is not possible with a dictionary.</span></span>
<span id="cb12-39"><a href="#cb12-39" aria-hidden="true" tabindex="-1"></a>        <span class="co">#</span></span>
<span id="cb12-40"><a href="#cb12-40" aria-hidden="true" tabindex="-1"></a>        <span class="co"># To be lazy and avoid I/O in `__init__()`, this will only be populated on</span></span>
<span id="cb12-41"><a href="#cb12-41" aria-hidden="true" tabindex="-1"></a>        <span class="co"># the first call to `__getitem__()` or `__len__()`.</span></span>
<span id="cb12-42"><a href="#cb12-42" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._img_label_map: Optional[<span class="bu">list</span>[<span class="bu">tuple</span>[<span class="bu">str</span>, <span class="bu">str</span>]]] <span class="op">=</span> <span class="va">None</span></span>
<span id="cb12-43"><a href="#cb12-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-44"><a href="#cb12-44" aria-hidden="true" tabindex="-1"></a>    <span class="at">@property</span></span>
<span id="cb12-45"><a href="#cb12-45" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> categories(<span class="va">self</span>) <span class="op">-&gt;</span> <span class="bu">set</span>[<span class="bu">str</span>]:</span>
<span id="cb12-46"><a href="#cb12-46" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb12-47"><a href="#cb12-47" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns the unique categories in the dataset.</span></span>
<span id="cb12-48"><a href="#cb12-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-49"><a href="#cb12-49" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns:</span></span>
<span id="cb12-50"><a href="#cb12-50" aria-hidden="true" tabindex="-1"></a><span class="co">            set[str]: Set of unique categories in the dataset.</span></span>
<span id="cb12-51"><a href="#cb12-51" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb12-52"><a href="#cb12-52" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>._img_label_map <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb12-53"><a href="#cb12-53" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>._init_img_label_map()</span>
<span id="cb12-54"><a href="#cb12-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-55"><a href="#cb12-55" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>._categories</span>
<span id="cb12-56"><a href="#cb12-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-57"><a href="#cb12-57" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> targets(<span class="va">self</span>, indices: Optional[Iterable[<span class="bu">int</span>]] <span class="op">=</span> <span class="va">None</span>) <span class="op">-&gt;</span> Iterable:</span>
<span id="cb12-58"><a href="#cb12-58" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb12-59"><a href="#cb12-59" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns the target labels for the specified indices or for all samples if indices are not provided.</span></span>
<span id="cb12-60"><a href="#cb12-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-61"><a href="#cb12-61" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb12-62"><a href="#cb12-62" aria-hidden="true" tabindex="-1"></a><span class="co">            indices (Optional[Iterable[int]]): Indices of the target labels to retrieve.</span></span>
<span id="cb12-63"><a href="#cb12-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-64"><a href="#cb12-64" aria-hidden="true" tabindex="-1"></a><span class="co">        Yields:</span></span>
<span id="cb12-65"><a href="#cb12-65" aria-hidden="true" tabindex="-1"></a><span class="co">            Iterable: Transformed target labels or raw labels if no transformation is applied.</span></span>
<span id="cb12-66"><a href="#cb12-66" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb12-67"><a href="#cb12-67" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> indices <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb12-68"><a href="#cb12-68" aria-hidden="true" tabindex="-1"></a>            it <span class="op">=</span> <span class="va">self</span>._img_label_map</span>
<span id="cb12-69"><a href="#cb12-69" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb12-70"><a href="#cb12-70" aria-hidden="true" tabindex="-1"></a>            it <span class="op">=</span> (<span class="va">self</span>._img_label_map[idx] <span class="cf">for</span> idx <span class="kw">in</span> indices)</span>
<span id="cb12-71"><a href="#cb12-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-72"><a href="#cb12-72" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> path, cat <span class="kw">in</span> it:</span>
<span id="cb12-73"><a href="#cb12-73" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="va">self</span>.target_transform <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb12-74"><a href="#cb12-74" aria-hidden="true" tabindex="-1"></a>                <span class="cf">yield</span> cat</span>
<span id="cb12-75"><a href="#cb12-75" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span>:</span>
<span id="cb12-76"><a href="#cb12-76" aria-hidden="true" tabindex="-1"></a>                <span class="cf">yield</span> <span class="va">self</span>.target_transform(cat)</span>
<span id="cb12-77"><a href="#cb12-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-78"><a href="#cb12-78" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _init_img_label_map(<span class="va">self</span>):</span>
<span id="cb12-79"><a href="#cb12-79" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb12-80"><a href="#cb12-80" aria-hidden="true" tabindex="-1"></a><span class="co">        Initialises the image-label mapping by listing all images and their corresponding categories</span></span>
<span id="cb12-81"><a href="#cb12-81" aria-hidden="true" tabindex="-1"></a><span class="co">        from the specified directories. The subdirectory names represent the categories.</span></span>
<span id="cb12-82"><a href="#cb12-82" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb12-83"><a href="#cb12-83" aria-hidden="true" tabindex="-1"></a>        cats <span class="op">=</span> {</span>
<span id="cb12-84"><a href="#cb12-84" aria-hidden="true" tabindex="-1"></a>            cat</span>
<span id="cb12-85"><a href="#cb12-85" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> img_dir <span class="kw">in</span> <span class="va">self</span>._img_dirs</span>
<span id="cb12-86"><a href="#cb12-86" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> cat <span class="kw">in</span> os.listdir(img_dir)</span>
<span id="cb12-87"><a href="#cb12-87" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> os.path.isdir(os.path.join(img_dir, cat))</span>
<span id="cb12-88"><a href="#cb12-88" aria-hidden="true" tabindex="-1"></a>        }</span>
<span id="cb12-89"><a href="#cb12-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-90"><a href="#cb12-90" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._categories <span class="op">=</span> cats</span>
<span id="cb12-91"><a href="#cb12-91" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._img_label_map <span class="op">=</span> []</span>
<span id="cb12-92"><a href="#cb12-92" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> img_dir, cat <span class="kw">in</span> itertools.product(<span class="va">self</span>._img_dirs, cats):</span>
<span id="cb12-93"><a href="#cb12-93" aria-hidden="true" tabindex="-1"></a>            cat_dir <span class="op">=</span> os.path.join(img_dir, cat)</span>
<span id="cb12-94"><a href="#cb12-94" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="kw">not</span> os.path.isdir(cat_dir):</span>
<span id="cb12-95"><a href="#cb12-95" aria-hidden="true" tabindex="-1"></a>                <span class="cf">continue</span></span>
<span id="cb12-96"><a href="#cb12-96" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-97"><a href="#cb12-97" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> img <span class="kw">in</span> os.listdir(cat_dir):</span>
<span id="cb12-98"><a href="#cb12-98" aria-hidden="true" tabindex="-1"></a>                img_path <span class="op">=</span> os.path.join(cat_dir, img)</span>
<span id="cb12-99"><a href="#cb12-99" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>._img_label_map.append((img_path, cat))</span>
<span id="cb12-100"><a href="#cb12-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-101"><a href="#cb12-101" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__getitem__</span>(<span class="va">self</span>, idx: <span class="bu">int</span>) <span class="op">-&gt;</span> <span class="bu">tuple</span>[Tensor, <span class="bu">str</span>]:</span>
<span id="cb12-102"><a href="#cb12-102" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb12-103"><a href="#cb12-103" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns the image and corresponding label at the specified index.</span></span>
<span id="cb12-104"><a href="#cb12-104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-105"><a href="#cb12-105" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb12-106"><a href="#cb12-106" aria-hidden="true" tabindex="-1"></a><span class="co">            idx (int): Index of the image-label pair to retrieve.</span></span>
<span id="cb12-107"><a href="#cb12-107" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-108"><a href="#cb12-108" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns:</span></span>
<span id="cb12-109"><a href="#cb12-109" aria-hidden="true" tabindex="-1"></a><span class="co">            tuple[Tensor, str]: A pair of the image tensor and its corresponding label, with optional</span></span>
<span id="cb12-110"><a href="#cb12-110" aria-hidden="true" tabindex="-1"></a><span class="co">            transformations applied.</span></span>
<span id="cb12-111"><a href="#cb12-111" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb12-112"><a href="#cb12-112" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>._img_label_map <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb12-113"><a href="#cb12-113" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>._init_img_label_map()</span>
<span id="cb12-114"><a href="#cb12-114" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-115"><a href="#cb12-115" aria-hidden="true" tabindex="-1"></a>        img_path, label <span class="op">=</span> <span class="va">self</span>._img_label_map[idx]</span>
<span id="cb12-116"><a href="#cb12-116" aria-hidden="true" tabindex="-1"></a>        img <span class="op">=</span> torchvision.io.read_image(img_path)</span>
<span id="cb12-117"><a href="#cb12-117" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-118"><a href="#cb12-118" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.input_transform <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb12-119"><a href="#cb12-119" aria-hidden="true" tabindex="-1"></a>            img <span class="op">=</span> <span class="va">self</span>.input_transform(img)</span>
<span id="cb12-120"><a href="#cb12-120" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-121"><a href="#cb12-121" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.target_transform <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb12-122"><a href="#cb12-122" aria-hidden="true" tabindex="-1"></a>            label <span class="op">=</span> <span class="va">self</span>.target_transform(label)</span>
<span id="cb12-123"><a href="#cb12-123" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-124"><a href="#cb12-124" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> img, label</span>
<span id="cb12-125"><a href="#cb12-125" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-126"><a href="#cb12-126" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__len__</span>(<span class="va">self</span>) <span class="op">-&gt;</span> <span class="bu">int</span>:</span>
<span id="cb12-127"><a href="#cb12-127" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb12-128"><a href="#cb12-128" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns the total number of samples in the dataset.</span></span>
<span id="cb12-129"><a href="#cb12-129" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-130"><a href="#cb12-130" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns:</span></span>
<span id="cb12-131"><a href="#cb12-131" aria-hidden="true" tabindex="-1"></a><span class="co">            int: The number of images in the dataset.</span></span>
<span id="cb12-132"><a href="#cb12-132" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb12-133"><a href="#cb12-133" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>._img_label_map <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb12-134"><a href="#cb12-134" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>._init_img_label_map()</span>
<span id="cb12-135"><a href="#cb12-135" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-136"><a href="#cb12-136" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="bu">len</span>(<span class="va">self</span>._img_label_map)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>I also define a custom <code>Subset</code> to make it easier to split the dataset and access the target labels of a subset:</p>
<div id="2017451a" class="cell">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ClothesSubset(torch.utils.data.Subset):</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="co">    A subset of a `ClothesDataset` that maintains access to the dataset's target labels.</span></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a><span class="co">        dataset (ClothesDataset): The `ClothesDataset` from which the subset is drawn.</span></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a><span class="co">        indices (Sequence[int]): The indices of the elements in the original dataset to include in the subset.</span></span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(</span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>, dataset: torch.utils.data.Dataset, indices: Sequence[<span class="bu">int</span>]</span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a>    ) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a><span class="co">        Initialises the subset with the given dataset and a sequence of indices.</span></span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a><span class="co">            dataset (Dataset): The `ClothesDataset` to create a subset from.</span></span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a><span class="co">            indices (Sequence[int]): Indices specifying which elements of the dataset</span></span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a><span class="co">                should be included in this subset.</span></span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb13-21"><a href="#cb13-21" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(dataset, indices)</span>
<span id="cb13-22"><a href="#cb13-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-23"><a href="#cb13-23" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> targets(<span class="va">self</span>, indices: Optional[Iterable[<span class="bu">int</span>]] <span class="op">=</span> <span class="va">None</span>):</span>
<span id="cb13-24"><a href="#cb13-24" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb13-25"><a href="#cb13-25" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns the target labels for the specified indices, or for the entire subset if no indices are provided.</span></span>
<span id="cb13-26"><a href="#cb13-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-27"><a href="#cb13-27" aria-hidden="true" tabindex="-1"></a><span class="co">        This method is designed to work specifically with `ClothesDataset` and also handles</span></span>
<span id="cb13-28"><a href="#cb13-28" aria-hidden="true" tabindex="-1"></a><span class="co">        the case where the subset is a subset of another subset.</span></span>
<span id="cb13-29"><a href="#cb13-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-30"><a href="#cb13-30" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb13-31"><a href="#cb13-31" aria-hidden="true" tabindex="-1"></a><span class="co">            indices (Optional[Iterable[int]]): Indices of the target labels to retrieve from the subset.</span></span>
<span id="cb13-32"><a href="#cb13-32" aria-hidden="true" tabindex="-1"></a><span class="co">                If no indices are provided, returns the labels for all items in the subset.</span></span>
<span id="cb13-33"><a href="#cb13-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-34"><a href="#cb13-34" aria-hidden="true" tabindex="-1"></a><span class="co">        Yields:</span></span>
<span id="cb13-35"><a href="#cb13-35" aria-hidden="true" tabindex="-1"></a><span class="co">            The target labels corresponding to the specified indices in the subset.</span></span>
<span id="cb13-36"><a href="#cb13-36" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb13-37"><a href="#cb13-37" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> indices <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb13-38"><a href="#cb13-38" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> target <span class="kw">in</span> <span class="va">self</span>.dataset.targets(<span class="va">self</span>.indices):</span>
<span id="cb13-39"><a href="#cb13-39" aria-hidden="true" tabindex="-1"></a>                <span class="cf">yield</span> target</span>
<span id="cb13-40"><a href="#cb13-40" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb13-41"><a href="#cb13-41" aria-hidden="true" tabindex="-1"></a>            <span class="co"># In case we have a subset of a subset, use a set to handle index lookups efficiently.</span></span>
<span id="cb13-42"><a href="#cb13-42" aria-hidden="true" tabindex="-1"></a>            indices_set <span class="op">=</span> <span class="bu">set</span>(indices)</span>
<span id="cb13-43"><a href="#cb13-43" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> idx, target <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="va">self</span>.dataset.targets(<span class="va">self</span>.indices)):</span>
<span id="cb13-44"><a href="#cb13-44" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> idx <span class="kw">in</span> indices_set:</span>
<span id="cb13-45"><a href="#cb13-45" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">yield</span> target</span>
<span id="cb13-46"><a href="#cb13-46" aria-hidden="true" tabindex="-1"></a></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We can now load and explore the dataset.</p>
<p><strong>Note:</strong> I intentionally ignore the test split because it is unlabelled and only has 8 samples, which makes it unuseful for evaluation. In the code cell below, I combine the train and validation splits together — later in the notebook, I perform my own train-validation-test split on this combined dataset.</p>
<div id="70703d68" class="cell">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>ds <span class="op">=</span> ClothesDataset((os.path.join(DATASET_ROOT_PATH, split) <span class="cf">for</span> split <span class="kw">in</span> (<span class="st">"train"</span>, <span class="st">"valid"</span>)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="fb7bc69c" class="cell">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Total number of samples: </span><span class="sc">{</span><span class="bu">len</span>(ds)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Categories: </span><span class="sc">{</span>ds<span class="sc">.</span>categories<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-container">
<div class="cell-output cell-output-stdout">
<pre><code>Total number of samples: 3849
Categories: {'shirts', 'shoes', 'shorts', 'accessories', 'knitwear', 'jackets', 'jeans', 'tees'}</code></pre>
</div>
</div>
</div>
<p>The dataset has 3849 samples, which is relatively small compared to other image datasets.</p>
</section>
<section id="preliminary-analysis" class="level3">
<h3 class="anchored" data-anchor-id="preliminary-analysis">Preliminary Analysis</h3>
<p>To start, I’ll plot some of the images to have a feel of what the images look like:</p>
<div id="cb6829e5" class="cell">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>rows <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>cols <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(rows, cols, figsize<span class="op">=</span>(<span class="dv">2</span> <span class="op">*</span> cols, <span class="dv">2</span> <span class="op">*</span> rows))</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> idx <span class="kw">in</span> <span class="bu">range</span>(rows <span class="op">*</span> cols):</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>    img, label <span class="op">=</span> ds[idx <span class="op">*</span> <span class="dv">200</span>] <span class="co"># Multiply by 200 so that we don't just see shorts.</span></span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>    ax <span class="op">=</span> axes[idx <span class="op">//</span> cols, idx <span class="op">%</span> cols]</span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>    ax.imshow(img.permute(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">0</span>))</span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a>    ax.set_title(<span class="ss">f"Label: </span><span class="sc">{</span>label<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-container">
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="mlp-vgg16-fashion_files/figure-html/cell-12-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</div>
<p>Some observations:</p>
<ul>
<li><p>The images are <em>coloured</em>, not grayscale.</p></li>
<li><p>The images appear to all be the same size (of course, this is just a small sample size of 16, so there’s no guarantee).</p></li>
</ul>
<p>The dimensions of each image are likely 432 by 300:</p>
<div id="9d7f01c4" class="cell">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Dimensions of the first image.</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Dimensions: </span><span class="sc">{</span>ds[<span class="dv">0</span>][<span class="dv">0</span>]<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-container">
<div class="cell-output cell-output-stdout">
<pre><code>Dimensions: torch.Size([3, 432, 300])</code></pre>
</div>
</div>
</div>
<p>In case there are any images that have different dimensions (which would cause problems when feeding them to the models), we’ll resize all images to be the same size later. 432 by 300 is quite large and could potentially exhaust GPU memory during hyperparameter tuning and training, so the images will be resized to smaller dimensions (the dimensions will be tuned during hyperparameter tuning).</p>
<p>Next, are the pixel values normalised? They do not seem to be. Instead, they seem to be in the typical range of 0 to 255. We can gain some assurance from the documentation for PyTorch’s <a href="https://pytorch.org/vision/main/generated/torchvision.io.read_image.html"><code>read_image()</code></a>: “The values of the output tensor are in uint8 in [0, 255] for most cases”. We will need to normalise the pixel values later.</p>
<div id="607d00b4" class="cell">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Pixel values of the first image:</span><span class="ch">\n</span><span class="sc">{</span>ds[<span class="dv">0</span>][<span class="dv">0</span>]<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-container">
<div class="cell-output cell-output-stdout">
<pre><code>Pixel values of the first image:
tensor([[[242, 242, 242,  ..., 242, 242, 242],
         [242, 242, 242,  ..., 242, 242, 242],
         [242, 242, 242,  ..., 242, 242, 242],
         ...,
         [242, 242, 242,  ..., 242, 242, 242],
         [242, 242, 242,  ..., 242, 242, 242],
         [242, 242, 242,  ..., 242, 242, 242]],

        [[242, 242, 242,  ..., 242, 242, 242],
         [242, 242, 242,  ..., 242, 242, 242],
         [242, 242, 242,  ..., 242, 242, 242],
         ...,
         [242, 242, 242,  ..., 242, 242, 242],
         [242, 242, 242,  ..., 242, 242, 242],
         [242, 242, 242,  ..., 242, 242, 242]],

        [[242, 242, 242,  ..., 242, 242, 242],
         [242, 242, 242,  ..., 242, 242, 242],
         [242, 242, 242,  ..., 242, 242, 242],
         ...,
         [242, 242, 242,  ..., 242, 242, 242],
         [242, 242, 242,  ..., 242, 242, 242],
         [242, 242, 242,  ..., 242, 242, 242]]], dtype=torch.uint8)</code></pre>
</div>
</div>
</div>
<p>Unfortunately, looking at the distribution of the classes, there is an imbalanced class problem — the “shoes” and “tees” classes have a much higher number of samples than the others. This could affect the performance of the models and make them biased towards these dominant classes. Thankfully, the other classes are fairly balanced, so we can resolve the issue by undersampling “shoes” and “tees”. This will lead to less training data, however, so we will try out various data augmentations (e.g., horizontal flips, colour jitter) later during hyperparameter tuning to introduce more varied data.</p>
<div id="47f6740f" class="cell">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>sns.displot(<span class="bu">list</span>(ds.targets()), height<span class="op">=</span><span class="dv">8</span>, aspect<span class="op">=</span><span class="dv">10</span><span class="op">/</span><span class="dv">8</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-container">
<div class="cell-output cell-output-stderr">
<pre><code>/opt/conda/lib/python3.10/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.
  with pd.option_context('mode.use_inf_as_na', True):</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="mlp-vgg16-fashion_files/figure-html/cell-15-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</div>
</section>
<section id="preprocessing" class="level3">
<h3 class="anchored" data-anchor-id="preprocessing">Preprocessing</h3>
<section id="normalisation-and-label-encoding" class="level4">
<h4 class="anchored" data-anchor-id="normalisation-and-label-encoding">Normalisation and Label Encoding</h4>
<p>Now, to prepare the data for use, we’ll first normalise the pixel values. The raw images have pixel values that range from 0 to 255, so we can normalise them by simply dividing each value by 255:</p>
<div id="5765fbe2" class="cell">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Normalise.</span></span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> input_transform(X):</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> X <span class="op">/</span> <span class="dv">255</span></span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>ds.input_transform <span class="op">=</span> input_transform</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>To confirm that the pixel values were normalised correctly, we’ll inspect the pixel values for one of the images:</p>
<div id="10afbfea" class="cell">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Pixel values:</span><span class="ch">\n</span><span class="sc">{</span>ds[<span class="dv">0</span>][<span class="dv">0</span>]<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-container">
<div class="cell-output cell-output-stdout">
<pre><code>Pixel values:
tensor([[[0.9490, 0.9490, 0.9490,  ..., 0.9490, 0.9490, 0.9490],
         [0.9490, 0.9490, 0.9490,  ..., 0.9490, 0.9490, 0.9490],
         [0.9490, 0.9490, 0.9490,  ..., 0.9490, 0.9490, 0.9490],
         ...,
         [0.9490, 0.9490, 0.9490,  ..., 0.9490, 0.9490, 0.9490],
         [0.9490, 0.9490, 0.9490,  ..., 0.9490, 0.9490, 0.9490],
         [0.9490, 0.9490, 0.9490,  ..., 0.9490, 0.9490, 0.9490]],

        [[0.9490, 0.9490, 0.9490,  ..., 0.9490, 0.9490, 0.9490],
         [0.9490, 0.9490, 0.9490,  ..., 0.9490, 0.9490, 0.9490],
         [0.9490, 0.9490, 0.9490,  ..., 0.9490, 0.9490, 0.9490],
         ...,
         [0.9490, 0.9490, 0.9490,  ..., 0.9490, 0.9490, 0.9490],
         [0.9490, 0.9490, 0.9490,  ..., 0.9490, 0.9490, 0.9490],
         [0.9490, 0.9490, 0.9490,  ..., 0.9490, 0.9490, 0.9490]],

        [[0.9490, 0.9490, 0.9490,  ..., 0.9490, 0.9490, 0.9490],
         [0.9490, 0.9490, 0.9490,  ..., 0.9490, 0.9490, 0.9490],
         [0.9490, 0.9490, 0.9490,  ..., 0.9490, 0.9490, 0.9490],
         ...,
         [0.9490, 0.9490, 0.9490,  ..., 0.9490, 0.9490, 0.9490],
         [0.9490, 0.9490, 0.9490,  ..., 0.9490, 0.9490, 0.9490],
         [0.9490, 0.9490, 0.9490,  ..., 0.9490, 0.9490, 0.9490]]])</code></pre>
</div>
</div>
</div>
<p>Next, because our models can only understand numbers, we need to encode the labels (i.e., the categories) into numbers. I use <code>scikit-learn</code>’s <code>LabelEncoder</code> for this:</p>
<div id="fa81b545" class="cell">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>target_le <span class="op">=</span> LabelEncoder()</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>target_le.fit(<span class="bu">list</span>(ds.categories))</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> target_transform(y):</span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a>  encoded <span class="op">=</span> target_le.transform([y])</span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> encoded[<span class="dv">0</span>]</span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a>ds.target_transform <span class="op">=</span> target_transform</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We’ll check the label encoder’s classes and ensure that we have 8 unique integers for the labels:</p>
<div id="ea517174" class="cell">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a>target_le.classes_</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-container">
<div class="cell-output cell-output-display" data-execution_count="18">
<pre><code>array(['accessories', 'jackets', 'jeans', 'knitwear', 'shirts', 'shoes',
       'shorts', 'tees'], dtype='&lt;U11')</code></pre>
</div>
</div>
</div>
<div id="e184b596" class="cell">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="bu">set</span>(ds.targets())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-container">
<div class="cell-output cell-output-display" data-execution_count="19">
<pre><code>{0, 1, 2, 3, 4, 5, 6, 7}</code></pre>
</div>
</div>
</div>
</section>
<section id="splitting-the-dataset" class="level4">
<h4 class="anchored" data-anchor-id="splitting-the-dataset">Splitting the Dataset</h4>
<p>As briefly mentioned earlier, I do not use the test split provided by the dataset as it only contains 8 samples and does not contain labels. I instead perform my own train-validation-test split with a ratio of <code>70:15:15</code>.</p>
<p>The test set will be used for the final evaluation of the models — we will <strong>not</strong> touch this split at all during hyperparameter tuning and training to avoid data leakage, which would lead to overly optimistic results.</p>
<p>The validation set will be used to estimate the validation loss during training so that we can compare it to the training loss. This validation loss will also be used to stop training early when the generalisation loss stops improving to prevent the model from overfitting.</p>
<p>The training set will, of course, be used for training. However, it will also be used for hyperparameter tuning. Note that the validation set will not be used for hyperparameter tuning. I prefer to use K-fold cross-validation to further split the training set into train and validation folds during hyperparameter tuning to get a more unbiased estimate of model performance — if I re-use the validation set for hyperparameter tuning, there is a risk of overfitting to it, especially since the dataset is rather small.</p>
<div id="64a06171" class="cell">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a>indices_train, indices_other <span class="op">=</span> sklearn.model_selection.train_test_split(</span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a>    np.arange(<span class="bu">len</span>(ds)),</span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a>    train_size<span class="op">=</span><span class="fl">0.7</span>,</span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a>    random_state<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a>    shuffle<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a>    stratify<span class="op">=</span>np.fromiter(ds.targets(), dtype<span class="op">=</span><span class="bu">int</span>),</span>
<span id="cb32-7"><a href="#cb32-7" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb32-8"><a href="#cb32-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-9"><a href="#cb32-9" aria-hidden="true" tabindex="-1"></a>indices_val, indices_test <span class="op">=</span> sklearn.model_selection.train_test_split(</span>
<span id="cb32-10"><a href="#cb32-10" aria-hidden="true" tabindex="-1"></a>    indices_other,</span>
<span id="cb32-11"><a href="#cb32-11" aria-hidden="true" tabindex="-1"></a>    train_size<span class="op">=</span><span class="fl">0.5</span>,</span>
<span id="cb32-12"><a href="#cb32-12" aria-hidden="true" tabindex="-1"></a>    random_state<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb32-13"><a href="#cb32-13" aria-hidden="true" tabindex="-1"></a>    shuffle<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb32-14"><a href="#cb32-14" aria-hidden="true" tabindex="-1"></a>    stratify<span class="op">=</span>np.fromiter(ds.targets(indices_other), dtype<span class="op">=</span><span class="bu">int</span>),</span>
<span id="cb32-15"><a href="#cb32-15" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb32-16"><a href="#cb32-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-17"><a href="#cb32-17" aria-hidden="true" tabindex="-1"></a>ds_train <span class="op">=</span> ClothesSubset(ds, indices_train)</span>
<span id="cb32-18"><a href="#cb32-18" aria-hidden="true" tabindex="-1"></a>ds_val <span class="op">=</span> ClothesSubset(ds, indices_val)</span>
<span id="cb32-19"><a href="#cb32-19" aria-hidden="true" tabindex="-1"></a>ds_test <span class="op">=</span> ClothesSubset(ds, indices_test)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="7ca46281" class="cell">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Size of train split: </span><span class="sc">{</span><span class="bu">len</span>(ds_train)<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-container">
<div class="cell-output cell-output-stdout">
<pre><code>Size of train split: 2694</code></pre>
</div>
</div>
</div>
<div id="d259bf6c" class="cell">
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Size of validation split: </span><span class="sc">{</span><span class="bu">len</span>(ds_val)<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-container">
<div class="cell-output cell-output-stdout">
<pre><code>Size of validation split: 577</code></pre>
</div>
</div>
</div>
<div id="c31c12b7" class="cell">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Size of test split: </span><span class="sc">{</span><span class="bu">len</span>(ds_test)<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-container">
<div class="cell-output cell-output-stdout">
<pre><code>Size of test split: 578</code></pre>
</div>
</div>
</div>
</section>
<section id="undersampling" class="level4">
<h4 class="anchored" data-anchor-id="undersampling">Undersampling</h4>
<p>As mentioned earlier, the dataset has a class imbalance problem, where “shoes” and “tees” have significantly more samples than the other classes. To combat this issue, we will undersample the “shoes” and “tees” classes by using a <code>WeightedRandomSampler</code> to give smaller weights (i.e., lower probability of being sampled) to their instances. This sampler will be passed to a <code>DataLoader</code> later during training and hyperparameter tuning.</p>
<p><strong>Note:</strong> The weights are determined <strong>only from the train split</strong>. Otherwise, we would be leaking data from the test and validation splits. We also need to be careful during hyperparameter tuning to not use the weights determined by the whole train split since each iteration of K-fold cross-validation will have its own train set. Hence, I define a function to create an undersampler from specified training labels so that each iteration of K-fold cross-validation can pass its own train split and create its own undersampler:</p>
<div id="d308d446" class="cell">
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> make_undersampler(labels_train: torch.Tensor):</span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Count the number of samples in each class</span></span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># of the train split.</span></span>
<span id="cb39-4"><a href="#cb39-4" aria-hidden="true" tabindex="-1"></a>    class_counts <span class="op">=</span> torch.bincount(labels_train)</span>
<span id="cb39-5"><a href="#cb39-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-6"><a href="#cb39-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Use the inverses of the class counts as the weights</span></span>
<span id="cb39-7"><a href="#cb39-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># so that "shoes" and "tees" have lower weights.</span></span>
<span id="cb39-8"><a href="#cb39-8" aria-hidden="true" tabindex="-1"></a>    class_weights <span class="op">=</span> <span class="fl">1.</span> <span class="op">/</span> class_counts.<span class="bu">float</span>()</span>
<span id="cb39-9"><a href="#cb39-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-10"><a href="#cb39-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Assign weights to each sample.</span></span>
<span id="cb39-11"><a href="#cb39-11" aria-hidden="true" tabindex="-1"></a>    sample_weights <span class="op">=</span> class_weights[labels_train]</span>
<span id="cb39-12"><a href="#cb39-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-13"><a href="#cb39-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> torch.utils.data.WeightedRandomSampler(</span>
<span id="cb39-14"><a href="#cb39-14" aria-hidden="true" tabindex="-1"></a>        weights<span class="op">=</span>sample_weights,</span>
<span id="cb39-15"><a href="#cb39-15" aria-hidden="true" tabindex="-1"></a>        num_samples<span class="op">=</span><span class="bu">len</span>(sample_weights),</span>
<span id="cb39-16"><a href="#cb39-16" aria-hidden="true" tabindex="-1"></a>        replacement<span class="op">=</span><span class="va">True</span></span>
<span id="cb39-17"><a href="#cb39-17" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb39-18"><a href="#cb39-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-19"><a href="#cb39-19" aria-hidden="true" tabindex="-1"></a>undersampler <span class="op">=</span> make_undersampler(torch.tensor(np.fromiter(ds_train.targets(), dtype<span class="op">=</span><span class="bu">int</span>)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The undersampler created here (<code>undersampler</code>) is <strong>only for the real training</strong> process that will use the entire train split. Each iteration of K-fold cross-validation during hyperparameter tuning will call the <code>make_undersampler()</code> function with its own train set to create its own undersampler.</p>
</section>
</section>
</section>
<section id="model-definitions" class="level2">
<h2 class="anchored" data-anchor-id="model-definitions">Model Definitions</h2>
<p>This section contains definitions for each model (MLP, VGG-16 and pretrained VGG-16).</p>
<p>Note that the MLP and VGG-16 that I implemented from scratch use the <em>lazy</em> variants of PyTorch’s neural network modules, which defer the initialisation of layer parameters. The rationale is so that (a) I do not need to manually calculate the dimensions of the inputs and outputs for each layer, which is error-prone, and (b) the model will automatically detect the shape of the inputs. These make the model definitions much more readable and easier to prototype with. The trade-offs are that there is a slight overhead for initialising the layers during the first forward pass, and shape mismatch errors are detected later instead of during model initialisation.</p>
<p><strong>Note:</strong> For visualisations of the architectures of the final models, please see the Training and Evaluation section. The model architectures defined here are “semi-fixed” — some architectural decisions are treated as hyperparameters that need to be tuned, so the models are not instantiated until then.</p>
<section id="multilayer-perceptron" class="level3">
<h3 class="anchored" data-anchor-id="multilayer-perceptron">Multilayer Perceptron</h3>
<p>Since it is not obvious what MLP architecture would work best for the dataset, the <code>MultilayerPercecptron</code> class below provides a semi-fixed architecture and allows specifying the activation function to use and the number of layers and their corresponding sizes. These are treated as hyperparameters to be tuned.</p>
<p>This MLP model adds a batch normalisation layer after each linear layer to stabilise training, reduce dependency on input scaling and add a regularisation effect. Ideally, whether to use batch normalisation would have been a hyperparameter to tune, but I decided not to make it configurable to reduce the hyperparameter search space.</p>
<p>Other architectural options I <em>could</em> have explored include: use of dropout layers, use of L2 regularisation and which weight initialisation technique to use. However, these were left unexplored to keep the hyperparameter search space small. MLPs are not well-suited for image tasks anyway (they do not consider the spacial structure of the pixels — you could randomly rearrange the pixels and the MLP would still yield the same performance results), so trying to explore this architectural space will likely be unfruitful.</p>
<p><strong>Note:</strong> Flattening of the input images is performed outside of the <code>MultilayerPerceptron</code> class so that the class can be reused for other inputs that do not require flattening.</p>
<div id="47b40afb" class="cell">
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MultilayerPerceptron(torch.nn.Module):</span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(</span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>,</span>
<span id="cb40-4"><a href="#cb40-4" aria-hidden="true" tabindex="-1"></a>        layer_sizes: Iterable[<span class="bu">int</span>],</span>
<span id="cb40-5"><a href="#cb40-5" aria-hidden="true" tabindex="-1"></a>        activation_constructor: Callable[[], torch.nn.Module],</span>
<span id="cb40-6"><a href="#cb40-6" aria-hidden="true" tabindex="-1"></a>    ):</span>
<span id="cb40-7"><a href="#cb40-7" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb40-8"><a href="#cb40-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.layers <span class="op">=</span> torch.nn.ModuleList(</span>
<span id="cb40-9"><a href="#cb40-9" aria-hidden="true" tabindex="-1"></a>            [</span>
<span id="cb40-10"><a href="#cb40-10" aria-hidden="true" tabindex="-1"></a>                module</span>
<span id="cb40-11"><a href="#cb40-11" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> out_features <span class="kw">in</span> layer_sizes[:<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb40-12"><a href="#cb40-12" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> module <span class="kw">in</span> (</span>
<span id="cb40-13"><a href="#cb40-13" aria-hidden="true" tabindex="-1"></a>                    torch.nn.LazyLinear(out_features),</span>
<span id="cb40-14"><a href="#cb40-14" aria-hidden="true" tabindex="-1"></a>                    torch.nn.LazyBatchNorm1d(),</span>
<span id="cb40-15"><a href="#cb40-15" aria-hidden="true" tabindex="-1"></a>                    activation_constructor(),</span>
<span id="cb40-16"><a href="#cb40-16" aria-hidden="true" tabindex="-1"></a>                )</span>
<span id="cb40-17"><a href="#cb40-17" aria-hidden="true" tabindex="-1"></a>            ]</span>
<span id="cb40-18"><a href="#cb40-18" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb40-19"><a href="#cb40-19" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.layers.append(torch.nn.LazyLinear(layer_sizes[<span class="op">-</span><span class="dv">1</span>]))</span>
<span id="cb40-20"><a href="#cb40-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-21"><a href="#cb40-21" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb40-22"><a href="#cb40-22" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> layer <span class="kw">in</span> <span class="va">self</span>.layers:</span>
<span id="cb40-23"><a href="#cb40-23" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> layer(x)</span>
<span id="cb40-24"><a href="#cb40-24" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><strong>Note:</strong> For visualisation of the architecture of the final model, please see the Training and Evaluation section.</p>
</section>
<section id="vgg-16" class="level3">
<h3 class="anchored" data-anchor-id="vgg-16">VGG-16</h3>
<p>The VGG-16 model follows the original VGG-16 architecture (configuration D) proposed in <a href="https://arxiv.org/abs/1409.1556">Very Deep Convultional Networks for Large-Scale Image Recognition</a>:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="mlp-vgg16-fashion_files/figure-html/e71dc542-1-070ade36-eeb9-401c-b3d8-565d89047370.png" class="img-fluid figure-img"></p>
<figcaption>vgg16-architecture.png</figcaption>
</figure>
</div>
<p>Although not depicted in the image above, the authors mention in the paper that dropout is applied to the first two linear layers with a probability of 0.5.</p>
<p>While my implementation follows the original architecture closely, some minor adjustments / additions were made:</p>
<ul>
<li><p>When the model is instantiated, you can specify whether to use batch normalisation after each convolutional layer. This is a hyperparameter that will be tuned. During my initial tests, I found that the model struggled to learn the features of the input images without batch normalisation (the loss very quickly plateaued). This could be because VGG-16 is a considerably deep model and thus experiences the vanishing / exploding gradient problem — batch normalisation may help stabilise the gradients.</p></li>
<li><p>The VGG-16 model provided by PyTorch adds an additional adaptive average pooling layer before flattening the last feature map for the linear layers, presumably to allow the network to handle variable input image sizes by ensuring that the dimensions of the feature map before the linear layers is always the same. I replicate that addition in my implementation for the same reason.</p></li>
</ul>
<p>To reduce code repetition, I define a function to create a single VGG block. This function is loosely based on a similar function from <a href="https://d2l.ai/chapter_convolutional-modern/vgg.html">d2l.ai</a>.</p>
<div id="b23eda51" class="cell">
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> vgg_block(n_conv_layers: <span class="bu">int</span>, out_channels: <span class="bu">int</span>, batch_norm: <span class="bu">bool</span> <span class="op">=</span> <span class="va">True</span>):</span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a>    conv_layers <span class="op">=</span> (</span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a>        module</span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(n_conv_layers)</span>
<span id="cb41-5"><a href="#cb41-5" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> module <span class="kw">in</span> (</span>
<span id="cb41-6"><a href="#cb41-6" aria-hidden="true" tabindex="-1"></a>            torch.nn.LazyConv2d(out_channels, kernel_size<span class="op">=</span><span class="dv">3</span>, padding<span class="op">=</span><span class="dv">1</span>),</span>
<span id="cb41-7"><a href="#cb41-7" aria-hidden="true" tabindex="-1"></a>            torch.nn.LazyBatchNorm2d() <span class="cf">if</span> batch_norm <span class="cf">else</span> torch.nn.Identity(),</span>
<span id="cb41-8"><a href="#cb41-8" aria-hidden="true" tabindex="-1"></a>            torch.nn.ReLU(inplace<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb41-9"><a href="#cb41-9" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb41-10"><a href="#cb41-10" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb41-11"><a href="#cb41-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-12"><a href="#cb41-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> torch.nn.Sequential(</span>
<span id="cb41-13"><a href="#cb41-13" aria-hidden="true" tabindex="-1"></a>        <span class="op">*</span>conv_layers, torch.nn.MaxPool2d(kernel_size<span class="op">=</span><span class="dv">2</span>, stride<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb41-14"><a href="#cb41-14" aria-hidden="true" tabindex="-1"></a>    )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The implementation of VGG-16 is then as follows:</p>
<div id="d552c753" class="cell">
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> VGG16(torch.nn.Module):</span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, out_classes: <span class="bu">int</span>, batch_norm: <span class="bu">bool</span> <span class="op">=</span> <span class="va">True</span>):</span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb42-4"><a href="#cb42-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.convs <span class="op">=</span> torch.nn.Sequential(</span>
<span id="cb42-5"><a href="#cb42-5" aria-hidden="true" tabindex="-1"></a>            vgg_block(n_conv_layers<span class="op">=</span><span class="dv">2</span>, out_channels<span class="op">=</span><span class="dv">64</span>, batch_norm<span class="op">=</span>batch_norm),</span>
<span id="cb42-6"><a href="#cb42-6" aria-hidden="true" tabindex="-1"></a>            vgg_block(n_conv_layers<span class="op">=</span><span class="dv">2</span>, out_channels<span class="op">=</span><span class="dv">128</span>, batch_norm<span class="op">=</span>batch_norm),</span>
<span id="cb42-7"><a href="#cb42-7" aria-hidden="true" tabindex="-1"></a>            vgg_block(n_conv_layers<span class="op">=</span><span class="dv">3</span>, out_channels<span class="op">=</span><span class="dv">256</span>, batch_norm<span class="op">=</span>batch_norm),</span>
<span id="cb42-8"><a href="#cb42-8" aria-hidden="true" tabindex="-1"></a>            vgg_block(n_conv_layers<span class="op">=</span><span class="dv">3</span>, out_channels<span class="op">=</span><span class="dv">512</span>, batch_norm<span class="op">=</span>batch_norm),</span>
<span id="cb42-9"><a href="#cb42-9" aria-hidden="true" tabindex="-1"></a>            vgg_block(n_conv_layers<span class="op">=</span><span class="dv">3</span>, out_channels<span class="op">=</span><span class="dv">512</span>, batch_norm<span class="op">=</span>batch_norm),</span>
<span id="cb42-10"><a href="#cb42-10" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb42-11"><a href="#cb42-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.avg_pool <span class="op">=</span> torch.nn.Sequential(</span>
<span id="cb42-12"><a href="#cb42-12" aria-hidden="true" tabindex="-1"></a>            torch.nn.AdaptiveAvgPool2d((<span class="dv">7</span>, <span class="dv">7</span>)),</span>
<span id="cb42-13"><a href="#cb42-13" aria-hidden="true" tabindex="-1"></a>            torch.nn.Flatten(),</span>
<span id="cb42-14"><a href="#cb42-14" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb42-15"><a href="#cb42-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fcs <span class="op">=</span> torch.nn.Sequential(</span>
<span id="cb42-16"><a href="#cb42-16" aria-hidden="true" tabindex="-1"></a>            <span class="op">*</span>(</span>
<span id="cb42-17"><a href="#cb42-17" aria-hidden="true" tabindex="-1"></a>                module</span>
<span id="cb42-18"><a href="#cb42-18" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2</span>)</span>
<span id="cb42-19"><a href="#cb42-19" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> module <span class="kw">in</span> (</span>
<span id="cb42-20"><a href="#cb42-20" aria-hidden="true" tabindex="-1"></a>                    torch.nn.LazyLinear(out_features<span class="op">=</span><span class="dv">4096</span>),</span>
<span id="cb42-21"><a href="#cb42-21" aria-hidden="true" tabindex="-1"></a>                    torch.nn.ReLU(inplace<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb42-22"><a href="#cb42-22" aria-hidden="true" tabindex="-1"></a>                    torch.nn.Dropout(p<span class="op">=</span><span class="fl">0.5</span>),</span>
<span id="cb42-23"><a href="#cb42-23" aria-hidden="true" tabindex="-1"></a>                )</span>
<span id="cb42-24"><a href="#cb42-24" aria-hidden="true" tabindex="-1"></a>            ),</span>
<span id="cb42-25"><a href="#cb42-25" aria-hidden="true" tabindex="-1"></a>            torch.nn.LazyLinear(out_features<span class="op">=</span>out_classes)</span>
<span id="cb42-26"><a href="#cb42-26" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb42-27"><a href="#cb42-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-28"><a href="#cb42-28" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, X):</span>
<span id="cb42-29"><a href="#cb42-29" aria-hidden="true" tabindex="-1"></a>        X <span class="op">=</span> <span class="va">self</span>.convs(X)</span>
<span id="cb42-30"><a href="#cb42-30" aria-hidden="true" tabindex="-1"></a>        X <span class="op">=</span> <span class="va">self</span>.avg_pool(X)</span>
<span id="cb42-31"><a href="#cb42-31" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.fcs(X)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="vgg-16-pretrained" class="level3">
<h3 class="anchored" data-anchor-id="vgg-16-pretrained">VGG-16 Pretrained</h3>
<p>For the pretrained VGG-16 model, we still need to perform a bit of surgery:</p>
<ul>
<li><p>The pretrained VGG-16 was trained to perform classification on ImageNet, which has 1000 different classes. We need to replace the last linear layer with one whose number of outputs matches the number of classes in our dataset.</p></li>
<li><p>The pretrained VGG-16 expects inputs with 3 channels, so the first convolutional layer expects that. Since I want to experiment with different numbers of input channels (e.g., grayscale images), I replace the first layer with a lazy convolutional layer (lazy means that the layer will automatically determine the number of input channels based on the first input).</p></li>
</ul>
<p>Performing these adjustments does mean that we lose some of the pretrained weights. However, most of them are still kept, so there should not be any issues.</p>
<p>The following cell shows the untouched architecture of the pretrained VGG-16 model:</p>
<div id="f44be5f8" class="cell">
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a>torchvision.models.vgg16(weights<span class="op">=</span><span class="st">"DEFAULT"</span>, progress<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-container">
<div class="cell-output cell-output-stderr">
<pre><code>Downloading: "https://download.pytorch.org/models/vgg16-397923af.pth" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth
100%|██████████| 528M/528M [00:02&lt;00:00, 205MB/s]</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="29">
<pre><code>VGG(
  (features): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU(inplace=True)
    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (3): ReLU(inplace=True)
    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (6): ReLU(inplace=True)
    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (8): ReLU(inplace=True)
    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (11): ReLU(inplace=True)
    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (13): ReLU(inplace=True)
    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (15): ReLU(inplace=True)
    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (18): ReLU(inplace=True)
    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (20): ReLU(inplace=True)
    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (22): ReLU(inplace=True)
    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (25): ReLU(inplace=True)
    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (27): ReLU(inplace=True)
    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (29): ReLU(inplace=True)
    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))
  (classifier): Sequential(
    (0): Linear(in_features=25088, out_features=4096, bias=True)
    (1): ReLU(inplace=True)
    (2): Dropout(p=0.5, inplace=False)
    (3): Linear(in_features=4096, out_features=4096, bias=True)
    (4): ReLU(inplace=True)
    (5): Dropout(p=0.5, inplace=False)
    (6): Linear(in_features=4096, out_features=1000, bias=True)
  )
)</code></pre>
</div>
</div>
</div>
<p>The function below performs the surgery operations described above:</p>
<div id="0f695258" class="cell">
<div class="sourceCode cell-code" id="cb46"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> make_vgg16_pretrained(weights: Optional[<span class="bu">str</span>] <span class="op">=</span> <span class="st">"DEFAULT"</span>, freeze_conv: <span class="bu">bool</span> <span class="op">=</span> <span class="va">False</span>) <span class="op">-&gt;</span> torchvision.models.vgg.VGG:</span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a>    vgg16_pretrained <span class="op">=</span> torchvision.models.vgg16(weights<span class="op">=</span>weights, progress<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb46-3"><a href="#cb46-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-4"><a href="#cb46-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> freeze_conv:</span>
<span id="cb46-5"><a href="#cb46-5" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Freeze the convolutional layers.</span></span>
<span id="cb46-6"><a href="#cb46-6" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> param <span class="kw">in</span> vgg16_pretrained.features.parameters():</span>
<span id="cb46-7"><a href="#cb46-7" aria-hidden="true" tabindex="-1"></a>            param.requires_grad <span class="op">=</span> <span class="va">False</span></span>
<span id="cb46-8"><a href="#cb46-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-9"><a href="#cb46-9" aria-hidden="true" tabindex="-1"></a>    vgg16_pretrained.features[<span class="dv">0</span>] <span class="op">=</span> torch.nn.LazyConv2d(<span class="dv">64</span>, kernel_size<span class="op">=</span><span class="dv">3</span>, stride<span class="op">=</span><span class="dv">1</span>, padding<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb46-10"><a href="#cb46-10" aria-hidden="true" tabindex="-1"></a>    vgg16_pretrained.classifier[<span class="dv">6</span>] <span class="op">=</span> torch.nn.LazyLinear(<span class="dv">10</span>)</span>
<span id="cb46-11"><a href="#cb46-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> vgg16_pretrained</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We can verify that the function works as intended by manually inspecting the architecture of the output model:</p>
<div id="e2f0d8c9" class="cell">
<div class="sourceCode cell-code" id="cb47"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a>make_vgg16_pretrained()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-container">
<div class="cell-output cell-output-display" data-execution_count="31">
<pre><code>VGG(
  (features): Sequential(
    (0): LazyConv2d(0, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU(inplace=True)
    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (3): ReLU(inplace=True)
    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (6): ReLU(inplace=True)
    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (8): ReLU(inplace=True)
    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (11): ReLU(inplace=True)
    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (13): ReLU(inplace=True)
    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (15): ReLU(inplace=True)
    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (18): ReLU(inplace=True)
    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (20): ReLU(inplace=True)
    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (22): ReLU(inplace=True)
    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (25): ReLU(inplace=True)
    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (27): ReLU(inplace=True)
    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (29): ReLU(inplace=True)
    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))
  (classifier): Sequential(
    (0): Linear(in_features=25088, out_features=4096, bias=True)
    (1): ReLU(inplace=True)
    (2): Dropout(p=0.5, inplace=False)
    (3): Linear(in_features=4096, out_features=4096, bias=True)
    (4): ReLU(inplace=True)
    (5): Dropout(p=0.5, inplace=False)
    (6): LazyLinear(in_features=0, out_features=10, bias=True)
  )
)</code></pre>
</div>
</div>
</div>
</section>
</section>
<section id="definitions-for-training-and-evaluation" class="level2">
<h2 class="anchored" data-anchor-id="definitions-for-training-and-evaluation">Definitions for Training and Evaluation</h2>
<p>Before proceeding further, we’ll define the necessary functions and classes needed ahead of time for hyperparameter tuning, training and evaluation so that the notebook is more readable. I suggest reading the documentation for each class to get a better understanding of their capabilities.</p>
<p>First, for evaluation:</p>
<div id="d7762acd" class="cell">
<div class="sourceCode cell-code" id="cb49"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a><span class="at">@dataclass</span></span>
<span id="cb49-2"><a href="#cb49-2" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> EvaluationResults:</span>
<span id="cb49-3"><a href="#cb49-3" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb49-4"><a href="#cb49-4" aria-hidden="true" tabindex="-1"></a><span class="co">    Stores the evaluation results of a model.</span></span>
<span id="cb49-5"><a href="#cb49-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-6"><a href="#cb49-6" aria-hidden="true" tabindex="-1"></a><span class="co">    Attributes:</span></span>
<span id="cb49-7"><a href="#cb49-7" aria-hidden="true" tabindex="-1"></a><span class="co">        mean_loss (float): The mean loss across the evaluation dataset.</span></span>
<span id="cb49-8"><a href="#cb49-8" aria-hidden="true" tabindex="-1"></a><span class="co">        classification_report (dict): A dictionary containing precision, recall,</span></span>
<span id="cb49-9"><a href="#cb49-9" aria-hidden="true" tabindex="-1"></a><span class="co">            f1-score, and support for each class. See scikit-learn's `classification_report()`.</span></span>
<span id="cb49-10"><a href="#cb49-10" aria-hidden="true" tabindex="-1"></a><span class="co">        classification_report_str (str): A string representation of the classification report.</span></span>
<span id="cb49-11"><a href="#cb49-11" aria-hidden="true" tabindex="-1"></a><span class="co">            See scikit-learn's `classification_report()`.</span></span>
<span id="cb49-12"><a href="#cb49-12" aria-hidden="true" tabindex="-1"></a><span class="co">        confusion_matrix (numpy.ndarray): A confusion matrix indicating the performance</span></span>
<span id="cb49-13"><a href="#cb49-13" aria-hidden="true" tabindex="-1"></a><span class="co">            of the model on the evaluation dataset. See scikit-learn's `confusion_matrix()`.</span></span>
<span id="cb49-14"><a href="#cb49-14" aria-hidden="true" tabindex="-1"></a><span class="co">        predictions (numpy.ndarray): An array of predicted labels for the evaluation dataset.</span></span>
<span id="cb49-15"><a href="#cb49-15" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb49-16"><a href="#cb49-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-17"><a href="#cb49-17" aria-hidden="true" tabindex="-1"></a>    mean_loss: <span class="bu">float</span></span>
<span id="cb49-18"><a href="#cb49-18" aria-hidden="true" tabindex="-1"></a>    classification_report: <span class="bu">dict</span></span>
<span id="cb49-19"><a href="#cb49-19" aria-hidden="true" tabindex="-1"></a>    classification_report_str: <span class="bu">str</span></span>
<span id="cb49-20"><a href="#cb49-20" aria-hidden="true" tabindex="-1"></a>    confusion_matrix: np.ndarray</span>
<span id="cb49-21"><a href="#cb49-21" aria-hidden="true" tabindex="-1"></a>    predictions: np.ndarray</span>
<span id="cb49-22"><a href="#cb49-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-23"><a href="#cb49-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-24"><a href="#cb49-24" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Evaluator:</span>
<span id="cb49-25"><a href="#cb49-25" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb49-26"><a href="#cb49-26" aria-hidden="true" tabindex="-1"></a><span class="co">    Evaluator for evaluating machine learning models on a dataset.</span></span>
<span id="cb49-27"><a href="#cb49-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-28"><a href="#cb49-28" aria-hidden="true" tabindex="-1"></a><span class="co">    Attributes:</span></span>
<span id="cb49-29"><a href="#cb49-29" aria-hidden="true" tabindex="-1"></a><span class="co">        batch_size (int): The size of the batches used during evaluation. Default is</span></span>
<span id="cb49-30"><a href="#cb49-30" aria-hidden="true" tabindex="-1"></a><span class="co">            set to `DEFAULT_BATCH_SIZE`.</span></span>
<span id="cb49-31"><a href="#cb49-31" aria-hidden="true" tabindex="-1"></a><span class="co">        desc (str): A short description label for the evaluation process used for</span></span>
<span id="cb49-32"><a href="#cb49-32" aria-hidden="true" tabindex="-1"></a><span class="co">            progress tracking with tqdm. Defaults to "test".</span></span>
<span id="cb49-33"><a href="#cb49-33" aria-hidden="true" tabindex="-1"></a><span class="co">        zero_division (str or int): Controls the behavior of metrics when there is</span></span>
<span id="cb49-34"><a href="#cb49-34" aria-hidden="true" tabindex="-1"></a><span class="co">            a zero division. Can be "warn", 0 or 1. Defaults to "warn".</span></span>
<span id="cb49-35"><a href="#cb49-35" aria-hidden="true" tabindex="-1"></a><span class="co">        device (str): The device on which to perform evaluation.</span></span>
<span id="cb49-36"><a href="#cb49-36" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb49-37"><a href="#cb49-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-38"><a href="#cb49-38" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(</span>
<span id="cb49-39"><a href="#cb49-39" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>,</span>
<span id="cb49-40"><a href="#cb49-40" aria-hidden="true" tabindex="-1"></a>        batch_size: <span class="bu">int</span> <span class="op">=</span> DEFAULT_BATCH_SIZE,</span>
<span id="cb49-41"><a href="#cb49-41" aria-hidden="true" tabindex="-1"></a>        desc: <span class="bu">str</span> <span class="op">=</span> <span class="st">"test"</span>,</span>
<span id="cb49-42"><a href="#cb49-42" aria-hidden="true" tabindex="-1"></a>        zero_division<span class="op">=</span><span class="st">"warn"</span>,</span>
<span id="cb49-43"><a href="#cb49-43" aria-hidden="true" tabindex="-1"></a>        device: <span class="bu">str</span> <span class="op">=</span> DEVICE,</span>
<span id="cb49-44"><a href="#cb49-44" aria-hidden="true" tabindex="-1"></a>    ):</span>
<span id="cb49-45"><a href="#cb49-45" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb49-46"><a href="#cb49-46" aria-hidden="true" tabindex="-1"></a><span class="co">        Initialises the Evaluator with the specified parameters.</span></span>
<span id="cb49-47"><a href="#cb49-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-48"><a href="#cb49-48" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb49-49"><a href="#cb49-49" aria-hidden="true" tabindex="-1"></a><span class="co">            batch_size (int): The size of the batches to use during evaluation.</span></span>
<span id="cb49-50"><a href="#cb49-50" aria-hidden="true" tabindex="-1"></a><span class="co">            desc (str): A description for the evaluation process.</span></span>
<span id="cb49-51"><a href="#cb49-51" aria-hidden="true" tabindex="-1"></a><span class="co">            zero_division (str or int): Specifies the behavior when there is a zero division.</span></span>
<span id="cb49-52"><a href="#cb49-52" aria-hidden="true" tabindex="-1"></a><span class="co">            device (str): The device for evaluation.</span></span>
<span id="cb49-53"><a href="#cb49-53" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb49-54"><a href="#cb49-54" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.batch_size: <span class="bu">int</span> <span class="op">=</span> batch_size</span>
<span id="cb49-55"><a href="#cb49-55" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.desc <span class="op">=</span> desc</span>
<span id="cb49-56"><a href="#cb49-56" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.zero_division <span class="op">=</span> zero_division</span>
<span id="cb49-57"><a href="#cb49-57" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.device: <span class="bu">str</span> <span class="op">=</span> device</span>
<span id="cb49-58"><a href="#cb49-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-59"><a href="#cb49-59" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> evaluate(</span>
<span id="cb49-60"><a href="#cb49-60" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>,</span>
<span id="cb49-61"><a href="#cb49-61" aria-hidden="true" tabindex="-1"></a>        model: torch.nn.Module,</span>
<span id="cb49-62"><a href="#cb49-62" aria-hidden="true" tabindex="-1"></a>        ds_test: Dataset,</span>
<span id="cb49-63"><a href="#cb49-63" aria-hidden="true" tabindex="-1"></a>        loss_fn: torch.nn.Module,</span>
<span id="cb49-64"><a href="#cb49-64" aria-hidden="true" tabindex="-1"></a>        transform: Optional[torchvision.transforms.v2.Transform] <span class="op">=</span> <span class="va">None</span>,</span>
<span id="cb49-65"><a href="#cb49-65" aria-hidden="true" tabindex="-1"></a>        target_labels: Optional[Iterable[<span class="bu">str</span>]] <span class="op">=</span> <span class="va">None</span>,</span>
<span id="cb49-66"><a href="#cb49-66" aria-hidden="true" tabindex="-1"></a>    ) <span class="op">-&gt;</span> EvaluationResults:</span>
<span id="cb49-67"><a href="#cb49-67" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb49-68"><a href="#cb49-68" aria-hidden="true" tabindex="-1"></a><span class="co">        Evaluates the specified model on the given test dataset.</span></span>
<span id="cb49-69"><a href="#cb49-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-70"><a href="#cb49-70" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb49-71"><a href="#cb49-71" aria-hidden="true" tabindex="-1"></a><span class="co">            model (torch.nn.Module): The model to be evaluated.</span></span>
<span id="cb49-72"><a href="#cb49-72" aria-hidden="true" tabindex="-1"></a><span class="co">            ds_test (Dataset): The test dataset to evaluate the model on.</span></span>
<span id="cb49-73"><a href="#cb49-73" aria-hidden="true" tabindex="-1"></a><span class="co">            loss_fn (torch.nn.Module): The loss function to compute the loss.</span></span>
<span id="cb49-74"><a href="#cb49-74" aria-hidden="true" tabindex="-1"></a><span class="co">            transform (Optional[torchvision.transforms.v2.Transform]): Optional</span></span>
<span id="cb49-75"><a href="#cb49-75" aria-hidden="true" tabindex="-1"></a><span class="co">                transformations to apply to the input data before passing to the model.</span></span>
<span id="cb49-76"><a href="#cb49-76" aria-hidden="true" tabindex="-1"></a><span class="co">            target_labels (Optional[Iterable[str]]): Optional list of target labels</span></span>
<span id="cb49-77"><a href="#cb49-77" aria-hidden="true" tabindex="-1"></a><span class="co">                for classification report.</span></span>
<span id="cb49-78"><a href="#cb49-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-79"><a href="#cb49-79" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns:</span></span>
<span id="cb49-80"><a href="#cb49-80" aria-hidden="true" tabindex="-1"></a><span class="co">            EvaluationResults: Contains the evaluation metrics and predictions.</span></span>
<span id="cb49-81"><a href="#cb49-81" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb49-82"><a href="#cb49-82" aria-hidden="true" tabindex="-1"></a>        model.<span class="bu">eval</span>()</span>
<span id="cb49-83"><a href="#cb49-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-84"><a href="#cb49-84" aria-hidden="true" tabindex="-1"></a>        dl_test <span class="op">=</span> DataLoader(ds_test, batch_size<span class="op">=</span><span class="va">self</span>.batch_size, shuffle<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb49-85"><a href="#cb49-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-86"><a href="#cb49-86" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Sum of the losses from each batch for the current epoch.</span></span>
<span id="cb49-87"><a href="#cb49-87" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Used to calculate the mean loss for the current epoch.</span></span>
<span id="cb49-88"><a href="#cb49-88" aria-hidden="true" tabindex="-1"></a>        total_loss: <span class="bu">float</span> <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb49-89"><a href="#cb49-89" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> torch.no_grad(), tqdm(total<span class="op">=</span><span class="bu">len</span>(dl_test.dataset)) <span class="im">as</span> pbar:</span>
<span id="cb49-90"><a href="#cb49-90" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Tensor of all predictions.</span></span>
<span id="cb49-91"><a href="#cb49-91" aria-hidden="true" tabindex="-1"></a>            truth_all: torch.Tensor <span class="op">=</span> torch.tensor([], dtype<span class="op">=</span><span class="bu">int</span>)</span>
<span id="cb49-92"><a href="#cb49-92" aria-hidden="true" tabindex="-1"></a>            pred_all: torch.Tensor <span class="op">=</span> torch.tensor([], dtype<span class="op">=</span><span class="bu">int</span>)</span>
<span id="cb49-93"><a href="#cb49-93" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-94"><a href="#cb49-94" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> batch, (X, y) <span class="kw">in</span> <span class="bu">enumerate</span>(dl_test):</span>
<span id="cb49-95"><a href="#cb49-95" aria-hidden="true" tabindex="-1"></a>                pbar.set_description(<span class="ss">f"</span><span class="sc">{</span><span class="va">self</span><span class="sc">.</span>desc<span class="sc">}</span><span class="ss"> [batch </span><span class="sc">{</span>batch <span class="op">+</span> <span class="dv">1</span><span class="sc">}</span><span class="ss">]"</span>)</span>
<span id="cb49-96"><a href="#cb49-96" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-97"><a href="#cb49-97" aria-hidden="true" tabindex="-1"></a>                truth_all <span class="op">=</span> torch.cat((truth_all, y))</span>
<span id="cb49-98"><a href="#cb49-98" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-99"><a href="#cb49-99" aria-hidden="true" tabindex="-1"></a>                X <span class="op">=</span> X.to(<span class="va">self</span>.device)</span>
<span id="cb49-100"><a href="#cb49-100" aria-hidden="true" tabindex="-1"></a>                y <span class="op">=</span> y.to(<span class="va">self</span>.device)</span>
<span id="cb49-101"><a href="#cb49-101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-102"><a href="#cb49-102" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> transform <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb49-103"><a href="#cb49-103" aria-hidden="true" tabindex="-1"></a>                    X <span class="op">=</span> transform(X)</span>
<span id="cb49-104"><a href="#cb49-104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-105"><a href="#cb49-105" aria-hidden="true" tabindex="-1"></a>                unnormalised_logits <span class="op">=</span> model(X)</span>
<span id="cb49-106"><a href="#cb49-106" aria-hidden="true" tabindex="-1"></a>                pred_indices <span class="op">=</span> unnormalised_logits.argmax(dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb49-107"><a href="#cb49-107" aria-hidden="true" tabindex="-1"></a>                pred_all <span class="op">=</span> torch.cat((pred_all, pred_indices.cpu()))</span>
<span id="cb49-108"><a href="#cb49-108" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-109"><a href="#cb49-109" aria-hidden="true" tabindex="-1"></a>                loss <span class="op">=</span> loss_fn(unnormalised_logits, y)</span>
<span id="cb49-110"><a href="#cb49-110" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-111"><a href="#cb49-111" aria-hidden="true" tabindex="-1"></a>                <span class="kw">del</span> X</span>
<span id="cb49-112"><a href="#cb49-112" aria-hidden="true" tabindex="-1"></a>                <span class="kw">del</span> y</span>
<span id="cb49-113"><a href="#cb49-113" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-114"><a href="#cb49-114" aria-hidden="true" tabindex="-1"></a>                total_loss <span class="op">+=</span> loss.item()</span>
<span id="cb49-115"><a href="#cb49-115" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-116"><a href="#cb49-116" aria-hidden="true" tabindex="-1"></a>                pbar.update(dl_test.batch_size)</span>
<span id="cb49-117"><a href="#cb49-117" aria-hidden="true" tabindex="-1"></a>                pbar.set_postfix({<span class="st">"loss"</span>: loss.item()})</span>
<span id="cb49-118"><a href="#cb49-118" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-119"><a href="#cb49-119" aria-hidden="true" tabindex="-1"></a>            gc.collect()</span>
<span id="cb49-120"><a href="#cb49-120" aria-hidden="true" tabindex="-1"></a>            torch.cuda.empty_cache()</span>
<span id="cb49-121"><a href="#cb49-121" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-122"><a href="#cb49-122" aria-hidden="true" tabindex="-1"></a>            mean_loss: <span class="bu">float</span> <span class="op">=</span> total_loss <span class="op">/</span> <span class="bu">len</span>(dl_test)</span>
<span id="cb49-123"><a href="#cb49-123" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-124"><a href="#cb49-124" aria-hidden="true" tabindex="-1"></a>            report_kwargs <span class="op">=</span> {<span class="st">"output_dict"</span>: <span class="va">True</span>}</span>
<span id="cb49-125"><a href="#cb49-125" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> target_labels <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb49-126"><a href="#cb49-126" aria-hidden="true" tabindex="-1"></a>                report_kwargs[<span class="st">"target_names"</span>] <span class="op">=</span> target_labels</span>
<span id="cb49-127"><a href="#cb49-127" aria-hidden="true" tabindex="-1"></a>            report <span class="op">=</span> sklearn.metrics.classification_report(</span>
<span id="cb49-128"><a href="#cb49-128" aria-hidden="true" tabindex="-1"></a>                truth_all.cpu(),  <span class="co"># sklearn can only work with CPU tensors.</span></span>
<span id="cb49-129"><a href="#cb49-129" aria-hidden="true" tabindex="-1"></a>                pred_all.cpu(),</span>
<span id="cb49-130"><a href="#cb49-130" aria-hidden="true" tabindex="-1"></a>                zero_division<span class="op">=</span><span class="va">self</span>.zero_division,</span>
<span id="cb49-131"><a href="#cb49-131" aria-hidden="true" tabindex="-1"></a>                <span class="op">**</span>report_kwargs,</span>
<span id="cb49-132"><a href="#cb49-132" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb49-133"><a href="#cb49-133" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-134"><a href="#cb49-134" aria-hidden="true" tabindex="-1"></a>            pbar.set_postfix(</span>
<span id="cb49-135"><a href="#cb49-135" aria-hidden="true" tabindex="-1"></a>                {<span class="st">"mean loss"</span>: mean_loss, <span class="st">"macro f1"</span>: report[<span class="st">"macro avg"</span>][<span class="st">"f1-score"</span>]}</span>
<span id="cb49-136"><a href="#cb49-136" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb49-137"><a href="#cb49-137" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-138"><a href="#cb49-138" aria-hidden="true" tabindex="-1"></a>        report_str_kwargs <span class="op">=</span> {}</span>
<span id="cb49-139"><a href="#cb49-139" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> target_labels <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb49-140"><a href="#cb49-140" aria-hidden="true" tabindex="-1"></a>            report_str_kwargs[<span class="st">"target_names"</span>] <span class="op">=</span> target_labels</span>
<span id="cb49-141"><a href="#cb49-141" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-142"><a href="#cb49-142" aria-hidden="true" tabindex="-1"></a>        report_str <span class="op">=</span> sklearn.metrics.classification_report(</span>
<span id="cb49-143"><a href="#cb49-143" aria-hidden="true" tabindex="-1"></a>            truth_all.cpu(),  <span class="co"># sklearn can only work with CPU tensors.</span></span>
<span id="cb49-144"><a href="#cb49-144" aria-hidden="true" tabindex="-1"></a>            pred_all.cpu(),</span>
<span id="cb49-145"><a href="#cb49-145" aria-hidden="true" tabindex="-1"></a>            zero_division<span class="op">=</span><span class="va">self</span>.zero_division,</span>
<span id="cb49-146"><a href="#cb49-146" aria-hidden="true" tabindex="-1"></a>            <span class="op">**</span>report_str_kwargs,</span>
<span id="cb49-147"><a href="#cb49-147" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb49-148"><a href="#cb49-148" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-149"><a href="#cb49-149" aria-hidden="true" tabindex="-1"></a>        confusion_matrix <span class="op">=</span> sklearn.metrics.confusion_matrix(</span>
<span id="cb49-150"><a href="#cb49-150" aria-hidden="true" tabindex="-1"></a>            truth_all.cpu(),  <span class="co"># sklearn can only work with CPU tensors.</span></span>
<span id="cb49-151"><a href="#cb49-151" aria-hidden="true" tabindex="-1"></a>            pred_all.cpu(),</span>
<span id="cb49-152"><a href="#cb49-152" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb49-153"><a href="#cb49-153" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-154"><a href="#cb49-154" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> EvaluationResults(</span>
<span id="cb49-155"><a href="#cb49-155" aria-hidden="true" tabindex="-1"></a>            mean_loss,</span>
<span id="cb49-156"><a href="#cb49-156" aria-hidden="true" tabindex="-1"></a>            report,</span>
<span id="cb49-157"><a href="#cb49-157" aria-hidden="true" tabindex="-1"></a>            report_str,</span>
<span id="cb49-158"><a href="#cb49-158" aria-hidden="true" tabindex="-1"></a>            confusion_matrix,</span>
<span id="cb49-159"><a href="#cb49-159" aria-hidden="true" tabindex="-1"></a>            pred_all.detach().cpu().numpy(),</span>
<span id="cb49-160"><a href="#cb49-160" aria-hidden="true" tabindex="-1"></a>        )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>For training, I use early stopping based on the validation loss to prevent overfitting and save time if the model’s performance starts to plateau. If the observed validation loss does not improve after a specified number of consecutive epochs (patience), the training process is stopped. The delta parameter specifies the minimum increase in validation loss for it to be counted as a lack of improvement.</p>
<div id="767788e4" class="cell">
<div class="sourceCode cell-code" id="cb50"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ValidationLossEarlyStopper:</span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb50-3"><a href="#cb50-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Implements early stopping based on validation loss to prevent overfitting</span></span>
<span id="cb50-4"><a href="#cb50-4" aria-hidden="true" tabindex="-1"></a><span class="co">    during model training.</span></span>
<span id="cb50-5"><a href="#cb50-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-6"><a href="#cb50-6" aria-hidden="true" tabindex="-1"></a><span class="co">    The training process is halted when no improvement in validation loss is observed</span></span>
<span id="cb50-7"><a href="#cb50-7" aria-hidden="true" tabindex="-1"></a><span class="co">    for a specified number of consecutive epochs (patience).</span></span>
<span id="cb50-8"><a href="#cb50-8" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb50-9"><a href="#cb50-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-10"><a href="#cb50-10" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, patience: <span class="bu">int</span> <span class="op">=</span> <span class="dv">1</span>, delta: <span class="bu">float</span> <span class="op">=</span> <span class="fl">0.0</span>):</span>
<span id="cb50-11"><a href="#cb50-11" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb50-12"><a href="#cb50-12" aria-hidden="true" tabindex="-1"></a><span class="co">        Initialises the early stopper with the specified patience and delta values.</span></span>
<span id="cb50-13"><a href="#cb50-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-14"><a href="#cb50-14" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb50-15"><a href="#cb50-15" aria-hidden="true" tabindex="-1"></a><span class="co">            patience (int): Number of epochs to wait for an improvement before stopping.</span></span>
<span id="cb50-16"><a href="#cb50-16" aria-hidden="true" tabindex="-1"></a><span class="co">                Default is 1.</span></span>
<span id="cb50-17"><a href="#cb50-17" aria-hidden="true" tabindex="-1"></a><span class="co">            delta (float): The minimum increase in validation loss required to count as a</span></span>
<span id="cb50-18"><a href="#cb50-18" aria-hidden="true" tabindex="-1"></a><span class="co">                deterioration. Default is 0.0.</span></span>
<span id="cb50-19"><a href="#cb50-19" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb50-20"><a href="#cb50-20" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._patience: <span class="bu">int</span> <span class="op">=</span> patience</span>
<span id="cb50-21"><a href="#cb50-21" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._delta: <span class="bu">float</span> <span class="op">=</span> delta</span>
<span id="cb50-22"><a href="#cb50-22" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._lapse_count: <span class="bu">int</span> <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb50-23"><a href="#cb50-23" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._min_val_loss: <span class="bu">float</span> <span class="op">=</span> math.inf</span>
<span id="cb50-24"><a href="#cb50-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-25"><a href="#cb50-25" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> should_stop(<span class="va">self</span>, val_loss: <span class="bu">float</span>) <span class="op">-&gt;</span> <span class="bu">bool</span>:</span>
<span id="cb50-26"><a href="#cb50-26" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb50-27"><a href="#cb50-27" aria-hidden="true" tabindex="-1"></a><span class="co">        Checks whether training should stop based on the given validation loss.</span></span>
<span id="cb50-28"><a href="#cb50-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-29"><a href="#cb50-29" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb50-30"><a href="#cb50-30" aria-hidden="true" tabindex="-1"></a><span class="co">            val_loss (float): The current validation loss for the current epoch.</span></span>
<span id="cb50-31"><a href="#cb50-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-32"><a href="#cb50-32" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns:</span></span>
<span id="cb50-33"><a href="#cb50-33" aria-hidden="true" tabindex="-1"></a><span class="co">            bool: True if the training process should stop due to lack of improvement in</span></span>
<span id="cb50-34"><a href="#cb50-34" aria-hidden="true" tabindex="-1"></a><span class="co">            validation loss; otherwise, False.</span></span>
<span id="cb50-35"><a href="#cb50-35" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb50-36"><a href="#cb50-36" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> val_loss <span class="op">&lt;</span> <span class="va">self</span>._min_val_loss:</span>
<span id="cb50-37"><a href="#cb50-37" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>._min_val_loss <span class="op">=</span> val_loss</span>
<span id="cb50-38"><a href="#cb50-38" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>._lapse_count <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb50-39"><a href="#cb50-39" aria-hidden="true" tabindex="-1"></a>        <span class="cf">elif</span> val_loss <span class="op">&gt;</span> (<span class="va">self</span>._min_val_loss <span class="op">+</span> <span class="va">self</span>._delta):</span>
<span id="cb50-40"><a href="#cb50-40" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Validation loss went up by more than delta from the min.</span></span>
<span id="cb50-41"><a href="#cb50-41" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>._lapse_count <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb50-42"><a href="#cb50-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-43"><a href="#cb50-43" aria-hidden="true" tabindex="-1"></a>            <span class="co"># If the number of times the validation loss got worse</span></span>
<span id="cb50-44"><a href="#cb50-44" aria-hidden="true" tabindex="-1"></a>            <span class="co"># exceeds the patience level, then we should stop.</span></span>
<span id="cb50-45"><a href="#cb50-45" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="va">self</span>._lapse_count <span class="op">&gt;=</span> <span class="va">self</span>._patience:</span>
<span id="cb50-46"><a href="#cb50-46" aria-hidden="true" tabindex="-1"></a>                <span class="cf">return</span> <span class="va">True</span></span>
<span id="cb50-47"><a href="#cb50-47" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">False</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Finally, for the actual training of the models, I define a <code>Trainer</code> class. Notably, the <code>Trainer</code> class allows specifying transforms for data augmentation — this will be used to introduce more varied data.</p>
<div id="f5711a17" class="cell">
<div class="sourceCode cell-code" id="cb51"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a>ParamsT <span class="op">=</span> Iterable[torch.Tensor] <span class="op">|</span> Iterable[<span class="bu">dict</span>[<span class="bu">str</span>, Any]]</span>
<span id="cb51-2"><a href="#cb51-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-3"><a href="#cb51-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-4"><a href="#cb51-4" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Trainer:</span>
<span id="cb51-5"><a href="#cb51-5" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb51-6"><a href="#cb51-6" aria-hidden="true" tabindex="-1"></a><span class="co">    Trains a PyTorch model over multiple epochs.</span></span>
<span id="cb51-7"><a href="#cb51-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-8"><a href="#cb51-8" aria-hidden="true" tabindex="-1"></a><span class="co">    Attributes:</span></span>
<span id="cb51-9"><a href="#cb51-9" aria-hidden="true" tabindex="-1"></a><span class="co">        epochs (int): The total number of epochs to train the model.</span></span>
<span id="cb51-10"><a href="#cb51-10" aria-hidden="true" tabindex="-1"></a><span class="co">        batch_size (int): The number of samples per gradient update.</span></span>
<span id="cb51-11"><a href="#cb51-11" aria-hidden="true" tabindex="-1"></a><span class="co">        augment_transform (Optional[torchvision.transforms.v2.Transform]): Transformations to apply</span></span>
<span id="cb51-12"><a href="#cb51-12" aria-hidden="true" tabindex="-1"></a><span class="co">            for data augmentation during training.</span></span>
<span id="cb51-13"><a href="#cb51-13" aria-hidden="true" tabindex="-1"></a><span class="co">        train_loss_hook (Optional[Callable[[float, int], None]]): A callback function that is called</span></span>
<span id="cb51-14"><a href="#cb51-14" aria-hidden="true" tabindex="-1"></a><span class="co">            after each training epoch to log the training loss.</span></span>
<span id="cb51-15"><a href="#cb51-15" aria-hidden="true" tabindex="-1"></a><span class="co">        val_results_hook (Optional[Callable[[EvaluationResults, int], None]]): A callback function that</span></span>
<span id="cb51-16"><a href="#cb51-16" aria-hidden="true" tabindex="-1"></a><span class="co">            is called after each validation epoch to log validation results.</span></span>
<span id="cb51-17"><a href="#cb51-17" aria-hidden="true" tabindex="-1"></a><span class="co">        device (str): The device to run the model on.</span></span>
<span id="cb51-18"><a href="#cb51-18" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb51-19"><a href="#cb51-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-20"><a href="#cb51-20" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(</span>
<span id="cb51-21"><a href="#cb51-21" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>,</span>
<span id="cb51-22"><a href="#cb51-22" aria-hidden="true" tabindex="-1"></a>        epochs: <span class="bu">int</span>,</span>
<span id="cb51-23"><a href="#cb51-23" aria-hidden="true" tabindex="-1"></a>        batch_size: <span class="bu">int</span> <span class="op">=</span> DEFAULT_BATCH_SIZE,</span>
<span id="cb51-24"><a href="#cb51-24" aria-hidden="true" tabindex="-1"></a>        augment_transform: Optional[torchvision.transforms.v2.Transform] <span class="op">=</span> <span class="va">None</span>,</span>
<span id="cb51-25"><a href="#cb51-25" aria-hidden="true" tabindex="-1"></a>        train_loss_hook: Optional[Callable[[<span class="bu">float</span>, <span class="bu">int</span>], <span class="va">None</span>]] <span class="op">=</span> <span class="va">None</span>,</span>
<span id="cb51-26"><a href="#cb51-26" aria-hidden="true" tabindex="-1"></a>        val_results_hook: Optional[Callable[[EvaluationResults, <span class="bu">int</span>], <span class="va">None</span>]] <span class="op">=</span> <span class="va">None</span>,</span>
<span id="cb51-27"><a href="#cb51-27" aria-hidden="true" tabindex="-1"></a>        device: <span class="bu">str</span> <span class="op">=</span> DEVICE,</span>
<span id="cb51-28"><a href="#cb51-28" aria-hidden="true" tabindex="-1"></a>    ):</span>
<span id="cb51-29"><a href="#cb51-29" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb51-30"><a href="#cb51-30" aria-hidden="true" tabindex="-1"></a><span class="co">        Initialises the Trainer with the specified parameters.</span></span>
<span id="cb51-31"><a href="#cb51-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-32"><a href="#cb51-32" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb51-33"><a href="#cb51-33" aria-hidden="true" tabindex="-1"></a><span class="co">            epochs (int): The total number of epochs to train the model.</span></span>
<span id="cb51-34"><a href="#cb51-34" aria-hidden="true" tabindex="-1"></a><span class="co">            batch_size (int): The number of samples per gradient update. Default is `DEFAULT_BATCH_SIZE`.</span></span>
<span id="cb51-35"><a href="#cb51-35" aria-hidden="true" tabindex="-1"></a><span class="co">            augment_transform (Optional[torchvision.transforms.v2.Transform]): Transformations to apply</span></span>
<span id="cb51-36"><a href="#cb51-36" aria-hidden="true" tabindex="-1"></a><span class="co">                for data augmentation during training. Default is None.</span></span>
<span id="cb51-37"><a href="#cb51-37" aria-hidden="true" tabindex="-1"></a><span class="co">            train_loss_hook (Optional[Callable[[float, int], None]]): A callback function to log</span></span>
<span id="cb51-38"><a href="#cb51-38" aria-hidden="true" tabindex="-1"></a><span class="co">                training loss after each epoch. Default is None.</span></span>
<span id="cb51-39"><a href="#cb51-39" aria-hidden="true" tabindex="-1"></a><span class="co">            val_results_hook (Optional[Callable[[EvaluationResults, int], None]]): A callback function</span></span>
<span id="cb51-40"><a href="#cb51-40" aria-hidden="true" tabindex="-1"></a><span class="co">                to log validation results after each epoch. Default is None.</span></span>
<span id="cb51-41"><a href="#cb51-41" aria-hidden="true" tabindex="-1"></a><span class="co">            device (str): The device to run the model on. Default is `DEVICE`.</span></span>
<span id="cb51-42"><a href="#cb51-42" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb51-43"><a href="#cb51-43" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.epochs: <span class="bu">int</span> <span class="op">=</span> epochs</span>
<span id="cb51-44"><a href="#cb51-44" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.batch_size: <span class="bu">int</span> <span class="op">=</span> batch_size</span>
<span id="cb51-45"><a href="#cb51-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-46"><a href="#cb51-46" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.augment_transform: Optional[torchvision.transforms.v2.Transform] <span class="op">=</span> (</span>
<span id="cb51-47"><a href="#cb51-47" aria-hidden="true" tabindex="-1"></a>            augment_transform</span>
<span id="cb51-48"><a href="#cb51-48" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb51-49"><a href="#cb51-49" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.train_loss_hook: Optional[Callable[[<span class="bu">float</span>, <span class="bu">int</span>], <span class="va">None</span>]] <span class="op">=</span> train_loss_hook</span>
<span id="cb51-50"><a href="#cb51-50" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.val_results_hook: Optional[Callable[[EvaluationResults, <span class="bu">int</span>], <span class="va">None</span>]] <span class="op">=</span> (</span>
<span id="cb51-51"><a href="#cb51-51" aria-hidden="true" tabindex="-1"></a>            val_results_hook</span>
<span id="cb51-52"><a href="#cb51-52" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb51-53"><a href="#cb51-53" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.device: <span class="bu">str</span> <span class="op">=</span> device</span>
<span id="cb51-54"><a href="#cb51-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-55"><a href="#cb51-55" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> train(</span>
<span id="cb51-56"><a href="#cb51-56" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>,</span>
<span id="cb51-57"><a href="#cb51-57" aria-hidden="true" tabindex="-1"></a>        model: torch.nn.Module,</span>
<span id="cb51-58"><a href="#cb51-58" aria-hidden="true" tabindex="-1"></a>        ds_train: Dataset,</span>
<span id="cb51-59"><a href="#cb51-59" aria-hidden="true" tabindex="-1"></a>        loss_fn: torch.nn.Module,</span>
<span id="cb51-60"><a href="#cb51-60" aria-hidden="true" tabindex="-1"></a>        optimiser: torch.optim.Optimizer,</span>
<span id="cb51-61"><a href="#cb51-61" aria-hidden="true" tabindex="-1"></a>        train_sampler: Optional[torch.utils.data.Sampler] <span class="op">=</span> <span class="va">None</span>,</span>
<span id="cb51-62"><a href="#cb51-62" aria-hidden="true" tabindex="-1"></a>        transform: Optional[torchvision.transforms.v2.Transform] <span class="op">=</span> <span class="va">None</span>,</span>
<span id="cb51-63"><a href="#cb51-63" aria-hidden="true" tabindex="-1"></a>        ds_val: Optional[Dataset] <span class="op">=</span> <span class="va">None</span>,</span>
<span id="cb51-64"><a href="#cb51-64" aria-hidden="true" tabindex="-1"></a>        scheduler: Optional[torch.optim.lr_scheduler.ReduceLROnPlateau] <span class="op">=</span> <span class="va">None</span>,</span>
<span id="cb51-65"><a href="#cb51-65" aria-hidden="true" tabindex="-1"></a>        early_stop: Optional[ValidationLossEarlyStopper] <span class="op">=</span> <span class="va">None</span>,</span>
<span id="cb51-66"><a href="#cb51-66" aria-hidden="true" tabindex="-1"></a>    ):</span>
<span id="cb51-67"><a href="#cb51-67" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb51-68"><a href="#cb51-68" aria-hidden="true" tabindex="-1"></a><span class="co">        Trains the given model for a specified number of epochs.</span></span>
<span id="cb51-69"><a href="#cb51-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-70"><a href="#cb51-70" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb51-71"><a href="#cb51-71" aria-hidden="true" tabindex="-1"></a><span class="co">            model (torch.nn.Module): The PyTorch model to train.</span></span>
<span id="cb51-72"><a href="#cb51-72" aria-hidden="true" tabindex="-1"></a><span class="co">            ds_train (Dataset): The training dataset.</span></span>
<span id="cb51-73"><a href="#cb51-73" aria-hidden="true" tabindex="-1"></a><span class="co">            loss_fn (torch.nn.Module): The loss function to use for training.</span></span>
<span id="cb51-74"><a href="#cb51-74" aria-hidden="true" tabindex="-1"></a><span class="co">            optimiser (torch.optim.Optimizer): The optimiser for updating model weights.</span></span>
<span id="cb51-75"><a href="#cb51-75" aria-hidden="true" tabindex="-1"></a><span class="co">            train_sampler (Optional[torch.utils.data.Sampler]): An optional sampler for the training dataset.</span></span>
<span id="cb51-76"><a href="#cb51-76" aria-hidden="true" tabindex="-1"></a><span class="co">            transform (Optional[torchvision.transforms.v2.Transform]): Optional transformations to apply</span></span>
<span id="cb51-77"><a href="#cb51-77" aria-hidden="true" tabindex="-1"></a><span class="co">                to the training data before feeding them to the model.</span></span>
<span id="cb51-78"><a href="#cb51-78" aria-hidden="true" tabindex="-1"></a><span class="co">            ds_val (Optional[Dataset]): An optional validation dataset for validating the model.</span></span>
<span id="cb51-79"><a href="#cb51-79" aria-hidden="true" tabindex="-1"></a><span class="co">            scheduler (Optional[torch.optim.lr_scheduler.ReduceLROnPlateau]): Optional learning rate scheduler</span></span>
<span id="cb51-80"><a href="#cb51-80" aria-hidden="true" tabindex="-1"></a><span class="co">                to adjust the learning rate based on validation loss.</span></span>
<span id="cb51-81"><a href="#cb51-81" aria-hidden="true" tabindex="-1"></a><span class="co">            early_stop (Optional[ValidationLossEarlyStopper]): An optional early stopper to terminate training</span></span>
<span id="cb51-82"><a href="#cb51-82" aria-hidden="true" tabindex="-1"></a><span class="co">                if validation loss does not improve.</span></span>
<span id="cb51-83"><a href="#cb51-83" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb51-84"><a href="#cb51-84" aria-hidden="true" tabindex="-1"></a>        model.to(<span class="va">self</span>.device)</span>
<span id="cb51-85"><a href="#cb51-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-86"><a href="#cb51-86" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Create the data loader for the training set.</span></span>
<span id="cb51-87"><a href="#cb51-87" aria-hidden="true" tabindex="-1"></a>        <span class="co"># If a sampler was specified, use it.</span></span>
<span id="cb51-88"><a href="#cb51-88" aria-hidden="true" tabindex="-1"></a>        dl_train <span class="op">=</span> DataLoader(</span>
<span id="cb51-89"><a href="#cb51-89" aria-hidden="true" tabindex="-1"></a>            ds_train,</span>
<span id="cb51-90"><a href="#cb51-90" aria-hidden="true" tabindex="-1"></a>            batch_size<span class="op">=</span><span class="va">self</span>.batch_size,</span>
<span id="cb51-91"><a href="#cb51-91" aria-hidden="true" tabindex="-1"></a>            sampler<span class="op">=</span>train_sampler,</span>
<span id="cb51-92"><a href="#cb51-92" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb51-93"><a href="#cb51-93" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-94"><a href="#cb51-94" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Training loop.</span></span>
<span id="cb51-95"><a href="#cb51-95" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(<span class="va">self</span>.epochs):</span>
<span id="cb51-96"><a href="#cb51-96" aria-hidden="true" tabindex="-1"></a>            train_loss <span class="op">=</span> <span class="va">self</span>._train_one_epoch(</span>
<span id="cb51-97"><a href="#cb51-97" aria-hidden="true" tabindex="-1"></a>                model,</span>
<span id="cb51-98"><a href="#cb51-98" aria-hidden="true" tabindex="-1"></a>                dl_train,</span>
<span id="cb51-99"><a href="#cb51-99" aria-hidden="true" tabindex="-1"></a>                loss_fn,</span>
<span id="cb51-100"><a href="#cb51-100" aria-hidden="true" tabindex="-1"></a>                optimiser,</span>
<span id="cb51-101"><a href="#cb51-101" aria-hidden="true" tabindex="-1"></a>                epoch,</span>
<span id="cb51-102"><a href="#cb51-102" aria-hidden="true" tabindex="-1"></a>                transform<span class="op">=</span>transform,</span>
<span id="cb51-103"><a href="#cb51-103" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb51-104"><a href="#cb51-104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-105"><a href="#cb51-105" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="va">self</span>.train_loss_hook <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb51-106"><a href="#cb51-106" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>.train_loss_hook(train_loss, epoch)</span>
<span id="cb51-107"><a href="#cb51-107" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-108"><a href="#cb51-108" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> ds_val <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb51-109"><a href="#cb51-109" aria-hidden="true" tabindex="-1"></a>                val_results <span class="op">=</span> <span class="va">self</span>._validate_one_epoch(</span>
<span id="cb51-110"><a href="#cb51-110" aria-hidden="true" tabindex="-1"></a>                    model,</span>
<span id="cb51-111"><a href="#cb51-111" aria-hidden="true" tabindex="-1"></a>                    ds_val,</span>
<span id="cb51-112"><a href="#cb51-112" aria-hidden="true" tabindex="-1"></a>                    loss_fn,</span>
<span id="cb51-113"><a href="#cb51-113" aria-hidden="true" tabindex="-1"></a>                    epoch,</span>
<span id="cb51-114"><a href="#cb51-114" aria-hidden="true" tabindex="-1"></a>                    transform<span class="op">=</span>transform,</span>
<span id="cb51-115"><a href="#cb51-115" aria-hidden="true" tabindex="-1"></a>                )</span>
<span id="cb51-116"><a href="#cb51-116" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-117"><a href="#cb51-117" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> <span class="va">self</span>.val_results_hook <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb51-118"><a href="#cb51-118" aria-hidden="true" tabindex="-1"></a>                    <span class="va">self</span>.val_results_hook(val_results, epoch)</span>
<span id="cb51-119"><a href="#cb51-119" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-120"><a href="#cb51-120" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> early_stop <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span> <span class="kw">and</span> early_stop.should_stop(</span>
<span id="cb51-121"><a href="#cb51-121" aria-hidden="true" tabindex="-1"></a>                    val_results.mean_loss</span>
<span id="cb51-122"><a href="#cb51-122" aria-hidden="true" tabindex="-1"></a>                ):</span>
<span id="cb51-123"><a href="#cb51-123" aria-hidden="true" tabindex="-1"></a>                    <span class="bu">print</span>(<span class="ss">f"Early stopping at epoch </span><span class="sc">{</span>epoch <span class="op">+</span> <span class="dv">1</span><span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb51-124"><a href="#cb51-124" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">break</span></span>
<span id="cb51-125"><a href="#cb51-125" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-126"><a href="#cb51-126" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> scheduler <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb51-127"><a href="#cb51-127" aria-hidden="true" tabindex="-1"></a>                    scheduler.step(val_results.mean_loss)</span>
<span id="cb51-128"><a href="#cb51-128" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-129"><a href="#cb51-129" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _train_one_epoch(</span>
<span id="cb51-130"><a href="#cb51-130" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>,</span>
<span id="cb51-131"><a href="#cb51-131" aria-hidden="true" tabindex="-1"></a>        model: torch.nn.Module,</span>
<span id="cb51-132"><a href="#cb51-132" aria-hidden="true" tabindex="-1"></a>        dl_train: DataLoader,</span>
<span id="cb51-133"><a href="#cb51-133" aria-hidden="true" tabindex="-1"></a>        loss_fn: torch.nn.Module,</span>
<span id="cb51-134"><a href="#cb51-134" aria-hidden="true" tabindex="-1"></a>        optimiser: torch.optim.Optimizer,</span>
<span id="cb51-135"><a href="#cb51-135" aria-hidden="true" tabindex="-1"></a>        epoch: <span class="bu">int</span>,</span>
<span id="cb51-136"><a href="#cb51-136" aria-hidden="true" tabindex="-1"></a>        transform: Optional[torchvision.transforms.v2.Transform] <span class="op">=</span> <span class="va">None</span>,</span>
<span id="cb51-137"><a href="#cb51-137" aria-hidden="true" tabindex="-1"></a>    ):</span>
<span id="cb51-138"><a href="#cb51-138" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb51-139"><a href="#cb51-139" aria-hidden="true" tabindex="-1"></a><span class="co">        Trains the given model for one epoch on the provided dataloader.</span></span>
<span id="cb51-140"><a href="#cb51-140" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-141"><a href="#cb51-141" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb51-142"><a href="#cb51-142" aria-hidden="true" tabindex="-1"></a><span class="co">            model (torch.nn.Module): The model to train.</span></span>
<span id="cb51-143"><a href="#cb51-143" aria-hidden="true" tabindex="-1"></a><span class="co">            dl_train (DataLoader): The dataloader for the training dataset.</span></span>
<span id="cb51-144"><a href="#cb51-144" aria-hidden="true" tabindex="-1"></a><span class="co">            loss_fn (torch.nn.Module): The loss function to calculate the loss.</span></span>
<span id="cb51-145"><a href="#cb51-145" aria-hidden="true" tabindex="-1"></a><span class="co">            optimiser (torch.optim.Optimizer): The optimiser for updating weights.</span></span>
<span id="cb51-146"><a href="#cb51-146" aria-hidden="true" tabindex="-1"></a><span class="co">            epoch (int): The current epoch number.</span></span>
<span id="cb51-147"><a href="#cb51-147" aria-hidden="true" tabindex="-1"></a><span class="co">            transform (Optional[torchvision.transforms.v2.Transform]): Optional transformations to apply</span></span>
<span id="cb51-148"><a href="#cb51-148" aria-hidden="true" tabindex="-1"></a><span class="co">                to the training data before feeding them to the model.</span></span>
<span id="cb51-149"><a href="#cb51-149" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-150"><a href="#cb51-150" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns:</span></span>
<span id="cb51-151"><a href="#cb51-151" aria-hidden="true" tabindex="-1"></a><span class="co">            float: The mean loss for the training epoch.</span></span>
<span id="cb51-152"><a href="#cb51-152" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb51-153"><a href="#cb51-153" aria-hidden="true" tabindex="-1"></a>        model.train()</span>
<span id="cb51-154"><a href="#cb51-154" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-155"><a href="#cb51-155" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Sum of the losses from each batch for the current epoch.</span></span>
<span id="cb51-156"><a href="#cb51-156" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Used to calculate the mean loss for the current epoch.</span></span>
<span id="cb51-157"><a href="#cb51-157" aria-hidden="true" tabindex="-1"></a>        total_loss: <span class="bu">float</span> <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb51-158"><a href="#cb51-158" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-159"><a href="#cb51-159" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> tqdm(total<span class="op">=</span><span class="bu">len</span>(dl_train.dataset)) <span class="im">as</span> pbar:</span>
<span id="cb51-160"><a href="#cb51-160" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> batch, (X, y) <span class="kw">in</span> <span class="bu">enumerate</span>(dl_train):</span>
<span id="cb51-161"><a href="#cb51-161" aria-hidden="true" tabindex="-1"></a>                pbar.set_description(<span class="ss">f"train [epoch </span><span class="sc">{</span>epoch <span class="op">+</span> <span class="dv">1</span><span class="sc">}</span><span class="ss"> batch </span><span class="sc">{</span>batch <span class="op">+</span> <span class="dv">1</span><span class="sc">}</span><span class="ss">]"</span>)</span>
<span id="cb51-162"><a href="#cb51-162" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-163"><a href="#cb51-163" aria-hidden="true" tabindex="-1"></a>                X <span class="op">=</span> X.to(<span class="va">self</span>.device)</span>
<span id="cb51-164"><a href="#cb51-164" aria-hidden="true" tabindex="-1"></a>                y <span class="op">=</span> y.to(<span class="va">self</span>.device)</span>
<span id="cb51-165"><a href="#cb51-165" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-166"><a href="#cb51-166" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> <span class="va">self</span>.augment_transform <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb51-167"><a href="#cb51-167" aria-hidden="true" tabindex="-1"></a>                    X <span class="op">=</span> <span class="va">self</span>.augment_transform(X)</span>
<span id="cb51-168"><a href="#cb51-168" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-169"><a href="#cb51-169" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> transform <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb51-170"><a href="#cb51-170" aria-hidden="true" tabindex="-1"></a>                    X <span class="op">=</span> transform(X)</span>
<span id="cb51-171"><a href="#cb51-171" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-172"><a href="#cb51-172" aria-hidden="true" tabindex="-1"></a>                pred <span class="op">=</span> model(X)</span>
<span id="cb51-173"><a href="#cb51-173" aria-hidden="true" tabindex="-1"></a>                loss <span class="op">=</span> loss_fn(pred, y)</span>
<span id="cb51-174"><a href="#cb51-174" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-175"><a href="#cb51-175" aria-hidden="true" tabindex="-1"></a>                <span class="kw">del</span> X</span>
<span id="cb51-176"><a href="#cb51-176" aria-hidden="true" tabindex="-1"></a>                <span class="kw">del</span> y</span>
<span id="cb51-177"><a href="#cb51-177" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-178"><a href="#cb51-178" aria-hidden="true" tabindex="-1"></a>                <span class="co"># Backpropagation.</span></span>
<span id="cb51-179"><a href="#cb51-179" aria-hidden="true" tabindex="-1"></a>                loss.backward()</span>
<span id="cb51-180"><a href="#cb51-180" aria-hidden="true" tabindex="-1"></a>                optimiser.step()</span>
<span id="cb51-181"><a href="#cb51-181" aria-hidden="true" tabindex="-1"></a>                optimiser.zero_grad()</span>
<span id="cb51-182"><a href="#cb51-182" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-183"><a href="#cb51-183" aria-hidden="true" tabindex="-1"></a>                total_loss <span class="op">+=</span> loss.item()</span>
<span id="cb51-184"><a href="#cb51-184" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-185"><a href="#cb51-185" aria-hidden="true" tabindex="-1"></a>                pbar.update(dl_train.batch_size)</span>
<span id="cb51-186"><a href="#cb51-186" aria-hidden="true" tabindex="-1"></a>                pbar.set_postfix({<span class="st">"train loss"</span>: loss.item()})</span>
<span id="cb51-187"><a href="#cb51-187" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-188"><a href="#cb51-188" aria-hidden="true" tabindex="-1"></a>            mean_loss <span class="op">=</span> total_loss <span class="op">/</span> <span class="bu">len</span>(dl_train)</span>
<span id="cb51-189"><a href="#cb51-189" aria-hidden="true" tabindex="-1"></a>            pbar.set_postfix({<span class="st">"mean train loss"</span>: mean_loss})</span>
<span id="cb51-190"><a href="#cb51-190" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-191"><a href="#cb51-191" aria-hidden="true" tabindex="-1"></a>        gc.collect()</span>
<span id="cb51-192"><a href="#cb51-192" aria-hidden="true" tabindex="-1"></a>        torch.cuda.empty_cache()</span>
<span id="cb51-193"><a href="#cb51-193" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-194"><a href="#cb51-194" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> mean_loss</span>
<span id="cb51-195"><a href="#cb51-195" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-196"><a href="#cb51-196" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _validate_one_epoch(</span>
<span id="cb51-197"><a href="#cb51-197" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>,</span>
<span id="cb51-198"><a href="#cb51-198" aria-hidden="true" tabindex="-1"></a>        model: torch.nn.Module,</span>
<span id="cb51-199"><a href="#cb51-199" aria-hidden="true" tabindex="-1"></a>        ds_val: Dataset,</span>
<span id="cb51-200"><a href="#cb51-200" aria-hidden="true" tabindex="-1"></a>        loss_fn: torch.nn.Module,</span>
<span id="cb51-201"><a href="#cb51-201" aria-hidden="true" tabindex="-1"></a>        epoch: <span class="bu">int</span>,</span>
<span id="cb51-202"><a href="#cb51-202" aria-hidden="true" tabindex="-1"></a>        transform: Optional[torchvision.transforms.v2.Transform] <span class="op">=</span> <span class="va">None</span>,</span>
<span id="cb51-203"><a href="#cb51-203" aria-hidden="true" tabindex="-1"></a>    ) <span class="op">-&gt;</span> EvaluationResults:</span>
<span id="cb51-204"><a href="#cb51-204" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb51-205"><a href="#cb51-205" aria-hidden="true" tabindex="-1"></a><span class="co">        Validates the model for one epoch on the given validation dataset.</span></span>
<span id="cb51-206"><a href="#cb51-206" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-207"><a href="#cb51-207" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb51-208"><a href="#cb51-208" aria-hidden="true" tabindex="-1"></a><span class="co">            model (torch.nn.Module): The model to validate.</span></span>
<span id="cb51-209"><a href="#cb51-209" aria-hidden="true" tabindex="-1"></a><span class="co">            ds_val (Dataset): The validation dataset.</span></span>
<span id="cb51-210"><a href="#cb51-210" aria-hidden="true" tabindex="-1"></a><span class="co">            loss_fn (torch.nn.Module): The loss function to calculate the loss.</span></span>
<span id="cb51-211"><a href="#cb51-211" aria-hidden="true" tabindex="-1"></a><span class="co">            epoch (int): The current epoch number.</span></span>
<span id="cb51-212"><a href="#cb51-212" aria-hidden="true" tabindex="-1"></a><span class="co">            transform (Optional[torchvision.transforms.v2.Transform]): Optional transformations to apply</span></span>
<span id="cb51-213"><a href="#cb51-213" aria-hidden="true" tabindex="-1"></a><span class="co">                to the validation data before feeding them to the model.</span></span>
<span id="cb51-214"><a href="#cb51-214" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-215"><a href="#cb51-215" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns:</span></span>
<span id="cb51-216"><a href="#cb51-216" aria-hidden="true" tabindex="-1"></a><span class="co">            EvaluationResults: The results of the validation.</span></span>
<span id="cb51-217"><a href="#cb51-217" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb51-218"><a href="#cb51-218" aria-hidden="true" tabindex="-1"></a>        evaluator <span class="op">=</span> Evaluator(</span>
<span id="cb51-219"><a href="#cb51-219" aria-hidden="true" tabindex="-1"></a>            batch_size<span class="op">=</span><span class="va">self</span>.batch_size,</span>
<span id="cb51-220"><a href="#cb51-220" aria-hidden="true" tabindex="-1"></a>            desc<span class="op">=</span><span class="st">"validate"</span>,</span>
<span id="cb51-221"><a href="#cb51-221" aria-hidden="true" tabindex="-1"></a>            zero_division<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb51-222"><a href="#cb51-222" aria-hidden="true" tabindex="-1"></a>            device<span class="op">=</span><span class="va">self</span>.device,</span>
<span id="cb51-223"><a href="#cb51-223" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb51-224"><a href="#cb51-224" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-225"><a href="#cb51-225" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> evaluator.evaluate(</span>
<span id="cb51-226"><a href="#cb51-226" aria-hidden="true" tabindex="-1"></a>            model,</span>
<span id="cb51-227"><a href="#cb51-227" aria-hidden="true" tabindex="-1"></a>            ds_val,</span>
<span id="cb51-228"><a href="#cb51-228" aria-hidden="true" tabindex="-1"></a>            loss_fn,</span>
<span id="cb51-229"><a href="#cb51-229" aria-hidden="true" tabindex="-1"></a>            transform<span class="op">=</span>transform,</span>
<span id="cb51-230"><a href="#cb51-230" aria-hidden="true" tabindex="-1"></a>        )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now, to prepare for hyperparameter tuning, I define some convenience functions that each accepts a dictionary of hyperparameters determined by Optuna during hyperparameter tuning:</p>
<div id="16c5066f" class="cell">
<div class="sourceCode cell-code" id="cb52"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> augmentation_transforms_from_params(params: <span class="bu">dict</span>) <span class="op">-&gt;</span> Optional[torchvision.transforms.v2.Transform]:</span>
<span id="cb52-2"><a href="#cb52-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb52-3"><a href="#cb52-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns the augmentation transforms according to the given hyperparameter dictionary.</span></span>
<span id="cb52-4"><a href="#cb52-4" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb52-5"><a href="#cb52-5" aria-hidden="true" tabindex="-1"></a>    transforms <span class="op">=</span> []</span>
<span id="cb52-6"><a href="#cb52-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-7"><a href="#cb52-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> params[<span class="st">"jitter"</span>]:</span>
<span id="cb52-8"><a href="#cb52-8" aria-hidden="true" tabindex="-1"></a>        transforms.append(</span>
<span id="cb52-9"><a href="#cb52-9" aria-hidden="true" tabindex="-1"></a>            torchvision.transforms.v2.ColorJitter(</span>
<span id="cb52-10"><a href="#cb52-10" aria-hidden="true" tabindex="-1"></a>                brightness<span class="op">=</span><span class="fl">0.2</span>, contrast<span class="op">=</span><span class="fl">0.2</span>, saturation<span class="op">=</span><span class="fl">0.2</span>, hue<span class="op">=</span><span class="fl">0.1</span></span>
<span id="cb52-11"><a href="#cb52-11" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb52-12"><a href="#cb52-12" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb52-13"><a href="#cb52-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-14"><a href="#cb52-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> params[<span class="st">"horizontal_flip"</span>]:</span>
<span id="cb52-15"><a href="#cb52-15" aria-hidden="true" tabindex="-1"></a>        transforms.append(torchvision.transforms.v2.RandomHorizontalFlip())</span>
<span id="cb52-16"><a href="#cb52-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-17"><a href="#cb52-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> params[<span class="st">"rotation"</span>]:</span>
<span id="cb52-18"><a href="#cb52-18" aria-hidden="true" tabindex="-1"></a>        transforms.append(torchvision.transforms.v2.RandomRotation(<span class="dv">30</span>))</span>
<span id="cb52-19"><a href="#cb52-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-20"><a href="#cb52-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Compose does not allow an empty list of transformations.</span></span>
<span id="cb52-21"><a href="#cb52-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="kw">not</span> transforms:</span>
<span id="cb52-22"><a href="#cb52-22" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">None</span></span>
<span id="cb52-23"><a href="#cb52-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-24"><a href="#cb52-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> torchvision.transforms.v2.Compose(transforms)</span>
<span id="cb52-25"><a href="#cb52-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-26"><a href="#cb52-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-27"><a href="#cb52-27" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> transforms_from_params(params: <span class="bu">dict</span>) <span class="op">-&gt;</span> torchvision.transforms.v2.Transform:</span>
<span id="cb52-28"><a href="#cb52-28" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb52-29"><a href="#cb52-29" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns the preprocessing transformations according to the given hyperparameter dictionary.</span></span>
<span id="cb52-30"><a href="#cb52-30" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb52-31"><a href="#cb52-31" aria-hidden="true" tabindex="-1"></a>    transforms <span class="op">=</span> []</span>
<span id="cb52-32"><a href="#cb52-32" aria-hidden="true" tabindex="-1"></a>    length <span class="op">=</span> params[<span class="st">"resize_length"</span>]</span>
<span id="cb52-33"><a href="#cb52-33" aria-hidden="true" tabindex="-1"></a>    transforms.append(torchvision.transforms.v2.Resize((length, length)))</span>
<span id="cb52-34"><a href="#cb52-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-35"><a href="#cb52-35" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> params[<span class="st">"grayscale"</span>]:</span>
<span id="cb52-36"><a href="#cb52-36" aria-hidden="true" tabindex="-1"></a>        transforms.append(torchvision.transforms.v2.Grayscale())</span>
<span id="cb52-37"><a href="#cb52-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-38"><a href="#cb52-38" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> torchvision.transforms.v2.Compose(transforms)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The following function simply ties everything together into one function to reduce code duplication:</p>
<div id="edb26a96" class="cell">
<div class="sourceCode cell-code" id="cb53"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train_eval_generic_params(</span>
<span id="cb53-2"><a href="#cb53-2" aria-hidden="true" tabindex="-1"></a>    model: torch.nn.Module,</span>
<span id="cb53-3"><a href="#cb53-3" aria-hidden="true" tabindex="-1"></a>    ds_train: torch.utils.data.Dataset,</span>
<span id="cb53-4"><a href="#cb53-4" aria-hidden="true" tabindex="-1"></a>    ds_val: torch.utils.data.Dataset,</span>
<span id="cb53-5"><a href="#cb53-5" aria-hidden="true" tabindex="-1"></a>    ds_test: torch.utils.data.Dataset,</span>
<span id="cb53-6"><a href="#cb53-6" aria-hidden="true" tabindex="-1"></a>    sampler: torch.utils.data.Sampler,</span>
<span id="cb53-7"><a href="#cb53-7" aria-hidden="true" tabindex="-1"></a>    params: <span class="bu">dict</span>,</span>
<span id="cb53-8"><a href="#cb53-8" aria-hidden="true" tabindex="-1"></a>    train_loss_hook: Optional[Callable[[<span class="bu">float</span>, <span class="bu">int</span>], <span class="va">None</span>]] <span class="op">=</span> <span class="va">None</span>,</span>
<span id="cb53-9"><a href="#cb53-9" aria-hidden="true" tabindex="-1"></a>    val_results_hook: Optional[Callable[[EvaluationResults, <span class="bu">int</span>], <span class="va">None</span>]] <span class="op">=</span> <span class="va">None</span>,</span>
<span id="cb53-10"><a href="#cb53-10" aria-hidden="true" tabindex="-1"></a>    transforms_override: Optional[torchvision.transforms.v2.Transform] <span class="op">=</span> <span class="va">None</span>,</span>
<span id="cb53-11"><a href="#cb53-11" aria-hidden="true" tabindex="-1"></a>    max_epochs: <span class="bu">int</span> <span class="op">=</span> N_EPOCHS_TRAIN,</span>
<span id="cb53-12"><a href="#cb53-12" aria-hidden="true" tabindex="-1"></a>    device: <span class="bu">str</span> <span class="op">=</span> DEVICE,</span>
<span id="cb53-13"><a href="#cb53-13" aria-hidden="true" tabindex="-1"></a>):</span>
<span id="cb53-14"><a href="#cb53-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Train.</span></span>
<span id="cb53-15"><a href="#cb53-15" aria-hidden="true" tabindex="-1"></a>    trainer <span class="op">=</span> Trainer(</span>
<span id="cb53-16"><a href="#cb53-16" aria-hidden="true" tabindex="-1"></a>        epochs<span class="op">=</span>max_epochs,</span>
<span id="cb53-17"><a href="#cb53-17" aria-hidden="true" tabindex="-1"></a>        batch_size<span class="op">=</span>params[<span class="st">"batch_size"</span>],</span>
<span id="cb53-18"><a href="#cb53-18" aria-hidden="true" tabindex="-1"></a>        augment_transform<span class="op">=</span>augmentation_transforms_from_params(params),</span>
<span id="cb53-19"><a href="#cb53-19" aria-hidden="true" tabindex="-1"></a>        train_loss_hook<span class="op">=</span>train_loss_hook,</span>
<span id="cb53-20"><a href="#cb53-20" aria-hidden="true" tabindex="-1"></a>        val_results_hook<span class="op">=</span>val_results_hook,</span>
<span id="cb53-21"><a href="#cb53-21" aria-hidden="true" tabindex="-1"></a>        device<span class="op">=</span>device,</span>
<span id="cb53-22"><a href="#cb53-22" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb53-23"><a href="#cb53-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-24"><a href="#cb53-24" aria-hidden="true" tabindex="-1"></a>    optimiser <span class="op">=</span> torch.optim.SGD(</span>
<span id="cb53-25"><a href="#cb53-25" aria-hidden="true" tabindex="-1"></a>        model.parameters(),</span>
<span id="cb53-26"><a href="#cb53-26" aria-hidden="true" tabindex="-1"></a>        lr<span class="op">=</span>params[<span class="st">"lr"</span>],</span>
<span id="cb53-27"><a href="#cb53-27" aria-hidden="true" tabindex="-1"></a>        weight_decay<span class="op">=</span>params[<span class="st">"weight_decay"</span>],</span>
<span id="cb53-28"><a href="#cb53-28" aria-hidden="true" tabindex="-1"></a>        momentum<span class="op">=</span>params[<span class="st">"momentum"</span>],</span>
<span id="cb53-29"><a href="#cb53-29" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb53-30"><a href="#cb53-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-31"><a href="#cb53-31" aria-hidden="true" tabindex="-1"></a>    scheduler <span class="op">=</span> torch.optim.lr_scheduler.ReduceLROnPlateau(</span>
<span id="cb53-32"><a href="#cb53-32" aria-hidden="true" tabindex="-1"></a>        optimiser, factor<span class="op">=</span>params[<span class="st">"reduce_lr_factor"</span>]</span>
<span id="cb53-33"><a href="#cb53-33" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb53-34"><a href="#cb53-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-35"><a href="#cb53-35" aria-hidden="true" tabindex="-1"></a>    transforms <span class="op">=</span> transforms_override <span class="cf">if</span> transforms_override <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span> <span class="cf">else</span> transforms_from_params(params)</span>
<span id="cb53-36"><a href="#cb53-36" aria-hidden="true" tabindex="-1"></a>    trainer.train(</span>
<span id="cb53-37"><a href="#cb53-37" aria-hidden="true" tabindex="-1"></a>        model,</span>
<span id="cb53-38"><a href="#cb53-38" aria-hidden="true" tabindex="-1"></a>        ds_train,</span>
<span id="cb53-39"><a href="#cb53-39" aria-hidden="true" tabindex="-1"></a>        loss_fn<span class="op">=</span>torch.nn.CrossEntropyLoss(),</span>
<span id="cb53-40"><a href="#cb53-40" aria-hidden="true" tabindex="-1"></a>        optimiser<span class="op">=</span>optimiser,</span>
<span id="cb53-41"><a href="#cb53-41" aria-hidden="true" tabindex="-1"></a>        train_sampler<span class="op">=</span>sampler,</span>
<span id="cb53-42"><a href="#cb53-42" aria-hidden="true" tabindex="-1"></a>        transform<span class="op">=</span>transforms,</span>
<span id="cb53-43"><a href="#cb53-43" aria-hidden="true" tabindex="-1"></a>        ds_val<span class="op">=</span>ds_val,</span>
<span id="cb53-44"><a href="#cb53-44" aria-hidden="true" tabindex="-1"></a>        scheduler<span class="op">=</span>scheduler,</span>
<span id="cb53-45"><a href="#cb53-45" aria-hidden="true" tabindex="-1"></a>        early_stop<span class="op">=</span>ValidationLossEarlyStopper(</span>
<span id="cb53-46"><a href="#cb53-46" aria-hidden="true" tabindex="-1"></a>            patience<span class="op">=</span>params[<span class="st">"early_stop_patience"</span>],</span>
<span id="cb53-47"><a href="#cb53-47" aria-hidden="true" tabindex="-1"></a>            delta<span class="op">=</span><span class="fl">0.0</span>,</span>
<span id="cb53-48"><a href="#cb53-48" aria-hidden="true" tabindex="-1"></a>        ),</span>
<span id="cb53-49"><a href="#cb53-49" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb53-50"><a href="#cb53-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-51"><a href="#cb53-51" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Evaluate.</span></span>
<span id="cb53-52"><a href="#cb53-52" aria-hidden="true" tabindex="-1"></a>    evaluator <span class="op">=</span> Evaluator(</span>
<span id="cb53-53"><a href="#cb53-53" aria-hidden="true" tabindex="-1"></a>        batch_size<span class="op">=</span>params[<span class="st">"batch_size"</span>],</span>
<span id="cb53-54"><a href="#cb53-54" aria-hidden="true" tabindex="-1"></a>        zero_division<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb53-55"><a href="#cb53-55" aria-hidden="true" tabindex="-1"></a>        device<span class="op">=</span>device,</span>
<span id="cb53-56"><a href="#cb53-56" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb53-57"><a href="#cb53-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-58"><a href="#cb53-58" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> evaluator.evaluate(</span>
<span id="cb53-59"><a href="#cb53-59" aria-hidden="true" tabindex="-1"></a>        model,</span>
<span id="cb53-60"><a href="#cb53-60" aria-hidden="true" tabindex="-1"></a>        ds_test,</span>
<span id="cb53-61"><a href="#cb53-61" aria-hidden="true" tabindex="-1"></a>        loss_fn<span class="op">=</span>torch.nn.CrossEntropyLoss(),</span>
<span id="cb53-62"><a href="#cb53-62" aria-hidden="true" tabindex="-1"></a>        transform<span class="op">=</span>transforms,</span>
<span id="cb53-63"><a href="#cb53-63" aria-hidden="true" tabindex="-1"></a>        target_labels<span class="op">=</span>target_le.classes_,</span>
<span id="cb53-64"><a href="#cb53-64" aria-hidden="true" tabindex="-1"></a>    )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="hyperparameter-tuning" class="level2">
<h2 class="anchored" data-anchor-id="hyperparameter-tuning">Hyperparameter Tuning</h2>
<p>I use the Optuna library for hyperparameter tuning. Optuna uses Bayesian optimisation (at least by default — it also provides other optimisation methods, but I used Bayesian optimisation) to find the best set of hyperparameters. Based on the performance from past hyperparameter sets, it tries to select promising hyperparameters. Unlike the traditional grid search and random search, Bayesian optimisation can reduce the number of trials needed to find an optimal set of hyperparameters.</p>
<p>We first need to define an objective to optimise. I chose to maximise the macro average F1-score. A quick explanation of what macro average F1 is:</p>
<ul>
<li><p>Precision measures the accuracy of positive predictions (i.e., high precision means that the model tends to be correct when it predicts a positive instance).</p></li>
<li><p>Recall measures how well the model is able to identify positive instances (i.e., high recall means that model is able to identify most positive instances).</p></li>
<li><p>The F1 score is the harmonic mean of precision and recall, so it balances these two metrics into a single measure.</p></li>
<li><p>The macro average F1 score calculates the F1 score for each class independently and then takes the average, treating all classes equally. This is important in multi-class problems where class distribution may be imbalanced (which is the case for our dataset) as it ensures that the performance on each class contributes equally to the overall score.</p></li>
</ul>
<p>Why macro average F1?</p>
<ul>
<li><p>It is one of the metrics that will be used for final evaluation and has real-world implications.</p></li>
<li><p>Unlike accuracy, the macro average F1-score takes into account both precision and recall, so maximising it will likely lead to the model being less biased towards specific classes.</p></li>
<li><p>We could minimise the loss, but that does not always mean that the model meets real-world objectives.</p></li>
</ul>
<p>Although the models have the same objective to maximise, they have different hyperparameters. Some of these hyperparameters are general and applicable to all of the models (e.g., learning rate and batch size). These general hyperparameters are:</p>
<ul>
<li><p><strong>Batch size:</strong> The number of training samples to process in a single batch before updating the model’s parameters. Smaller batch sizes means more frequent updates and potentially better generalisation at the cost of longer training time. Larger batch sizes can stabilise the training process but may require more memory and may lead to poorer generalisation. See <a href="https://arxiv.org/abs/1609.04836">On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima</a> for a more detailed explanation. Unfortunately, while I wanted to try larger batch sizes (64 and above), I frequently encountered scenarios in which the GPU ran out of memory; hence, the search space for the batch size is kept on the lower side (24 and 32).</p></li>
<li><p><strong>Learning rate:</strong> Controls the size of the steps for gradient updates. A low learning rate means updates are more precise but requires more epochs to get comparable performance. A high learning rate speeds up training at the cost of risking <em>overshooting</em>, where the optimal model parameters are constantly missed, potentially causing failure to converge.</p></li>
<li><p><strong>Weight decay:</strong> A regularisation parameter that prevents weights from getting too large, which helps to prevent overfitting. A higher weight decay can improve generalisation but may make learning too slow.</p></li>
<li><p><strong>Learning rate reduction factor:</strong> Factor by which to reduce the learning rate when validation performance starts to stagnate. A smaller factor allows for finer adjustments to the learning rate, which can help the model converge better towards the end of training.</p></li>
<li><p><strong>Momentum:</strong> A parameter for stochastic gradient descent that accelerates the gradient descent process using past gradients to smooth out updates, which can lead to faster convergence and prevent the model from getting stuck in local minima.</p></li>
<li><p><strong>Early stop patience:</strong> The number of epochs to wait without improvement in validation performance before stopping training. A higher patience value allows more epochs for the model to improve but risks overfitting to the training set while a lower value may stop training prematurely if the model is slow to converge.</p></li>
<li><p><strong>Color jitter?:</strong> Whether to randomly alter the colours in the input images (e.g., perturb brightness, contrast, saturation and hue). Color jittering is useful as a data augmentation technique and can improve the model’s robustness to variations in lighting and color conditions.</p></li>
<li><p><strong>Random horizontal flip?:</strong> Whether to randomly flip images horizontally during training. Serves as a data augmentation technique that may help the model become more generalisable with respect to left-right orientation. I kept the probability of randomly flipping an image to 0.5.</p></li>
<li><p><strong>Random rotation?:</strong> Whether to randomly rotate input images. Serves as a data augmentation technique that may help the model generalise better with respect to orientation. In this case, I kept this is a simple Boolean toggle to reduce the search space and time — random rotations are always done within 30 degrees.</p></li>
</ul>
<p>For the MLP and custom VGG-16 models, but <em>not</em> the pretrained VGG-16 model, the following additional hyperparameters were tuned:</p>
<ul>
<li><p><strong>Resize length:</strong> Specifies the target size for input images — images are resized to a square. Resizing makes input dimensions consistent and reduces computational load. However, if images are resized to dimensions that are too small, we may lose too much information. Unfortunately, I had to keep the maximum resize length rather small (224) because the GPU kept running out of memory.</p></li>
<li><p><strong>Grayscale?:</strong> Whether to convert images to grayscale. Using grayscale reduces input dimensionality and computational requirements, which is especially useful for the MLP model (since there are many fully-connected units). However, it removes colour information that could have been important.</p></li>
</ul>
<p>The reason these hyperparameters are not tuned for the pretrained VGG-16 model is because we need to use the same transformations that were used when it was trained on ImageNet. These transformations can be accessed from the pretrained weights, as documented <a href="https://pytorch.org/vision/main/models/generated/torchvision.models.vgg16.html#torchvision.models.VGG16_Weights">here</a>.</p>
<p>The function below creates and returns an objective function that includes the logic for suggesting the general hyperparameters. The <code>model_from_params</code> argument should be a callable that creates a model with the hyperparameter suggestions. Since the dataset is quite small, the function uses K-fold cross-validation to get a more unbiased estimate of model performance — using a single validation set for hyperparameter tuning would risk overfitting.</p>
<p>Finally, the hyperparameter tuning process uses <em>median pruning</em>, which compares the intermediate results of a trial (e.g., validation loss) against the median value of all previously completed trials at the same step. If a trial’s results is worse than the median, it is unlikely to improve on the previous trials, so the trial is pruned to save computational resources and time.</p>
<div id="5e84d3e5" class="cell">
<div class="sourceCode cell-code" id="cb54"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> make_objective(</span>
<span id="cb54-2"><a href="#cb54-2" aria-hidden="true" tabindex="-1"></a>    suggest_model_params: Callable[[optuna.trial.Trial], <span class="va">None</span>],</span>
<span id="cb54-3"><a href="#cb54-3" aria-hidden="true" tabindex="-1"></a>    model_from_params: Callable[[<span class="bu">dict</span>], torch.nn.Module],</span>
<span id="cb54-4"><a href="#cb54-4" aria-hidden="true" tabindex="-1"></a>    ds_train: torch.utils.data.Dataset,</span>
<span id="cb54-5"><a href="#cb54-5" aria-hidden="true" tabindex="-1"></a>    prune: <span class="bu">bool</span> <span class="op">=</span> PRUNE_TRIALS,</span>
<span id="cb54-6"><a href="#cb54-6" aria-hidden="true" tabindex="-1"></a>    n_folds: <span class="bu">int</span> <span class="op">=</span> N_FOLDS,</span>
<span id="cb54-7"><a href="#cb54-7" aria-hidden="true" tabindex="-1"></a>    max_epochs: <span class="bu">int</span> <span class="op">=</span> N_EPOCHS_TUNE,</span>
<span id="cb54-8"><a href="#cb54-8" aria-hidden="true" tabindex="-1"></a>    transforms_override: Optional[torchvision.transforms.v2.Transform] <span class="op">=</span> <span class="va">None</span>,</span>
<span id="cb54-9"><a href="#cb54-9" aria-hidden="true" tabindex="-1"></a>    device: <span class="bu">str</span> <span class="op">=</span> DEVICE,</span>
<span id="cb54-10"><a href="#cb54-10" aria-hidden="true" tabindex="-1"></a>) <span class="op">-&gt;</span> Callable[[optuna.trial.Trial], <span class="bu">tuple</span>[<span class="bu">float</span>, <span class="bu">float</span>]]:</span>
<span id="cb54-11"><a href="#cb54-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> objective(trial: optuna.trial.Trial) <span class="op">-&gt;</span> <span class="bu">tuple</span>[<span class="bu">float</span>, <span class="bu">float</span>]:</span>
<span id="cb54-12"><a href="#cb54-12" aria-hidden="true" tabindex="-1"></a>        <span class="co"># General hyperparameters.</span></span>
<span id="cb54-13"><a href="#cb54-13" aria-hidden="true" tabindex="-1"></a>        trial.suggest_int(<span class="st">"batch_size"</span>, <span class="dv">24</span>, <span class="dv">32</span>, step<span class="op">=</span><span class="dv">8</span>)</span>
<span id="cb54-14"><a href="#cb54-14" aria-hidden="true" tabindex="-1"></a>        trial.suggest_float(<span class="st">"lr"</span>, <span class="fl">1e-5</span>, <span class="fl">1e-3</span>)</span>
<span id="cb54-15"><a href="#cb54-15" aria-hidden="true" tabindex="-1"></a>        trial.suggest_float(<span class="st">"weight_decay"</span>, <span class="dv">0</span>, <span class="fl">1e-2</span>)</span>
<span id="cb54-16"><a href="#cb54-16" aria-hidden="true" tabindex="-1"></a>        trial.suggest_float(<span class="st">"reduce_lr_factor"</span>, <span class="fl">0.01</span>, <span class="fl">0.1</span>)</span>
<span id="cb54-17"><a href="#cb54-17" aria-hidden="true" tabindex="-1"></a>        trial.suggest_float(<span class="st">"momentum"</span>, <span class="fl">0.8</span>, <span class="fl">0.99</span>)</span>
<span id="cb54-18"><a href="#cb54-18" aria-hidden="true" tabindex="-1"></a>        trial.suggest_int(<span class="st">"early_stop_patience"</span>, <span class="dv">1</span>, <span class="dv">3</span>)</span>
<span id="cb54-19"><a href="#cb54-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-20"><a href="#cb54-20" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Preprocessing input transforms.</span></span>
<span id="cb54-21"><a href="#cb54-21" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> transforms_override <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb54-22"><a href="#cb54-22" aria-hidden="true" tabindex="-1"></a>            trial.suggest_int(<span class="st">"resize_length"</span>, <span class="dv">32</span>, <span class="dv">224</span>, step<span class="op">=</span><span class="dv">32</span>)</span>
<span id="cb54-23"><a href="#cb54-23" aria-hidden="true" tabindex="-1"></a>            trial.suggest_categorical(<span class="st">"grayscale"</span>, [<span class="va">True</span>, <span class="va">False</span>])</span>
<span id="cb54-24"><a href="#cb54-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-25"><a href="#cb54-25" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Augmentation transforms.</span></span>
<span id="cb54-26"><a href="#cb54-26" aria-hidden="true" tabindex="-1"></a>        trial.suggest_categorical(<span class="st">"jitter"</span>, [<span class="va">True</span>, <span class="va">False</span>])</span>
<span id="cb54-27"><a href="#cb54-27" aria-hidden="true" tabindex="-1"></a>        trial.suggest_categorical(<span class="st">"horizontal_flip"</span>, [<span class="va">True</span>, <span class="va">False</span>])</span>
<span id="cb54-28"><a href="#cb54-28" aria-hidden="true" tabindex="-1"></a>        trial.suggest_categorical(<span class="st">"rotation"</span>, [<span class="va">True</span>, <span class="va">False</span>])</span>
<span id="cb54-29"><a href="#cb54-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-30"><a href="#cb54-30" aria-hidden="true" tabindex="-1"></a>        suggest_model_params(trial)</span>
<span id="cb54-31"><a href="#cb54-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-32"><a href="#cb54-32" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Trial </span><span class="sc">{</span>trial<span class="sc">.</span>number<span class="sc">}</span><span class="ss"> params: </span><span class="sc">{</span>trial<span class="sc">.</span>params<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb54-33"><a href="#cb54-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-34"><a href="#cb54-34" aria-hidden="true" tabindex="-1"></a>        f1_scores: <span class="bu">list</span>[<span class="bu">float</span>] <span class="op">=</span> []</span>
<span id="cb54-35"><a href="#cb54-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-36"><a href="#cb54-36" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Use stratified K-fold cross-validation with 4 folds.</span></span>
<span id="cb54-37"><a href="#cb54-37" aria-hidden="true" tabindex="-1"></a>        kfold_cv <span class="op">=</span> sklearn.model_selection.StratifiedKFold(</span>
<span id="cb54-38"><a href="#cb54-38" aria-hidden="true" tabindex="-1"></a>            n_splits<span class="op">=</span>n_folds, shuffle<span class="op">=</span><span class="va">True</span>, random_state<span class="op">=</span><span class="dv">0</span></span>
<span id="cb54-39"><a href="#cb54-39" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb54-40"><a href="#cb54-40" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> fold_idx, (cv_indices_other, cv_indices_test) <span class="kw">in</span> <span class="bu">enumerate</span>(</span>
<span id="cb54-41"><a href="#cb54-41" aria-hidden="true" tabindex="-1"></a>            kfold_cv.split(</span>
<span id="cb54-42"><a href="#cb54-42" aria-hidden="true" tabindex="-1"></a>                X<span class="op">=</span>np.arange(<span class="bu">len</span>(ds_train)),</span>
<span id="cb54-43"><a href="#cb54-43" aria-hidden="true" tabindex="-1"></a>                y<span class="op">=</span>np.fromiter(ds_train.targets(), dtype<span class="op">=</span><span class="bu">int</span>),</span>
<span id="cb54-44"><a href="#cb54-44" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb54-45"><a href="#cb54-45" aria-hidden="true" tabindex="-1"></a>        ):</span>
<span id="cb54-46"><a href="#cb54-46" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"Trial </span><span class="sc">{</span>trial<span class="sc">.</span>number<span class="sc">}</span><span class="ss"> fold </span><span class="sc">{</span>fold_idx <span class="op">+</span> <span class="dv">1</span><span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb54-47"><a href="#cb54-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-48"><a href="#cb54-48" aria-hidden="true" tabindex="-1"></a>            cv_ds_other <span class="op">=</span> ClothesSubset(ds_train, cv_indices_other)</span>
<span id="cb54-49"><a href="#cb54-49" aria-hidden="true" tabindex="-1"></a>            cv_ds_test <span class="op">=</span> ClothesSubset(ds_train, cv_indices_test)</span>
<span id="cb54-50"><a href="#cb54-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-51"><a href="#cb54-51" aria-hidden="true" tabindex="-1"></a>            <span class="co"># The `other` set is 3 out of 4 folds.</span></span>
<span id="cb54-52"><a href="#cb54-52" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Use 80% of the `other` set as the training set.</span></span>
<span id="cb54-53"><a href="#cb54-53" aria-hidden="true" tabindex="-1"></a>            <span class="co"># The remaining 20% are used as the validation set for early stopping.</span></span>
<span id="cb54-54"><a href="#cb54-54" aria-hidden="true" tabindex="-1"></a>            <span class="co"># The 4th fold is used as the test set for evaluation.</span></span>
<span id="cb54-55"><a href="#cb54-55" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Note: These indices are indices into `cv_ds_other`.</span></span>
<span id="cb54-56"><a href="#cb54-56" aria-hidden="true" tabindex="-1"></a>            cv_indices_train, cv_indices_val <span class="op">=</span> sklearn.model_selection.train_test_split(</span>
<span id="cb54-57"><a href="#cb54-57" aria-hidden="true" tabindex="-1"></a>                np.arange(<span class="bu">len</span>(cv_ds_other)),</span>
<span id="cb54-58"><a href="#cb54-58" aria-hidden="true" tabindex="-1"></a>                train_size<span class="op">=</span><span class="fl">0.8</span>,</span>
<span id="cb54-59"><a href="#cb54-59" aria-hidden="true" tabindex="-1"></a>                random_state<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb54-60"><a href="#cb54-60" aria-hidden="true" tabindex="-1"></a>                shuffle<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb54-61"><a href="#cb54-61" aria-hidden="true" tabindex="-1"></a>                stratify<span class="op">=</span>np.fromiter(cv_ds_other.targets(), dtype<span class="op">=</span><span class="bu">int</span>),</span>
<span id="cb54-62"><a href="#cb54-62" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb54-63"><a href="#cb54-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-64"><a href="#cb54-64" aria-hidden="true" tabindex="-1"></a>            cv_ds_train <span class="op">=</span> ClothesSubset(cv_ds_other, cv_indices_train)</span>
<span id="cb54-65"><a href="#cb54-65" aria-hidden="true" tabindex="-1"></a>            cv_ds_val <span class="op">=</span> ClothesSubset(cv_ds_other, cv_indices_val)</span>
<span id="cb54-66"><a href="#cb54-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-67"><a href="#cb54-67" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Create the undersampler using `cv_ds_train`'s labels.</span></span>
<span id="cb54-68"><a href="#cb54-68" aria-hidden="true" tabindex="-1"></a>            <span class="co"># To avoid leakage, we must not use `ds_train`.</span></span>
<span id="cb54-69"><a href="#cb54-69" aria-hidden="true" tabindex="-1"></a>            <span class="co"># The shape won't match anyway.</span></span>
<span id="cb54-70"><a href="#cb54-70" aria-hidden="true" tabindex="-1"></a>            undersampler_cv_train <span class="op">=</span> make_undersampler(</span>
<span id="cb54-71"><a href="#cb54-71" aria-hidden="true" tabindex="-1"></a>                torch.tensor(np.fromiter(cv_ds_train.targets(), dtype<span class="op">=</span><span class="bu">int</span>))</span>
<span id="cb54-72"><a href="#cb54-72" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb54-73"><a href="#cb54-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-74"><a href="#cb54-74" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Hook onto the validation results for pruning trials.</span></span>
<span id="cb54-75"><a href="#cb54-75" aria-hidden="true" tabindex="-1"></a>            <span class="kw">def</span> val_results_hook(results: EvaluationResults, epoch: <span class="bu">int</span>):</span>
<span id="cb54-76"><a href="#cb54-76" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> <span class="kw">not</span> prune:</span>
<span id="cb54-77"><a href="#cb54-77" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">return</span></span>
<span id="cb54-78"><a href="#cb54-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-79"><a href="#cb54-79" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> epoch <span class="op">==</span> <span class="dv">4</span> <span class="kw">and</span> results.mean_loss <span class="op">&gt;=</span> <span class="fl">1.5</span>:</span>
<span id="cb54-80"><a href="#cb54-80" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">raise</span> optuna.TrialPruned(<span class="st">"Mean validation loss is still above or equal to 1.5 at the 5th epoch."</span>)</span>
<span id="cb54-81"><a href="#cb54-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-82"><a href="#cb54-82" aria-hidden="true" tabindex="-1"></a>                step <span class="op">=</span> fold_idx <span class="op">*</span> max_epochs <span class="op">+</span> epoch</span>
<span id="cb54-83"><a href="#cb54-83" aria-hidden="true" tabindex="-1"></a>                f1 <span class="op">=</span> results.classification_report[<span class="st">"macro avg"</span>][<span class="st">"f1-score"</span>]</span>
<span id="cb54-84"><a href="#cb54-84" aria-hidden="true" tabindex="-1"></a>                trial.report(f1, step)</span>
<span id="cb54-85"><a href="#cb54-85" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> trial.should_prune():</span>
<span id="cb54-86"><a href="#cb54-86" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">raise</span> optuna.TrialPruned()</span>
<span id="cb54-87"><a href="#cb54-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-88"><a href="#cb54-88" aria-hidden="true" tabindex="-1"></a>            model <span class="op">=</span> model_from_params(trial.params)</span>
<span id="cb54-89"><a href="#cb54-89" aria-hidden="true" tabindex="-1"></a>            results <span class="op">=</span> train_eval_generic_params(</span>
<span id="cb54-90"><a href="#cb54-90" aria-hidden="true" tabindex="-1"></a>                model<span class="op">=</span>model,</span>
<span id="cb54-91"><a href="#cb54-91" aria-hidden="true" tabindex="-1"></a>                ds_train<span class="op">=</span>cv_ds_train,</span>
<span id="cb54-92"><a href="#cb54-92" aria-hidden="true" tabindex="-1"></a>                ds_val<span class="op">=</span>cv_ds_val,</span>
<span id="cb54-93"><a href="#cb54-93" aria-hidden="true" tabindex="-1"></a>                ds_test<span class="op">=</span>cv_ds_test,</span>
<span id="cb54-94"><a href="#cb54-94" aria-hidden="true" tabindex="-1"></a>                sampler<span class="op">=</span>undersampler_cv_train,</span>
<span id="cb54-95"><a href="#cb54-95" aria-hidden="true" tabindex="-1"></a>                params<span class="op">=</span>trial.params,</span>
<span id="cb54-96"><a href="#cb54-96" aria-hidden="true" tabindex="-1"></a>                train_loss_hook<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb54-97"><a href="#cb54-97" aria-hidden="true" tabindex="-1"></a>                val_results_hook<span class="op">=</span>val_results_hook,</span>
<span id="cb54-98"><a href="#cb54-98" aria-hidden="true" tabindex="-1"></a>                max_epochs<span class="op">=</span>max_epochs,</span>
<span id="cb54-99"><a href="#cb54-99" aria-hidden="true" tabindex="-1"></a>                transforms_override<span class="op">=</span>transforms_override,</span>
<span id="cb54-100"><a href="#cb54-100" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb54-101"><a href="#cb54-101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-102"><a href="#cb54-102" aria-hidden="true" tabindex="-1"></a>            f1 <span class="op">=</span> results.classification_report[<span class="st">"macro avg"</span>][<span class="st">"f1-score"</span>]</span>
<span id="cb54-103"><a href="#cb54-103" aria-hidden="true" tabindex="-1"></a>            f1_scores.append(f1)</span>
<span id="cb54-104"><a href="#cb54-104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-105"><a href="#cb54-105" aria-hidden="true" tabindex="-1"></a>            <span class="kw">del</span> model</span>
<span id="cb54-106"><a href="#cb54-106" aria-hidden="true" tabindex="-1"></a>            gc.collect()</span>
<span id="cb54-107"><a href="#cb54-107" aria-hidden="true" tabindex="-1"></a>            torch.cuda.empty_cache()</span>
<span id="cb54-108"><a href="#cb54-108" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-109"><a href="#cb54-109" aria-hidden="true" tabindex="-1"></a>        mean_f1_score <span class="op">=</span> statistics.fmean(f1_scores)</span>
<span id="cb54-110"><a href="#cb54-110" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> mean_f1_score</span>
<span id="cb54-111"><a href="#cb54-111" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-112"><a href="#cb54-112" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> objective</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<section id="multilayer-perceptron-1" class="level3">
<h3 class="anchored" data-anchor-id="multilayer-perceptron-1">Multilayer Perceptron</h3>
<p>The hyperparameters for the MLP are as follows:</p>
<ul>
<li><p><strong>Number of layers:</strong> The number of layers the MLP has, including the input and output layers. A higher number of layers allows the model to capture more complex patterns. However, setting it too high can lead to overfitting, vanishing/exploding gradients and infeasible computational requirements.</p></li>
<li><p><strong>Number of units in each layer:</strong> The number of neurons in a specific layer. Each layer has its number of units tuned (except the first and last layers since those are fixed), so they may not always have the same number of units. Having more units increases the model’s ability to learn complex patterns, but too many units can lead to overfitting or high computational requirements.</p></li>
<li><p><strong>Activation function:</strong> The activation function to use after each linear layer. Affects how well the network captures nonlinear relationships.</p></li>
</ul>
<p>I would have liked to further explore other hyperparameters, such as whether to use dropout and regularisation, but the search space was starting to get too large to reasonably explore in time.</p>
<div id="76eda0c1" class="cell">
<div class="sourceCode cell-code" id="cb55"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> suggest_mlp_params(trial: optuna.trial.Trial):</span>
<span id="cb55-2"><a href="#cb55-2" aria-hidden="true" tabindex="-1"></a>    n_layers: <span class="bu">int</span> <span class="op">=</span> trial.suggest_int(<span class="st">"mlp_n_layers"</span>, <span class="dv">3</span>, <span class="dv">5</span>)</span>
<span id="cb55-3"><a href="#cb55-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-4"><a href="#cb55-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Suggest size of the second layer.</span></span>
<span id="cb55-5"><a href="#cb55-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># This layer is given a smaller size than the rest because</span></span>
<span id="cb55-6"><a href="#cb55-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># it will be connected to every single pixel of the input image,</span></span>
<span id="cb55-7"><a href="#cb55-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># which can lead to the model having too many parameters</span></span>
<span id="cb55-8"><a href="#cb55-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># and hence exhaust GPU memory.</span></span>
<span id="cb55-9"><a href="#cb55-9" aria-hidden="true" tabindex="-1"></a>    trial.suggest_categorical(<span class="st">"mlp_layer_1_size"</span>, [<span class="dv">256</span>, <span class="dv">512</span>])</span>
<span id="cb55-10"><a href="#cb55-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-11"><a href="#cb55-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Exclude the first and last layers since their sizes are predetermined.</span></span>
<span id="cb55-12"><a href="#cb55-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Also exclude the second layer since we already suggested a (smaller)</span></span>
<span id="cb55-13"><a href="#cb55-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># size for it.</span></span>
<span id="cb55-14"><a href="#cb55-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> idx <span class="kw">in</span> <span class="bu">range</span>(n_layers <span class="op">-</span> <span class="dv">3</span>):</span>
<span id="cb55-15"><a href="#cb55-15" aria-hidden="true" tabindex="-1"></a>        trial.suggest_categorical(<span class="ss">f"mlp_layer_</span><span class="sc">{</span>idx <span class="op">+</span> <span class="dv">2</span><span class="sc">}</span><span class="ss">_size"</span>, [<span class="dv">256</span>, <span class="dv">512</span>, <span class="dv">1024</span>, <span class="dv">2048</span>])</span>
<span id="cb55-16"><a href="#cb55-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-17"><a href="#cb55-17" aria-hidden="true" tabindex="-1"></a>    activation_f_str <span class="op">=</span> trial.suggest_categorical(</span>
<span id="cb55-18"><a href="#cb55-18" aria-hidden="true" tabindex="-1"></a>        <span class="st">"mlp_activation_f"</span>,</span>
<span id="cb55-19"><a href="#cb55-19" aria-hidden="true" tabindex="-1"></a>        [<span class="st">"relu"</span>, <span class="st">"sigmoid"</span>, <span class="st">"softplus"</span>],</span>
<span id="cb55-20"><a href="#cb55-20" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb55-21"><a href="#cb55-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-22"><a href="#cb55-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-23"><a href="#cb55-23" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> mlp_from_params(params: <span class="bu">dict</span>) <span class="op">-&gt;</span> MultilayerPerceptron:</span>
<span id="cb55-24"><a href="#cb55-24" aria-hidden="true" tabindex="-1"></a>    n_hidden_layers <span class="op">=</span> params[<span class="st">"mlp_n_layers"</span>] <span class="op">-</span> <span class="dv">2</span></span>
<span id="cb55-25"><a href="#cb55-25" aria-hidden="true" tabindex="-1"></a>    layer_sizes <span class="op">=</span> []</span>
<span id="cb55-26"><a href="#cb55-26" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> idx <span class="kw">in</span> <span class="bu">range</span>(n_hidden_layers):</span>
<span id="cb55-27"><a href="#cb55-27" aria-hidden="true" tabindex="-1"></a>        layer_sizes.append(params[<span class="ss">f"mlp_layer_</span><span class="sc">{</span>idx <span class="op">+</span> <span class="dv">1</span><span class="sc">}</span><span class="ss">_size"</span>])</span>
<span id="cb55-28"><a href="#cb55-28" aria-hidden="true" tabindex="-1"></a>    layer_sizes.append(<span class="dv">10</span>)  <span class="co"># Final prediction layer.</span></span>
<span id="cb55-29"><a href="#cb55-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-30"><a href="#cb55-30" aria-hidden="true" tabindex="-1"></a>    activation_f <span class="op">=</span> {</span>
<span id="cb55-31"><a href="#cb55-31" aria-hidden="true" tabindex="-1"></a>        <span class="st">"relu"</span>: torch.nn.ReLU,</span>
<span id="cb55-32"><a href="#cb55-32" aria-hidden="true" tabindex="-1"></a>        <span class="st">"sigmoid"</span>: torch.nn.Sigmoid,</span>
<span id="cb55-33"><a href="#cb55-33" aria-hidden="true" tabindex="-1"></a>        <span class="st">"softplus"</span>: torch.nn.Softplus,</span>
<span id="cb55-34"><a href="#cb55-34" aria-hidden="true" tabindex="-1"></a>    }[params[<span class="st">"mlp_activation_f"</span>]]</span>
<span id="cb55-35"><a href="#cb55-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-36"><a href="#cb55-36" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> torch.nn.Sequential(</span>
<span id="cb55-37"><a href="#cb55-37" aria-hidden="true" tabindex="-1"></a>        torch.nn.Flatten(),</span>
<span id="cb55-38"><a href="#cb55-38" aria-hidden="true" tabindex="-1"></a>        MultilayerPerceptron(layer_sizes, activation_f)</span>
<span id="cb55-39"><a href="#cb55-39" aria-hidden="true" tabindex="-1"></a>    )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="2a400875" class="cell">
<div class="sourceCode cell-code" id="cb56"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a>mlp_study <span class="op">=</span> optuna.create_study(</span>
<span id="cb56-2"><a href="#cb56-2" aria-hidden="true" tabindex="-1"></a>    study_name<span class="op">=</span><span class="st">"MLP study"</span>,</span>
<span id="cb56-3"><a href="#cb56-3" aria-hidden="true" tabindex="-1"></a>    sampler<span class="op">=</span>optuna.samplers.TPESampler(seed<span class="op">=</span><span class="dv">0</span>),</span>
<span id="cb56-4"><a href="#cb56-4" aria-hidden="true" tabindex="-1"></a>    pruner<span class="op">=</span>optuna.pruners.MedianPruner(n_startup_trials<span class="op">=</span><span class="dv">3</span>, n_warmup_steps<span class="op">=</span><span class="dv">5</span>),</span>
<span id="cb56-5"><a href="#cb56-5" aria-hidden="true" tabindex="-1"></a>    directions<span class="op">=</span>[<span class="st">"maximize"</span>],</span>
<span id="cb56-6"><a href="#cb56-6" aria-hidden="true" tabindex="-1"></a>    storage<span class="op">=</span>optuna.storages.JournalStorage(</span>
<span id="cb56-7"><a href="#cb56-7" aria-hidden="true" tabindex="-1"></a>        optuna.storages.journal.JournalFileBackend(<span class="st">"mlp_journal.log"</span>),</span>
<span id="cb56-8"><a href="#cb56-8" aria-hidden="true" tabindex="-1"></a>    ),</span>
<span id="cb56-9"><a href="#cb56-9" aria-hidden="true" tabindex="-1"></a>    load_if_exists<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb56-10"><a href="#cb56-10" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb56-11"><a href="#cb56-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-12"><a href="#cb56-12" aria-hidden="true" tabindex="-1"></a>mlp_study.optimize(</span>
<span id="cb56-13"><a href="#cb56-13" aria-hidden="true" tabindex="-1"></a>    make_objective(</span>
<span id="cb56-14"><a href="#cb56-14" aria-hidden="true" tabindex="-1"></a>        suggest_mlp_params,</span>
<span id="cb56-15"><a href="#cb56-15" aria-hidden="true" tabindex="-1"></a>        mlp_from_params,</span>
<span id="cb56-16"><a href="#cb56-16" aria-hidden="true" tabindex="-1"></a>        ds_train,</span>
<span id="cb56-17"><a href="#cb56-17" aria-hidden="true" tabindex="-1"></a>    ),</span>
<span id="cb56-18"><a href="#cb56-18" aria-hidden="true" tabindex="-1"></a>    n_trials<span class="op">=</span>N_TRIALS,</span>
<span id="cb56-19"><a href="#cb56-19" aria-hidden="true" tabindex="-1"></a>    gc_after_trial<span class="op">=</span><span class="va">True</span></span>
<span id="cb56-20"><a href="#cb56-20" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-container">
<div class="cell-output cell-output-stderr">
<pre><code>[I 2024-10-19 23:22:49,624] A new study created in Journal with name: MLP study</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 0 params: {'batch_size': 32, 'lr': 0.0007180374727086954, 'weight_decay': 0.006027633760716439, 'reduce_lr_factor': 0.05903948646972072, 'momentum': 0.8804944118743919, 'early_stop_patience': 2, 'resize_length': 128, 'grayscale': False, 'jitter': False, 'horizontal_flip': False, 'rotation': True, 'mlp_n_layers': 3, 'mlp_layer_1_size': 512, 'mlp_activation_f': 'softplus'}
Trial 0 fold 1</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 45]: : 1440it [00:04, 303.70it/s, mean train loss=1.02]
validate [batch 12]: : 384it [00:01, 288.69it/s, mean loss=1.16, macro f1=0.455]
train [epoch 2 batch 45]: : 1440it [00:04, 317.43it/s, mean train loss=0.718]
validate [batch 12]: : 384it [00:01, 280.00it/s, mean loss=0.833, macro f1=0.551]
train [epoch 3 batch 45]: : 1440it [00:04, 316.49it/s, mean train loss=0.639]
validate [batch 12]: : 384it [00:01, 288.45it/s, mean loss=0.776, macro f1=0.637]
train [epoch 4 batch 45]: : 1440it [00:04, 311.88it/s, mean train loss=0.577]
validate [batch 12]: : 384it [00:01, 289.62it/s, mean loss=0.99, macro f1=0.557]
train [epoch 5 batch 45]: : 1440it [00:04, 315.34it/s, mean train loss=0.552]
validate [batch 12]: : 384it [00:01, 287.94it/s, mean loss=1, macro f1=0.565]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Early stopping at epoch 5</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>test [batch 29]: : 928it [00:02, 310.55it/s, mean loss=1.12, macro f1=0.561]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 0 fold 2</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 45]: : 1440it [00:04, 311.10it/s, mean train loss=1.04]
validate [batch 12]: : 384it [00:01, 290.78it/s, mean loss=0.775, macro f1=0.553]
train [epoch 2 batch 45]: : 1440it [00:04, 315.27it/s, mean train loss=0.759]
validate [batch 12]: : 384it [00:01, 290.99it/s, mean loss=0.756, macro f1=0.597]
train [epoch 3 batch 45]: : 1440it [00:04, 315.25it/s, mean train loss=0.649]
validate [batch 12]: : 384it [00:01, 285.11it/s, mean loss=0.705, macro f1=0.61]
train [epoch 4 batch 45]: : 1440it [00:04, 316.99it/s, mean train loss=0.637]
validate [batch 12]: : 384it [00:01, 290.49it/s, mean loss=0.729, macro f1=0.612]
train [epoch 5 batch 45]: : 1440it [00:05, 264.14it/s, mean train loss=0.649]
validate [batch 12]: : 384it [00:01, 228.21it/s, mean loss=0.658, macro f1=0.645]
train [epoch 6 batch 45]: : 1440it [00:05, 287.25it/s, mean train loss=0.584]
validate [batch 12]: : 384it [00:01, 261.52it/s, mean loss=0.765, macro f1=0.643]
train [epoch 7 batch 45]: : 1440it [00:04, 306.48it/s, mean train loss=0.58]
validate [batch 12]: : 384it [00:01, 290.95it/s, mean loss=0.867, macro f1=0.622]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Early stopping at epoch 7</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>test [batch 29]: : 928it [00:02, 313.43it/s, mean loss=0.805, macro f1=0.619]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 0 fold 3</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 45]: : 1440it [00:04, 313.21it/s, mean train loss=1.02]
validate [batch 12]: : 384it [00:01, 288.27it/s, mean loss=1.21, macro f1=0.417]
train [epoch 2 batch 45]: : 1440it [00:04, 311.62it/s, mean train loss=0.736]
validate [batch 12]: : 384it [00:01, 287.49it/s, mean loss=0.842, macro f1=0.618]
train [epoch 3 batch 45]: : 1440it [00:04, 299.34it/s, mean train loss=0.713]
validate [batch 12]: : 384it [00:01, 282.64it/s, mean loss=0.913, macro f1=0.595]
train [epoch 4 batch 45]: : 1440it [00:04, 312.24it/s, mean train loss=0.637]
validate [batch 12]: : 384it [00:01, 285.67it/s, mean loss=0.928, macro f1=0.498]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Early stopping at epoch 4</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>test [batch 29]: : 928it [00:02, 317.66it/s, mean loss=0.946, macro f1=0.498]
[I 2024-10-19 23:24:42,002] Trial 0 finished with value: 0.5593087362010596 and parameters: {'batch_size': 32, 'lr': 0.0007180374727086954, 'weight_decay': 0.006027633760716439, 'reduce_lr_factor': 0.05903948646972072, 'momentum': 0.8804944118743919, 'early_stop_patience': 2, 'resize_length': 128, 'grayscale': False, 'jitter': False, 'horizontal_flip': False, 'rotation': True, 'mlp_n_layers': 3, 'mlp_layer_1_size': 512, 'mlp_activation_f': 'softplus'}. Best is trial 0 with value: 0.5593087362010596.</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 1 params: {'batch_size': 32, 'lr': 0.0004668645686304026, 'weight_decay': 0.007805291762864555, 'reduce_lr_factor': 0.02064469832820399, 'momentum': 0.9215849940522295, 'early_stop_patience': 1, 'resize_length': 224, 'grayscale': True, 'jitter': False, 'horizontal_flip': False, 'rotation': False, 'mlp_n_layers': 4, 'mlp_layer_1_size': 512, 'mlp_layer_2_size': 2048, 'mlp_activation_f': 'softplus'}
Trial 1 fold 1</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 45]: : 1440it [00:04, 318.07it/s, mean train loss=0.888]
validate [batch 12]: : 384it [00:01, 282.53it/s, mean loss=0.801, macro f1=0.543]
train [epoch 2 batch 45]: : 1440it [00:04, 317.27it/s, mean train loss=0.587]
validate [batch 12]: : 384it [00:01, 287.42it/s, mean loss=0.753, macro f1=0.643]
train [epoch 3 batch 45]: : 1440it [00:04, 317.14it/s, mean train loss=0.465]
validate [batch 12]: : 384it [00:01, 260.07it/s, mean loss=0.68, macro f1=0.615]
train [epoch 4 batch 45]: : 1440it [00:05, 287.72it/s, mean train loss=0.419]
validate [batch 12]: : 384it [00:01, 230.10it/s, mean loss=0.738, macro f1=0.643]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Early stopping at epoch 4</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>test [batch 29]: : 928it [00:03, 256.15it/s, mean loss=0.876, macro f1=0.594]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 1 fold 2</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 45]: : 1440it [00:05, 268.15it/s, mean train loss=0.998]
validate [batch 12]: : 384it [00:01, 287.02it/s, mean loss=0.824, macro f1=0.465]
train [epoch 2 batch 45]: : 1440it [00:04, 313.98it/s, mean train loss=0.641]
validate [batch 12]: : 384it [00:01, 284.55it/s, mean loss=0.717, macro f1=0.569]
train [epoch 3 batch 45]: : 1440it [00:04, 320.72it/s, mean train loss=0.512]
validate [batch 12]: : 384it [00:01, 259.50it/s, mean loss=0.631, macro f1=0.689]
train [epoch 4 batch 45]: : 1440it [00:04, 315.79it/s, mean train loss=0.426]
validate [batch 12]: : 384it [00:01, 286.77it/s, mean loss=0.66, macro f1=0.683]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Early stopping at epoch 4</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>test [batch 29]: : 928it [00:02, 315.31it/s, mean loss=0.701, macro f1=0.635]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 1 fold 3</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 45]: : 1440it [00:04, 318.63it/s, mean train loss=0.97]
validate [batch 12]: : 384it [00:01, 288.29it/s, mean loss=0.858, macro f1=0.492]
train [epoch 2 batch 45]: : 1440it [00:04, 318.71it/s, mean train loss=0.613]
validate [batch 12]: : 384it [00:01, 289.92it/s, mean loss=0.738, macro f1=0.626]
train [epoch 3 batch 45]: : 1440it [00:04, 321.49it/s, mean train loss=0.47]
validate [batch 12]: : 384it [00:01, 289.81it/s, mean loss=1.17, macro f1=0.517]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Early stopping at epoch 3</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>test [batch 29]: : 928it [00:03, 302.28it/s, mean loss=1.05, macro f1=0.549]
[I 2024-10-19 23:26:03,597] Trial 1 finished with value: 0.5927770480927247 and parameters: {'batch_size': 32, 'lr': 0.0004668645686304026, 'weight_decay': 0.007805291762864555, 'reduce_lr_factor': 0.02064469832820399, 'momentum': 0.9215849940522295, 'early_stop_patience': 1, 'resize_length': 224, 'grayscale': True, 'jitter': False, 'horizontal_flip': False, 'rotation': False, 'mlp_n_layers': 4, 'mlp_layer_1_size': 512, 'mlp_layer_2_size': 2048, 'mlp_activation_f': 'softplus'}. Best is trial 1 with value: 0.5927770480927247.</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 2 params: {'batch_size': 24, 'lr': 0.00013763703467830477, 'weight_decay': 0.0031542835092418387, 'reduce_lr_factor': 0.04273396938483604, 'momentum': 0.9083373863793971, 'early_stop_patience': 2, 'resize_length': 224, 'grayscale': False, 'jitter': False, 'horizontal_flip': False, 'rotation': True, 'mlp_n_layers': 3, 'mlp_layer_1_size': 256, 'mlp_activation_f': 'softplus'}
Trial 2 fold 1</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 60]: : 1440it [00:04, 294.02it/s, mean train loss=1.11]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 271.17it/s, mean loss=0.964, macro f1=0.481]
train [epoch 2 batch 60]: : 1440it [00:04, 315.20it/s, mean train loss=0.832]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 272.37it/s, mean loss=0.863, macro f1=0.604]
train [epoch 3 batch 60]: : 1440it [00:05, 281.28it/s, mean train loss=0.746]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 255.63it/s, mean loss=0.965, macro f1=0.572]
train [epoch 4 batch 60]: : 1440it [00:05, 280.02it/s, mean train loss=0.71]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 269.84it/s, mean loss=0.818, macro f1=0.601]
train [epoch 5 batch 60]: : 1440it [00:04, 295.38it/s, mean train loss=0.696]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 282.03it/s, mean loss=0.811, macro f1=0.607]
train [epoch 6 batch 60]: : 1440it [00:04, 315.13it/s, mean train loss=0.647]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 268.79it/s, mean loss=0.995, macro f1=0.561]
train [epoch 7 batch 60]: : 1440it [00:04, 314.14it/s, mean train loss=0.649]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 274.77it/s, mean loss=1.04, macro f1=0.519]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Early stopping at epoch 7</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>test [batch 38]: : 912it [00:02, 310.06it/s, mean loss=1.07, macro f1=0.551]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 2 fold 2</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 60]: : 1440it [00:04, 294.70it/s, mean train loss=1.15]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 264.40it/s, mean loss=0.867, macro f1=0.588]
train [epoch 2 batch 60]: : 1440it [00:04, 312.73it/s, mean train loss=0.881]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 282.58it/s, mean loss=0.822, macro f1=0.628]
train [epoch 3 batch 60]: : 1440it [00:05, 270.65it/s, mean train loss=0.762]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 268.46it/s, mean loss=0.819, macro f1=0.566]
train [epoch 4 batch 60]: : 1440it [00:04, 291.69it/s, mean train loss=0.801]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 283.46it/s, mean loss=0.766, macro f1=0.6]
train [epoch 5 batch 60]: : 1440it [00:04, 312.36it/s, mean train loss=0.723]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 276.49it/s, mean loss=0.817, macro f1=0.624]
train [epoch 6 batch 60]: : 1440it [00:04, 309.39it/s, mean train loss=0.668]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 274.67it/s, mean loss=0.954, macro f1=0.53]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Early stopping at epoch 6</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>test [batch 38]: : 912it [00:02, 315.11it/s, mean loss=0.917, macro f1=0.574]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 2 fold 3</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 60]: : 1440it [00:05, 281.62it/s, mean train loss=1.12]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 277.43it/s, mean loss=0.96, macro f1=0.469]
train [epoch 2 batch 60]: : 1440it [00:04, 292.89it/s, mean train loss=0.876]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 278.66it/s, mean loss=0.841, macro f1=0.628]
train [epoch 3 batch 60]: : 1440it [00:04, 293.35it/s, mean train loss=0.777]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 279.71it/s, mean loss=0.799, macro f1=0.666]
train [epoch 4 batch 60]: : 1440it [00:04, 293.06it/s, mean train loss=0.742]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 277.43it/s, mean loss=0.854, macro f1=0.664]
train [epoch 5 batch 60]: : 1440it [00:04, 308.60it/s, mean train loss=0.672]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 269.23it/s, mean loss=1.23, macro f1=0.474]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Early stopping at epoch 5</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>test [batch 38]: : 912it [00:02, 308.90it/s, mean loss=1.23, macro f1=0.498]
[I 2024-10-19 23:28:10,721] Trial 2 finished with value: 0.541029338155279 and parameters: {'batch_size': 24, 'lr': 0.00013763703467830477, 'weight_decay': 0.0031542835092418387, 'reduce_lr_factor': 0.04273396938483604, 'momentum': 0.9083373863793971, 'early_stop_patience': 2, 'resize_length': 224, 'grayscale': False, 'jitter': False, 'horizontal_flip': False, 'rotation': True, 'mlp_n_layers': 3, 'mlp_layer_1_size': 256, 'mlp_activation_f': 'softplus'}. Best is trial 1 with value: 0.5927770480927247.</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 3 params: {'batch_size': 24, 'lr': 0.0008395654584238159, 'weight_decay': 0.0009609840789396307, 'reduce_lr_factor': 0.09788135185120563, 'momentum': 0.8890437283130633, 'early_stop_patience': 3, 'resize_length': 160, 'grayscale': True, 'jitter': True, 'horizontal_flip': True, 'rotation': False, 'mlp_n_layers': 3, 'mlp_layer_1_size': 256, 'mlp_activation_f': 'sigmoid'}
Trial 3 fold 1</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 60]: : 1440it [00:05, 281.30it/s, mean train loss=1.39]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 272.19it/s, mean loss=1.15, macro f1=0.412]
train [epoch 2 batch 60]: : 1440it [00:05, 282.88it/s, mean train loss=1.1]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 270.48it/s, mean loss=1.03, macro f1=0.522]
train [epoch 3 batch 60]: : 1440it [00:05, 283.68it/s, mean train loss=0.973]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 267.16it/s, mean loss=0.966, macro f1=0.511]
train [epoch 4 batch 60]: : 1440it [00:06, 239.50it/s, mean train loss=0.884]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 256.53it/s, mean loss=0.896, macro f1=0.559]
train [epoch 5 batch 60]: : 1440it [00:06, 238.63it/s, mean train loss=0.893]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 214.66it/s, mean loss=0.859, macro f1=0.585]
train [epoch 6 batch 60]: : 1440it [00:05, 260.83it/s, mean train loss=0.841]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 268.32it/s, mean loss=0.91, macro f1=0.594]
train [epoch 7 batch 60]: : 1440it [00:05, 286.12it/s, mean train loss=0.802]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 270.27it/s, mean loss=0.854, macro f1=0.537]
train [epoch 8 batch 60]: : 1440it [00:05, 283.84it/s, mean train loss=0.757]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 263.41it/s, mean loss=0.823, macro f1=0.573]
train [epoch 9 batch 60]: : 1440it [00:05, 280.62it/s, mean train loss=0.722]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 239.77it/s, mean loss=0.795, macro f1=0.619]
train [epoch 10 batch 60]: : 1440it [00:05, 281.28it/s, mean train loss=0.691]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 265.73it/s, mean loss=0.88, macro f1=0.563]
train [epoch 11 batch 60]: : 1440it [00:05, 278.44it/s, mean train loss=0.701]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 259.96it/s, mean loss=0.786, macro f1=0.592]
train [epoch 12 batch 60]: : 1440it [00:05, 277.27it/s, mean train loss=0.692]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 267.52it/s, mean loss=0.82, macro f1=0.599]
train [epoch 13 batch 60]: : 1440it [00:05, 279.65it/s, mean train loss=0.682]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 268.74it/s, mean loss=0.851, macro f1=0.612]
train [epoch 14 batch 60]: : 1440it [00:05, 272.12it/s, mean train loss=0.626]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 266.22it/s, mean loss=0.748, macro f1=0.628]
train [epoch 15 batch 60]: : 1440it [00:05, 281.01it/s, mean train loss=0.608]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 262.75it/s, mean loss=0.73, macro f1=0.641]
train [epoch 16 batch 60]: : 1440it [00:05, 279.72it/s, mean train loss=0.582]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 262.20it/s, mean loss=0.83, macro f1=0.597]
train [epoch 17 batch 60]: : 1440it [00:05, 281.40it/s, mean train loss=0.569]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 259.31it/s, mean loss=0.731, macro f1=0.614]
train [epoch 18 batch 60]: : 1440it [00:05, 281.23it/s, mean train loss=0.544]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 268.67it/s, mean loss=0.742, macro f1=0.596]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Early stopping at epoch 18</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>test [batch 38]: : 912it [00:03, 283.01it/s, mean loss=0.797, macro f1=0.576]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 3 fold 2</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 60]: : 1440it [00:05, 281.17it/s, mean train loss=1.43]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 265.28it/s, mean loss=1.14, macro f1=0.392]
train [epoch 2 batch 60]: : 1440it [00:05, 282.57it/s, mean train loss=1.08]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 261.88it/s, mean loss=1.03, macro f1=0.519]
train [epoch 3 batch 60]: : 1440it [00:05, 282.55it/s, mean train loss=0.991]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 266.93it/s, mean loss=0.958, macro f1=0.507]
train [epoch 4 batch 60]: : 1440it [00:05, 280.80it/s, mean train loss=0.965]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 268.29it/s, mean loss=0.9, macro f1=0.524]
train [epoch 5 batch 60]: : 1440it [00:05, 253.26it/s, mean train loss=0.894]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 253.68it/s, mean loss=0.892, macro f1=0.535]
train [epoch 6 batch 60]: : 1440it [00:05, 270.74it/s, mean train loss=0.836]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 253.78it/s, mean loss=0.895, macro f1=0.507]
train [epoch 7 batch 60]: : 1440it [00:05, 247.18it/s, mean train loss=0.84]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 263.34it/s, mean loss=0.839, macro f1=0.555]
train [epoch 8 batch 60]: : 1440it [00:05, 282.31it/s, mean train loss=0.767]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 267.10it/s, mean loss=0.828, macro f1=0.564]
train [epoch 9 batch 60]: : 1440it [00:05, 282.69it/s, mean train loss=0.757]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 232.21it/s, mean loss=0.825, macro f1=0.568]
train [epoch 10 batch 60]: : 1440it [00:05, 278.18it/s, mean train loss=0.704]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 265.27it/s, mean loss=1.05, macro f1=0.521]
train [epoch 11 batch 60]: : 1440it [00:05, 282.80it/s, mean train loss=0.709]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 269.49it/s, mean loss=0.815, macro f1=0.599]
train [epoch 12 batch 60]: : 1440it [00:05, 282.99it/s, mean train loss=0.669]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 261.43it/s, mean loss=0.821, macro f1=0.597]
train [epoch 13 batch 60]: : 1440it [00:05, 282.84it/s, mean train loss=0.651]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 269.17it/s, mean loss=0.817, macro f1=0.598]
train [epoch 14 batch 60]: : 1440it [00:05, 267.71it/s, mean train loss=0.62]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 268.33it/s, mean loss=0.828, macro f1=0.592]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Early stopping at epoch 14</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>test [batch 38]: : 912it [00:02, 310.44it/s, mean loss=0.842, macro f1=0.556]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 3 fold 3</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 60]: : 1440it [00:05, 283.63it/s, mean train loss=1.42]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 268.80it/s, mean loss=1.19, macro f1=0.39]
train [epoch 2 batch 60]: : 1440it [00:05, 282.65it/s, mean train loss=1.12]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 265.08it/s, mean loss=1.07, macro f1=0.444]
train [epoch 3 batch 60]: : 1440it [00:05, 281.95it/s, mean train loss=1.02]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 271.42it/s, mean loss=1.01, macro f1=0.445]
train [epoch 4 batch 60]: : 1440it [00:05, 278.90it/s, mean train loss=0.953]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 245.01it/s, mean loss=1, macro f1=0.539]
train [epoch 5 batch 60]: : 1440it [00:05, 283.00it/s, mean train loss=0.879]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 262.83it/s, mean loss=0.937, macro f1=0.521]
train [epoch 6 batch 60]: : 1440it [00:05, 282.69it/s, mean train loss=0.86]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 262.82it/s, mean loss=0.956, macro f1=0.572]
train [epoch 7 batch 60]: : 1440it [00:05, 280.95it/s, mean train loss=0.797]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 264.37it/s, mean loss=0.948, macro f1=0.488]
train [epoch 8 batch 60]: : 1440it [00:05, 283.88it/s, mean train loss=0.769]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 268.05it/s, mean loss=0.877, macro f1=0.498]
train [epoch 9 batch 60]: : 1440it [00:05, 270.72it/s, mean train loss=0.761]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 266.44it/s, mean loss=0.87, macro f1=0.566]
train [epoch 10 batch 60]: : 1440it [00:05, 276.30it/s, mean train loss=0.73]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 258.79it/s, mean loss=0.858, macro f1=0.547]
train [epoch 11 batch 60]: : 1440it [00:06, 227.91it/s, mean train loss=0.651]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 210.83it/s, mean loss=0.909, macro f1=0.589]
train [epoch 12 batch 60]: : 1440it [00:06, 239.70it/s, mean train loss=0.665]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 266.98it/s, mean loss=0.862, macro f1=0.564]
train [epoch 13 batch 60]: : 1440it [00:05, 274.57it/s, mean train loss=0.639]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 259.63it/s, mean loss=0.877, macro f1=0.593]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Early stopping at epoch 13</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>test [batch 38]: : 912it [00:02, 305.43it/s, mean loss=0.818, macro f1=0.672]
[I 2024-10-19 23:33:34,724] Trial 3 finished with value: 0.6011556440238311 and parameters: {'batch_size': 24, 'lr': 0.0008395654584238159, 'weight_decay': 0.0009609840789396307, 'reduce_lr_factor': 0.09788135185120563, 'momentum': 0.8890437283130633, 'early_stop_patience': 3, 'resize_length': 160, 'grayscale': True, 'jitter': True, 'horizontal_flip': True, 'rotation': False, 'mlp_n_layers': 3, 'mlp_layer_1_size': 256, 'mlp_activation_f': 'sigmoid'}. Best is trial 3 with value: 0.6011556440238311.</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 4 params: {'batch_size': 32, 'lr': 0.0009300032356004519, 'weight_decay': 0.003185689524513237, 'reduce_lr_factor': 0.07006693419673135, 'momentum': 0.8250415938568345, 'early_stop_patience': 3, 'resize_length': 96, 'grayscale': False, 'jitter': False, 'horizontal_flip': False, 'rotation': False, 'mlp_n_layers': 5, 'mlp_layer_1_size': 512, 'mlp_layer_2_size': 2048, 'mlp_layer_3_size': 512, 'mlp_activation_f': 'softplus'}
Trial 4 fold 1</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 45]: : 1440it [00:04, 331.45it/s, mean train loss=0.906]
validate [batch 12]: : 384it [00:01, 285.61it/s, mean loss=0.875, macro f1=0.632]
train [epoch 2 batch 45]: : 1440it [00:04, 329.44it/s, mean train loss=0.564]
validate [batch 12]: : 384it [00:01, 288.01it/s, mean loss=0.664, macro f1=0.693]
train [epoch 3 batch 45]: : 1440it [00:04, 329.94it/s, mean train loss=0.403]
validate [batch 12]: : 384it [00:01, 281.32it/s, mean loss=0.626, macro f1=0.713]
train [epoch 4 batch 45]: : 1440it [00:04, 331.27it/s, mean train loss=0.353]
validate [batch 12]: : 384it [00:01, 286.60it/s, mean loss=0.746, macro f1=0.642]
train [epoch 5 batch 45]: : 1440it [00:04, 305.59it/s, mean train loss=0.266]
validate [batch 12]: : 384it [00:01, 286.65it/s, mean loss=0.738, macro f1=0.706]
train [epoch 6 batch 45]: : 1440it [00:04, 328.57it/s, mean train loss=0.251]
validate [batch 12]: : 384it [00:01, 284.58it/s, mean loss=0.734, macro f1=0.697]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Early stopping at epoch 6</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>test [batch 29]: : 928it [00:02, 313.78it/s, mean loss=0.773, macro f1=0.688]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 4 fold 2</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 45]: : 1440it [00:04, 330.17it/s, mean train loss=0.941]
validate [batch 12]: : 384it [00:01, 285.00it/s, mean loss=0.877, macro f1=0.571]
train [epoch 2 batch 45]: : 1440it [00:04, 328.60it/s, mean train loss=0.523]
validate [batch 12]: : 384it [00:01, 290.12it/s, mean loss=0.756, macro f1=0.643]
train [epoch 3 batch 45]: : 1440it [00:04, 331.79it/s, mean train loss=0.407]
validate [batch 12]: : 384it [00:01, 263.79it/s, mean loss=0.767, macro f1=0.578]
train [epoch 4 batch 45]: : 1440it [00:04, 319.67it/s, mean train loss=0.332]
validate [batch 12]: : 384it [00:01, 291.67it/s, mean loss=0.8, macro f1=0.661]
train [epoch 5 batch 45]: : 1440it [00:04, 330.17it/s, mean train loss=0.272]
validate [batch 12]: : 384it [00:01, 288.54it/s, mean loss=0.71, macro f1=0.684]
train [epoch 6 batch 45]: : 1440it [00:04, 330.06it/s, mean train loss=0.227]
validate [batch 12]: : 384it [00:01, 287.46it/s, mean loss=0.625, macro f1=0.725]
train [epoch 7 batch 45]: : 1440it [00:04, 325.56it/s, mean train loss=0.186]
validate [batch 12]: : 384it [00:01, 287.85it/s, mean loss=0.787, macro f1=0.711]
train [epoch 8 batch 45]: : 1440it [00:04, 329.45it/s, mean train loss=0.151]
validate [batch 12]: : 384it [00:01, 279.62it/s, mean loss=0.762, macro f1=0.707]
train [epoch 9 batch 45]: : 1440it [00:04, 305.04it/s, mean train loss=0.135]
validate [batch 12]: : 384it [00:01, 286.56it/s, mean loss=1.03, macro f1=0.663]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Early stopping at epoch 9</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>test [batch 29]: : 928it [00:02, 312.19it/s, mean loss=0.9, macro f1=0.652]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 4 fold 3</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 45]: : 1440it [00:04, 329.18it/s, mean train loss=0.968]
validate [batch 12]: : 384it [00:01, 291.31it/s, mean loss=0.865, macro f1=0.591]
train [epoch 2 batch 45]: : 1440it [00:04, 302.61it/s, mean train loss=0.517]
validate [batch 12]: : 384it [00:01, 236.67it/s, mean loss=0.774, macro f1=0.623]
train [epoch 3 batch 45]: : 1440it [00:05, 261.66it/s, mean train loss=0.39]
validate [batch 12]: : 384it [00:01, 256.67it/s, mean loss=0.669, macro f1=0.713]
train [epoch 4 batch 45]: : 1440it [00:04, 289.79it/s, mean train loss=0.304]
validate [batch 12]: : 384it [00:01, 285.14it/s, mean loss=0.7, macro f1=0.681]
train [epoch 5 batch 45]: : 1440it [00:04, 326.94it/s, mean train loss=0.244]
validate [batch 12]: : 384it [00:01, 286.92it/s, mean loss=0.536, macro f1=0.729]
train [epoch 6 batch 45]: : 1440it [00:04, 330.20it/s, mean train loss=0.209]
validate [batch 12]: : 384it [00:01, 287.60it/s, mean loss=0.745, macro f1=0.645]
train [epoch 7 batch 45]: : 1440it [00:04, 330.34it/s, mean train loss=0.184]
validate [batch 12]: : 384it [00:01, 286.77it/s, mean loss=0.601, macro f1=0.738]
train [epoch 8 batch 45]: : 1440it [00:04, 327.72it/s, mean train loss=0.153]
validate [batch 12]: : 384it [00:01, 288.53it/s, mean loss=0.583, macro f1=0.718]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Early stopping at epoch 8</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>test [batch 29]: : 928it [00:02, 319.84it/s, mean loss=0.608, macro f1=0.695]
[I 2024-10-19 23:36:07,453] Trial 4 finished with value: 0.6783023402471665 and parameters: {'batch_size': 32, 'lr': 0.0009300032356004519, 'weight_decay': 0.003185689524513237, 'reduce_lr_factor': 0.07006693419673135, 'momentum': 0.8250415938568345, 'early_stop_patience': 3, 'resize_length': 96, 'grayscale': False, 'jitter': False, 'horizontal_flip': False, 'rotation': False, 'mlp_n_layers': 5, 'mlp_layer_1_size': 512, 'mlp_layer_2_size': 2048, 'mlp_layer_3_size': 512, 'mlp_activation_f': 'softplus'}. Best is trial 4 with value: 0.6783023402471665.</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 5 params: {'batch_size': 32, 'lr': 0.0008829180082363043, 'weight_decay': 0.006925315900777659, 'reduce_lr_factor': 0.07527288518376765, 'momentum': 0.8952516325660734, 'early_stop_patience': 3, 'resize_length': 160, 'grayscale': False, 'jitter': False, 'horizontal_flip': True, 'rotation': True, 'mlp_n_layers': 3, 'mlp_layer_1_size': 512, 'mlp_activation_f': 'softplus'}
Trial 5 fold 1</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 45]: : 1440it [00:05, 279.50it/s, mean train loss=1.05]
validate [batch 12]: : 384it [00:01, 285.92it/s, mean loss=0.97, macro f1=0.541]
train [epoch 2 batch 45]: : 1440it [00:04, 302.73it/s, mean train loss=0.717]
validate [batch 12]: : 384it [00:01, 286.62it/s, mean loss=0.953, macro f1=0.554]
train [epoch 3 batch 45]: : 1440it [00:04, 301.29it/s, mean train loss=0.691]
validate [batch 12]: : 384it [00:01, 285.64it/s, mean loss=0.776, macro f1=0.649]
train [epoch 4 batch 45]: : 1440it [00:04, 299.96it/s, mean train loss=0.638]
validate [batch 12]: : 384it [00:01, 285.84it/s, mean loss=0.774, macro f1=0.658]
train [epoch 5 batch 45]: : 1440it [00:04, 303.19it/s, mean train loss=0.562]
validate [batch 12]: : 384it [00:01, 283.79it/s, mean loss=0.856, macro f1=0.65]
train [epoch 6 batch 45]: : 1440it [00:04, 292.20it/s, mean train loss=0.556]
validate [batch 12]: : 384it [00:01, 286.69it/s, mean loss=0.673, macro f1=0.669]
train [epoch 7 batch 45]: : 1440it [00:04, 301.62it/s, mean train loss=0.526]
validate [batch 12]: : 384it [00:01, 279.35it/s, mean loss=0.855, macro f1=0.638]
train [epoch 8 batch 45]: : 1440it [00:04, 302.41it/s, mean train loss=0.527]
validate [batch 12]: : 384it [00:01, 288.19it/s, mean loss=0.769, macro f1=0.682]
train [epoch 9 batch 45]: : 1440it [00:04, 299.36it/s, mean train loss=0.531]
validate [batch 12]: : 384it [00:01, 284.79it/s, mean loss=1.05, macro f1=0.587]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Early stopping at epoch 9</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>test [batch 29]: : 928it [00:02, 315.34it/s, mean loss=1.19, macro f1=0.539]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 5 fold 2</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 45]: : 1440it [00:05, 279.60it/s, mean train loss=1.1]
validate [batch 12]: : 384it [00:01, 286.34it/s, mean loss=0.864, macro f1=0.489]
train [epoch 2 batch 45]: : 1440it [00:04, 302.16it/s, mean train loss=0.771]
validate [batch 12]: : 384it [00:01, 280.49it/s, mean loss=0.843, macro f1=0.535]
train [epoch 3 batch 45]: : 1440it [00:04, 302.09it/s, mean train loss=0.703]
validate [batch 12]: : 384it [00:01, 283.69it/s, mean loss=0.68, macro f1=0.647]
train [epoch 4 batch 45]: : 1440it [00:04, 301.40it/s, mean train loss=0.678]
validate [batch 12]: : 384it [00:01, 286.63it/s, mean loss=0.826, macro f1=0.639]
train [epoch 5 batch 45]: : 1440it [00:05, 259.02it/s, mean train loss=0.663]
validate [batch 12]: : 384it [00:01, 239.91it/s, mean loss=0.912, macro f1=0.545]
train [epoch 6 batch 45]: : 1440it [00:05, 246.06it/s, mean train loss=0.604]
validate [batch 12]: : 384it [00:01, 232.12it/s, mean loss=0.75, macro f1=0.645]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Early stopping at epoch 6</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>test [batch 29]: : 928it [00:03, 254.05it/s, mean loss=0.754, macro f1=0.646]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 5 fold 3</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 45]: : 1440it [00:04, 300.49it/s, mean train loss=0.958]
validate [batch 12]: : 384it [00:01, 285.39it/s, mean loss=0.932, macro f1=0.525]
train [epoch 2 batch 45]: : 1440it [00:04, 300.30it/s, mean train loss=0.778]
validate [batch 12]: : 384it [00:01, 282.72it/s, mean loss=0.821, macro f1=0.613]
train [epoch 3 batch 45]: : 1440it [00:04, 300.01it/s, mean train loss=0.689]
validate [batch 12]: : 384it [00:01, 285.63it/s, mean loss=0.92, macro f1=0.58]
train [epoch 4 batch 45]: : 1440it [00:05, 276.93it/s, mean train loss=0.639]
validate [batch 12]: : 384it [00:01, 285.47it/s, mean loss=0.791, macro f1=0.593]
train [epoch 5 batch 45]: : 1440it [00:04, 299.26it/s, mean train loss=0.594]
validate [batch 12]: : 384it [00:01, 283.38it/s, mean loss=0.824, macro f1=0.62]
train [epoch 6 batch 45]: : 1440it [00:04, 299.92it/s, mean train loss=0.599]
validate [batch 12]: : 384it [00:01, 286.44it/s, mean loss=0.843, macro f1=0.61]
train [epoch 7 batch 45]: : 1440it [00:04, 301.79it/s, mean train loss=0.579]
validate [batch 12]: : 384it [00:01, 286.16it/s, mean loss=1.65, macro f1=0.544]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Early stopping at epoch 7</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>test [batch 29]: : 928it [00:02, 311.51it/s, mean loss=1.74, macro f1=0.576]
[I 2024-10-19 23:38:44,377] Trial 5 finished with value: 0.5868615498905451 and parameters: {'batch_size': 32, 'lr': 0.0008829180082363043, 'weight_decay': 0.006925315900777659, 'reduce_lr_factor': 0.07527288518376765, 'momentum': 0.8952516325660734, 'early_stop_patience': 3, 'resize_length': 160, 'grayscale': False, 'jitter': False, 'horizontal_flip': True, 'rotation': True, 'mlp_n_layers': 3, 'mlp_layer_1_size': 512, 'mlp_activation_f': 'softplus'}. Best is trial 4 with value: 0.6783023402471665.</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 6 params: {'batch_size': 32, 'lr': 0.00043710425107963425, 'weight_decay': 0.00896546595851063, 'reduce_lr_factor': 0.043080568304310694, 'momentum': 0.8828143358004691, 'early_stop_patience': 3, 'resize_length': 192, 'grayscale': True, 'jitter': True, 'horizontal_flip': True, 'rotation': True, 'mlp_n_layers': 4, 'mlp_layer_1_size': 512, 'mlp_layer_2_size': 256, 'mlp_activation_f': 'softplus'}
Trial 6 fold 1</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 45]: : 1440it [00:05, 265.34it/s, mean train loss=1.26]
validate [batch 12]: : 384it [00:01, 288.05it/s, mean loss=1.07, macro f1=0.429]
train [epoch 2 batch 45]: : 1440it [00:05, 271.44it/s, mean train loss=0.895]
validate [batch 12]: : 384it [00:01, 284.68it/s, mean loss=0.844, macro f1=0.615]
train [epoch 3 batch 45]: : 1440it [00:05, 269.57it/s, mean train loss=0.824]
validate [batch 12]: : 384it [00:01, 286.61it/s, mean loss=0.79, macro f1=0.565]
train [epoch 4 batch 45]: : 1440it [00:05, 269.74it/s, mean train loss=0.803]
validate [batch 12]: : 384it [00:01, 279.69it/s, mean loss=0.751, macro f1=0.632]
train [epoch 5 batch 45]: : 1440it [00:05, 270.66it/s, mean train loss=0.717]
validate [batch 12]: : 384it [00:01, 286.53it/s, mean loss=0.976, macro f1=0.549]
train [epoch 6 batch 45]: : 1440it [00:05, 253.04it/s, mean train loss=0.741]
validate [batch 12]: : 384it [00:01, 285.41it/s, mean loss=0.799, macro f1=0.57]
train [epoch 7 batch 45]: : 1440it [00:05, 268.64it/s, mean train loss=0.73]
validate [batch 12]: : 384it [00:01, 284.35it/s, mean loss=0.843, macro f1=0.638]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Early stopping at epoch 7</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>test [batch 29]: : 928it [00:02, 318.07it/s, mean loss=0.876, macro f1=0.612]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 6 fold 2</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 45]: : 1440it [00:05, 269.27it/s, mean train loss=1.24]
validate [batch 12]: : 384it [00:01, 287.78it/s, mean loss=1, macro f1=0.428]
train [epoch 2 batch 45]: : 1440it [00:05, 269.73it/s, mean train loss=0.943]
validate [batch 12]: : 384it [00:01, 285.76it/s, mean loss=0.863, macro f1=0.472]
train [epoch 3 batch 45]: : 1440it [00:05, 244.95it/s, mean train loss=0.851]
validate [batch 12]: : 384it [00:01, 237.99it/s, mean loss=0.787, macro f1=0.553]
train [epoch 4 batch 45]: : 1440it [00:06, 221.34it/s, mean train loss=0.818]
validate [batch 12]: : 384it [00:01, 233.22it/s, mean loss=0.787, macro f1=0.585]
train [epoch 5 batch 45]: : 1440it [00:05, 240.39it/s, mean train loss=0.786]
validate [batch 12]: : 384it [00:01, 286.32it/s, mean loss=0.854, macro f1=0.51]
train [epoch 6 batch 45]: : 1440it [00:05, 269.78it/s, mean train loss=0.757]
validate [batch 12]: : 384it [00:01, 280.42it/s, mean loss=0.762, macro f1=0.565]
[I 2024-10-19 23:40:22,987] Trial 6 pruned. </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 7 params: {'batch_size': 32, 'lr': 0.000975766289952857, 'weight_decay': 0.00855803342392611, 'reduce_lr_factor': 0.011054267576650179, 'momentum': 0.8683958322508891, 'early_stop_patience': 3, 'resize_length': 64, 'grayscale': True, 'jitter': True, 'horizontal_flip': True, 'rotation': False, 'mlp_n_layers': 5, 'mlp_layer_1_size': 512, 'mlp_layer_2_size': 2048, 'mlp_layer_3_size': 2048, 'mlp_activation_f': 'sigmoid'}
Trial 7 fold 1</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 45]: : 1440it [00:05, 265.89it/s, mean train loss=1.4]
validate [batch 12]: : 384it [00:01, 286.80it/s, mean loss=1.17, macro f1=0.369]
train [epoch 2 batch 45]: : 1440it [00:05, 285.25it/s, mean train loss=0.935]
validate [batch 12]: : 384it [00:01, 290.93it/s, mean loss=0.859, macro f1=0.587]
train [epoch 3 batch 45]: : 1440it [00:05, 284.32it/s, mean train loss=0.843]
validate [batch 12]: : 384it [00:01, 287.50it/s, mean loss=0.792, macro f1=0.562]
train [epoch 4 batch 45]: : 1440it [00:05, 285.79it/s, mean train loss=0.75]
validate [batch 12]: : 384it [00:01, 289.70it/s, mean loss=0.729, macro f1=0.602]
train [epoch 5 batch 45]: : 1440it [00:05, 282.88it/s, mean train loss=0.729]
validate [batch 12]: : 384it [00:01, 274.13it/s, mean loss=0.702, macro f1=0.606]
train [epoch 6 batch 45]: : 1440it [00:05, 278.53it/s, mean train loss=0.665]
validate [batch 12]: : 384it [00:01, 289.70it/s, mean loss=0.706, macro f1=0.609]
[I 2024-10-19 23:41:04,429] Trial 7 pruned. </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 8 params: {'batch_size': 24, 'lr': 0.0009449286660840943, 'weight_decay': 0.007395507950492875, 'reduce_lr_factor': 0.054141292775581044, 'momentum': 0.8432087793149314, 'early_stop_patience': 1, 'resize_length': 32, 'grayscale': True, 'jitter': True, 'horizontal_flip': True, 'rotation': False, 'mlp_n_layers': 4, 'mlp_layer_1_size': 512, 'mlp_layer_2_size': 256, 'mlp_activation_f': 'sigmoid'}
Trial 8 fold 1</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 60]: : 1440it [00:05, 285.28it/s, mean train loss=1.74]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 263.00it/s, mean loss=1.43, macro f1=0.251]
train [epoch 2 batch 60]: : 1440it [00:05, 282.07it/s, mean train loss=1.33]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 267.37it/s, mean loss=1.23, macro f1=0.302]
train [epoch 3 batch 60]: : 1440it [00:05, 282.87it/s, mean train loss=1.23]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 266.95it/s, mean loss=1.14, macro f1=0.37]
train [epoch 4 batch 60]: : 1440it [00:05, 271.59it/s, mean train loss=1.16]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 265.50it/s, mean loss=1.07, macro f1=0.463]
train [epoch 5 batch 60]: : 1440it [00:05, 283.61it/s, mean train loss=1.07]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 265.84it/s, mean loss=1.01, macro f1=0.43]
train [epoch 6 batch 60]: : 1440it [00:05, 282.05it/s, mean train loss=1.02]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 266.92it/s, mean loss=0.99, macro f1=0.412]
[I 2024-10-19 23:41:45,854] Trial 8 pruned. </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 9 params: {'batch_size': 24, 'lr': 0.0005924339424180696, 'weight_decay': 0.008310484552361904, 'reduce_lr_factor': 0.06660836592320339, 'momentum': 0.9658036245350051, 'early_stop_patience': 1, 'resize_length': 192, 'grayscale': False, 'jitter': True, 'horizontal_flip': True, 'rotation': True, 'mlp_n_layers': 4, 'mlp_layer_1_size': 512, 'mlp_layer_2_size': 1024, 'mlp_activation_f': 'sigmoid'}
Trial 9 fold 1</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 60]: : 1440it [00:05, 241.72it/s, mean train loss=1.26]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 261.46it/s, mean loss=1.2, macro f1=0.331]
train [epoch 2 batch 60]: : 1440it [00:05, 240.83it/s, mean train loss=0.878]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 228.43it/s, mean loss=0.954, macro f1=0.515]
train [epoch 3 batch 60]: : 1440it [00:06, 239.94it/s, mean train loss=0.79]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 263.81it/s, mean loss=0.909, macro f1=0.598]
train [epoch 4 batch 60]: : 1440it [00:06, 218.50it/s, mean train loss=0.72]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 206.99it/s, mean loss=0.733, macro f1=0.662]
train [epoch 5 batch 60]: : 1440it [00:07, 203.72it/s, mean train loss=0.659]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 209.82it/s, mean loss=0.76, macro f1=0.684]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Early stopping at epoch 5</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>test [batch 38]: : 912it [00:03, 274.89it/s, mean loss=0.81, macro f1=0.665]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 9 fold 2</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 60]: : 1440it [00:06, 228.81it/s, mean train loss=1.31]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 262.50it/s, mean loss=1.07, macro f1=0.408]
train [epoch 2 batch 60]: : 1440it [00:05, 241.79it/s, mean train loss=0.949]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 261.20it/s, mean loss=0.818, macro f1=0.543]
train [epoch 3 batch 60]: : 1440it [00:06, 239.00it/s, mean train loss=0.819]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 261.75it/s, mean loss=0.766, macro f1=0.565]
train [epoch 4 batch 60]: : 1440it [00:05, 240.70it/s, mean train loss=0.766]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 260.72it/s, mean loss=0.849, macro f1=0.563]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Early stopping at epoch 4</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>test [batch 38]: : 912it [00:03, 288.87it/s, mean loss=0.884, macro f1=0.549]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 9 fold 3</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 60]: : 1440it [00:05, 242.34it/s, mean train loss=1.18]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 259.74it/s, mean loss=1.18, macro f1=0.31]
train [epoch 2 batch 60]: : 1440it [00:05, 241.82it/s, mean train loss=0.869]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 262.47it/s, mean loss=0.856, macro f1=0.458]
train [epoch 3 batch 60]: : 1440it [00:05, 241.22it/s, mean train loss=0.758]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 261.09it/s, mean loss=1.2, macro f1=0.45]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Early stopping at epoch 3</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>test [batch 38]: : 912it [00:03, 303.69it/s, mean loss=1.07, macro f1=0.522]
[I 2024-10-19 23:43:32,294] Trial 9 finished with value: 0.5787833240374795 and parameters: {'batch_size': 24, 'lr': 0.0005924339424180696, 'weight_decay': 0.008310484552361904, 'reduce_lr_factor': 0.06660836592320339, 'momentum': 0.9658036245350051, 'early_stop_patience': 1, 'resize_length': 192, 'grayscale': False, 'jitter': True, 'horizontal_flip': True, 'rotation': True, 'mlp_n_layers': 4, 'mlp_layer_1_size': 512, 'mlp_layer_2_size': 1024, 'mlp_activation_f': 'sigmoid'}. Best is trial 4 with value: 0.6783023402471665.</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 10 params: {'batch_size': 32, 'lr': 0.0002329857865536498, 'weight_decay': 0.0038215851878374606, 'reduce_lr_factor': 0.09308742500540024, 'momentum': 0.8052673117171141, 'early_stop_patience': 2, 'resize_length': 96, 'grayscale': False, 'jitter': False, 'horizontal_flip': False, 'rotation': False, 'mlp_n_layers': 5, 'mlp_layer_1_size': 256, 'mlp_layer_2_size': 512, 'mlp_layer_3_size': 512, 'mlp_activation_f': 'relu'}
Trial 10 fold 1</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 45]: : 1440it [00:04, 319.81it/s, mean train loss=1.38]
validate [batch 12]: : 384it [00:01, 288.29it/s, mean loss=1.1, macro f1=0.571]
train [epoch 2 batch 45]: : 1440it [00:04, 334.05it/s, mean train loss=0.879]
validate [batch 12]: : 384it [00:01, 287.44it/s, mean loss=0.933, macro f1=0.608]
train [epoch 3 batch 45]: : 1440it [00:04, 337.46it/s, mean train loss=0.688]
validate [batch 12]: : 384it [00:01, 287.57it/s, mean loss=0.814, macro f1=0.625]
train [epoch 4 batch 45]: : 1440it [00:04, 334.28it/s, mean train loss=0.619]
validate [batch 12]: : 384it [00:01, 288.72it/s, mean loss=0.705, macro f1=0.674]
train [epoch 5 batch 45]: : 1440it [00:04, 338.11it/s, mean train loss=0.496]
validate [batch 12]: : 384it [00:01, 281.69it/s, mean loss=0.687, macro f1=0.66]
train [epoch 6 batch 45]: : 1440it [00:04, 337.06it/s, mean train loss=0.408]
validate [batch 12]: : 384it [00:01, 248.42it/s, mean loss=0.681, macro f1=0.7]
train [epoch 7 batch 45]: : 1440it [00:04, 331.13it/s, mean train loss=0.38]
validate [batch 12]: : 384it [00:01, 290.47it/s, mean loss=0.644, macro f1=0.705]
train [epoch 8 batch 45]: : 1440it [00:04, 340.20it/s, mean train loss=0.336]
validate [batch 12]: : 384it [00:01, 289.67it/s, mean loss=0.608, macro f1=0.737]
train [epoch 9 batch 45]: : 1440it [00:04, 335.89it/s, mean train loss=0.28]
validate [batch 12]: : 384it [00:01, 288.72it/s, mean loss=0.637, macro f1=0.728]
train [epoch 10 batch 45]: : 1440it [00:05, 287.43it/s, mean train loss=0.279]
validate [batch 12]: : 384it [00:01, 231.61it/s, mean loss=0.562, macro f1=0.707]
train [epoch 11 batch 45]: : 1440it [00:05, 269.83it/s, mean train loss=0.266]
validate [batch 12]: : 384it [00:01, 219.01it/s, mean loss=0.583, macro f1=0.724]
train [epoch 12 batch 45]: : 1440it [00:05, 266.61it/s, mean train loss=0.223]
validate [batch 12]: : 384it [00:01, 286.00it/s, mean loss=0.56, macro f1=0.771]
train [epoch 13 batch 45]: : 1440it [00:04, 333.57it/s, mean train loss=0.198]
validate [batch 12]: : 384it [00:01, 288.31it/s, mean loss=0.617, macro f1=0.704]
train [epoch 14 batch 45]: : 1440it [00:04, 337.87it/s, mean train loss=0.198]
validate [batch 12]: : 384it [00:01, 288.03it/s, mean loss=0.535, macro f1=0.734]
train [epoch 15 batch 45]: : 1440it [00:04, 332.86it/s, mean train loss=0.167]
validate [batch 12]: : 384it [00:01, 289.15it/s, mean loss=0.51, macro f1=0.769]
train [epoch 16 batch 45]: : 1440it [00:04, 337.92it/s, mean train loss=0.168]
validate [batch 12]: : 384it [00:01, 285.19it/s, mean loss=0.603, macro f1=0.735]
train [epoch 17 batch 45]: : 1440it [00:04, 319.90it/s, mean train loss=0.158]
validate [batch 12]: : 384it [00:01, 286.55it/s, mean loss=0.574, macro f1=0.782]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Early stopping at epoch 17</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>test [batch 29]: : 928it [00:02, 318.95it/s, mean loss=0.591, macro f1=0.745]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 10 fold 2</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 45]: : 1440it [00:04, 333.32it/s, mean train loss=1.35]
validate [batch 12]: : 384it [00:01, 280.28it/s, mean loss=1.11, macro f1=0.584]
train [epoch 2 batch 45]: : 1440it [00:04, 334.12it/s, mean train loss=0.881]
validate [batch 12]: : 384it [00:01, 279.90it/s, mean loss=0.902, macro f1=0.615]
train [epoch 3 batch 45]: : 1440it [00:04, 336.04it/s, mean train loss=0.717]
validate [batch 12]: : 384it [00:01, 287.66it/s, mean loss=0.82, macro f1=0.62]
train [epoch 4 batch 45]: : 1440it [00:04, 334.65it/s, mean train loss=0.646]
validate [batch 12]: : 384it [00:01, 252.66it/s, mean loss=0.769, macro f1=0.692]
train [epoch 5 batch 45]: : 1440it [00:04, 336.40it/s, mean train loss=0.569]
validate [batch 12]: : 384it [00:01, 290.23it/s, mean loss=0.771, macro f1=0.622]
train [epoch 6 batch 45]: : 1440it [00:04, 333.54it/s, mean train loss=0.491]
validate [batch 12]: : 384it [00:01, 285.41it/s, mean loss=0.716, macro f1=0.698]
train [epoch 7 batch 45]: : 1440it [00:04, 335.40it/s, mean train loss=0.414]
validate [batch 12]: : 384it [00:01, 282.40it/s, mean loss=0.687, macro f1=0.697]
train [epoch 8 batch 45]: : 1440it [00:04, 335.09it/s, mean train loss=0.377]
validate [batch 12]: : 384it [00:01, 290.41it/s, mean loss=0.665, macro f1=0.687]
train [epoch 9 batch 45]: : 1440it [00:04, 333.35it/s, mean train loss=0.341]
validate [batch 12]: : 384it [00:01, 287.47it/s, mean loss=0.685, macro f1=0.709]
train [epoch 10 batch 45]: : 1440it [00:04, 324.42it/s, mean train loss=0.287]
validate [batch 12]: : 384it [00:01, 289.24it/s, mean loss=0.631, macro f1=0.698]
train [epoch 11 batch 45]: : 1440it [00:04, 335.40it/s, mean train loss=0.282]
validate [batch 12]: : 384it [00:01, 289.14it/s, mean loss=0.613, macro f1=0.749]
train [epoch 12 batch 45]: : 1440it [00:04, 337.26it/s, mean train loss=0.253]
validate [batch 12]: : 384it [00:01, 286.20it/s, mean loss=0.66, macro f1=0.733]
train [epoch 13 batch 45]: : 1440it [00:04, 336.64it/s, mean train loss=0.243]
validate [batch 12]: : 384it [00:01, 287.89it/s, mean loss=0.582, macro f1=0.725]
train [epoch 14 batch 45]: : 1440it [00:04, 335.10it/s, mean train loss=0.197]
validate [batch 12]: : 384it [00:01, 285.83it/s, mean loss=0.611, macro f1=0.742]
train [epoch 15 batch 45]: : 1440it [00:04, 289.73it/s, mean train loss=0.201]
validate [batch 12]: : 384it [00:01, 264.93it/s, mean loss=0.62, macro f1=0.721]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Early stopping at epoch 15</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>test [batch 29]: : 928it [00:03, 276.18it/s, mean loss=0.598, macro f1=0.707]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 10 fold 3</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 45]: : 1440it [00:05, 280.70it/s, mean train loss=1.37]
validate [batch 12]: : 384it [00:01, 264.18it/s, mean loss=1.12, macro f1=0.522]
train [epoch 2 batch 45]: : 1440it [00:04, 294.85it/s, mean train loss=0.877]
validate [batch 12]: : 384it [00:01, 288.91it/s, mean loss=0.887, macro f1=0.598]
train [epoch 3 batch 45]: : 1440it [00:04, 332.97it/s, mean train loss=0.662]
validate [batch 12]: : 384it [00:01, 286.97it/s, mean loss=0.83, macro f1=0.62]
train [epoch 4 batch 45]: : 1440it [00:04, 333.91it/s, mean train loss=0.592]
validate [batch 12]: : 384it [00:01, 240.65it/s, mean loss=0.758, macro f1=0.714]
train [epoch 5 batch 45]: : 1440it [00:04, 337.80it/s, mean train loss=0.489]
validate [batch 12]: : 384it [00:01, 288.10it/s, mean loss=0.745, macro f1=0.703]
train [epoch 6 batch 45]: : 1440it [00:04, 338.90it/s, mean train loss=0.4]
validate [batch 12]: : 384it [00:01, 284.86it/s, mean loss=0.667, macro f1=0.676]
train [epoch 7 batch 45]: : 1440it [00:04, 339.06it/s, mean train loss=0.376]
validate [batch 12]: : 384it [00:01, 287.64it/s, mean loss=0.677, macro f1=0.689]
train [epoch 8 batch 45]: : 1440it [00:04, 336.74it/s, mean train loss=0.305]
validate [batch 12]: : 384it [00:01, 289.36it/s, mean loss=0.671, macro f1=0.661]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Early stopping at epoch 8</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>test [batch 29]: : 928it [00:02, 320.94it/s, mean loss=0.633, macro f1=0.683]
[I 2024-10-19 23:47:48,103] Trial 10 finished with value: 0.7117729729542099 and parameters: {'batch_size': 32, 'lr': 0.0002329857865536498, 'weight_decay': 0.0038215851878374606, 'reduce_lr_factor': 0.09308742500540024, 'momentum': 0.8052673117171141, 'early_stop_patience': 2, 'resize_length': 96, 'grayscale': False, 'jitter': False, 'horizontal_flip': False, 'rotation': False, 'mlp_n_layers': 5, 'mlp_layer_1_size': 256, 'mlp_layer_2_size': 512, 'mlp_layer_3_size': 512, 'mlp_activation_f': 'relu'}. Best is trial 10 with value: 0.7117729729542099.</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 11 params: {'batch_size': 32, 'lr': 0.00019298475863362353, 'weight_decay': 0.003512101966155307, 'reduce_lr_factor': 0.09326175431644539, 'momentum': 0.800792649502636, 'early_stop_patience': 2, 'resize_length': 96, 'grayscale': False, 'jitter': False, 'horizontal_flip': False, 'rotation': False, 'mlp_n_layers': 5, 'mlp_layer_1_size': 256, 'mlp_layer_2_size': 512, 'mlp_layer_3_size': 512, 'mlp_activation_f': 'relu'}
Trial 11 fold 1</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 45]: : 1440it [00:04, 316.61it/s, mean train loss=1.44]
validate [batch 12]: : 384it [00:01, 285.76it/s, mean loss=1.12, macro f1=0.531]
train [epoch 2 batch 45]: : 1440it [00:04, 339.78it/s, mean train loss=0.921]
validate [batch 12]: : 384it [00:01, 293.73it/s, mean loss=0.943, macro f1=0.609]
train [epoch 3 batch 45]: : 1440it [00:04, 337.48it/s, mean train loss=0.753]
validate [batch 12]: : 384it [00:01, 290.91it/s, mean loss=0.822, macro f1=0.631]
train [epoch 4 batch 45]: : 1440it [00:04, 333.43it/s, mean train loss=0.648]
validate [batch 12]: : 384it [00:01, 286.28it/s, mean loss=0.75, macro f1=0.687]
train [epoch 5 batch 45]: : 1440it [00:04, 337.42it/s, mean train loss=0.561]
validate [batch 12]: : 384it [00:01, 290.33it/s, mean loss=0.719, macro f1=0.667]
train [epoch 6 batch 45]: : 1440it [00:04, 337.68it/s, mean train loss=0.49]
validate [batch 12]: : 384it [00:01, 227.31it/s, mean loss=0.669, macro f1=0.712]
train [epoch 7 batch 45]: : 1440it [00:04, 337.26it/s, mean train loss=0.431]
validate [batch 12]: : 384it [00:01, 289.04it/s, mean loss=0.695, macro f1=0.7]
train [epoch 8 batch 45]: : 1440it [00:04, 335.06it/s, mean train loss=0.391]
validate [batch 12]: : 384it [00:01, 289.10it/s, mean loss=0.653, macro f1=0.698]
train [epoch 9 batch 45]: : 1440it [00:04, 339.19it/s, mean train loss=0.327]
validate [batch 12]: : 384it [00:01, 283.12it/s, mean loss=0.619, macro f1=0.732]
train [epoch 10 batch 45]: : 1440it [00:04, 338.14it/s, mean train loss=0.314]
validate [batch 12]: : 384it [00:01, 293.01it/s, mean loss=0.597, macro f1=0.746]
train [epoch 11 batch 45]: : 1440it [00:04, 336.71it/s, mean train loss=0.267]
validate [batch 12]: : 384it [00:01, 287.89it/s, mean loss=0.61, macro f1=0.737]
train [epoch 12 batch 45]: : 1440it [00:04, 325.21it/s, mean train loss=0.261]
validate [batch 12]: : 384it [00:01, 290.02it/s, mean loss=0.581, macro f1=0.702]
train [epoch 13 batch 45]: : 1440it [00:04, 333.79it/s, mean train loss=0.243]
validate [batch 12]: : 384it [00:01, 272.34it/s, mean loss=0.526, macro f1=0.715]
train [epoch 14 batch 45]: : 1440it [00:05, 260.43it/s, mean train loss=0.231]
validate [batch 12]: : 384it [00:01, 228.54it/s, mean loss=0.52, macro f1=0.794]
train [epoch 15 batch 45]: : 1440it [00:05, 269.85it/s, mean train loss=0.204]
validate [batch 12]: : 384it [00:01, 238.74it/s, mean loss=0.537, macro f1=0.769]
train [epoch 16 batch 45]: : 1440it [00:04, 313.46it/s, mean train loss=0.198]
validate [batch 12]: : 384it [00:01, 264.63it/s, mean loss=0.569, macro f1=0.752]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Early stopping at epoch 16</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>test [batch 29]: : 928it [00:03, 291.03it/s, mean loss=0.608, macro f1=0.752]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 11 fold 2</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 45]: : 1440it [00:04, 333.70it/s, mean train loss=1.42]
validate [batch 12]: : 384it [00:01, 291.62it/s, mean loss=1.16, macro f1=0.508]
train [epoch 2 batch 45]: : 1440it [00:04, 337.26it/s, mean train loss=0.939]
validate [batch 12]: : 384it [00:01, 284.96it/s, mean loss=0.897, macro f1=0.602]
train [epoch 3 batch 45]: : 1440it [00:04, 337.99it/s, mean train loss=0.77]
validate [batch 12]: : 384it [00:01, 292.61it/s, mean loss=0.839, macro f1=0.625]
train [epoch 4 batch 45]: : 1440it [00:04, 338.09it/s, mean train loss=0.644]
validate [batch 12]: : 384it [00:01, 293.96it/s, mean loss=0.786, macro f1=0.635]
train [epoch 5 batch 45]: : 1440it [00:04, 331.50it/s, mean train loss=0.592]
validate [batch 12]: : 384it [00:01, 234.48it/s, mean loss=0.772, macro f1=0.694]
train [epoch 6 batch 45]: : 1440it [00:04, 334.86it/s, mean train loss=0.519]
validate [batch 12]: : 384it [00:01, 291.38it/s, mean loss=0.731, macro f1=0.69]
train [epoch 7 batch 45]: : 1440it [00:04, 338.79it/s, mean train loss=0.476]
validate [batch 12]: : 384it [00:01, 287.13it/s, mean loss=0.669, macro f1=0.694]
train [epoch 8 batch 45]: : 1440it [00:04, 336.26it/s, mean train loss=0.434]
validate [batch 12]: : 384it [00:01, 294.18it/s, mean loss=0.701, macro f1=0.706]
train [epoch 9 batch 45]: : 1440it [00:04, 337.19it/s, mean train loss=0.364]
validate [batch 12]: : 384it [00:01, 292.69it/s, mean loss=0.661, macro f1=0.75]
train [epoch 10 batch 45]: : 1440it [00:04, 339.65it/s, mean train loss=0.336]
validate [batch 12]: : 384it [00:01, 288.83it/s, mean loss=0.67, macro f1=0.733]
train [epoch 11 batch 45]: : 1440it [00:04, 318.47it/s, mean train loss=0.32]
validate [batch 12]: : 384it [00:01, 291.06it/s, mean loss=0.638, macro f1=0.701]
train [epoch 12 batch 45]: : 1440it [00:04, 337.42it/s, mean train loss=0.295]
validate [batch 12]: : 384it [00:01, 287.04it/s, mean loss=0.604, macro f1=0.751]
train [epoch 13 batch 45]: : 1440it [00:04, 338.40it/s, mean train loss=0.273]
validate [batch 12]: : 384it [00:01, 291.56it/s, mean loss=0.653, macro f1=0.754]
train [epoch 14 batch 45]: : 1440it [00:04, 336.84it/s, mean train loss=0.238]
validate [batch 12]: : 384it [00:01, 293.01it/s, mean loss=0.608, macro f1=0.735]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Early stopping at epoch 14</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>test [batch 29]: : 928it [00:02, 320.39it/s, mean loss=0.58, macro f1=0.74]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 11 fold 3</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 45]: : 1440it [00:04, 338.61it/s, mean train loss=1.46]
validate [batch 12]: : 384it [00:01, 256.24it/s, mean loss=1.22, macro f1=0.473]
train [epoch 2 batch 45]: : 1440it [00:04, 327.64it/s, mean train loss=0.949]
validate [batch 12]: : 384it [00:01, 285.79it/s, mean loss=0.946, macro f1=0.603]
train [epoch 3 batch 45]: : 1440it [00:04, 335.68it/s, mean train loss=0.736]
validate [batch 12]: : 384it [00:01, 290.92it/s, mean loss=0.85, macro f1=0.637]
train [epoch 4 batch 45]: : 1440it [00:04, 332.63it/s, mean train loss=0.667]
validate [batch 12]: : 384it [00:01, 287.85it/s, mean loss=0.802, macro f1=0.624]
train [epoch 5 batch 45]: : 1440it [00:05, 273.01it/s, mean train loss=0.552]
validate [batch 12]: : 384it [00:01, 236.70it/s, mean loss=0.813, macro f1=0.668]
train [epoch 6 batch 45]: : 1440it [00:04, 305.21it/s, mean train loss=0.478]
validate [batch 12]: : 384it [00:01, 230.17it/s, mean loss=0.706, macro f1=0.699]
train [epoch 7 batch 45]: : 1440it [00:05, 266.87it/s, mean train loss=0.428]
validate [batch 12]: : 384it [00:01, 265.89it/s, mean loss=0.69, macro f1=0.717]
train [epoch 8 batch 45]: : 1440it [00:04, 336.40it/s, mean train loss=0.407]
validate [batch 12]: : 384it [00:01, 290.03it/s, mean loss=0.721, macro f1=0.677]
train [epoch 9 batch 45]: : 1440it [00:04, 335.36it/s, mean train loss=0.344]
validate [batch 12]: : 384it [00:01, 290.81it/s, mean loss=0.676, macro f1=0.749]
train [epoch 10 batch 45]: : 1440it [00:04, 338.86it/s, mean train loss=0.304]
validate [batch 12]: : 384it [00:01, 288.83it/s, mean loss=0.639, macro f1=0.683]
train [epoch 11 batch 45]: : 1440it [00:04, 335.84it/s, mean train loss=0.291]
validate [batch 12]: : 384it [00:01, 289.28it/s, mean loss=0.676, macro f1=0.741]
train [epoch 12 batch 45]: : 1440it [00:04, 316.24it/s, mean train loss=0.254]
validate [batch 12]: : 384it [00:01, 282.50it/s, mean loss=0.628, macro f1=0.715]
train [epoch 13 batch 45]: : 1440it [00:04, 339.54it/s, mean train loss=0.213]
validate [batch 12]: : 384it [00:01, 291.29it/s, mean loss=0.62, macro f1=0.753]
train [epoch 14 batch 45]: : 1440it [00:04, 335.40it/s, mean train loss=0.222]
validate [batch 12]: : 384it [00:01, 289.54it/s, mean loss=0.604, macro f1=0.677]
train [epoch 15 batch 45]: : 1440it [00:04, 330.81it/s, mean train loss=0.215]
validate [batch 12]: : 384it [00:01, 292.03it/s, mean loss=0.665, macro f1=0.701]
train [epoch 16 batch 45]: : 1440it [00:04, 334.05it/s, mean train loss=0.183]
validate [batch 12]: : 384it [00:01, 289.44it/s, mean loss=0.606, macro f1=0.725]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Early stopping at epoch 16</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>test [batch 29]: : 928it [00:03, 306.65it/s, mean loss=0.558, macro f1=0.7]
[I 2024-10-19 23:52:39,549] Trial 11 finished with value: 0.7305797239149951 and parameters: {'batch_size': 32, 'lr': 0.00019298475863362353, 'weight_decay': 0.003512101966155307, 'reduce_lr_factor': 0.09326175431644539, 'momentum': 0.800792649502636, 'early_stop_patience': 2, 'resize_length': 96, 'grayscale': False, 'jitter': False, 'horizontal_flip': False, 'rotation': False, 'mlp_n_layers': 5, 'mlp_layer_1_size': 256, 'mlp_layer_2_size': 512, 'mlp_layer_3_size': 512, 'mlp_activation_f': 'relu'}. Best is trial 11 with value: 0.7305797239149951.</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 12 params: {'batch_size': 32, 'lr': 0.0001281119397065285, 'weight_decay': 0.004109676468829812, 'reduce_lr_factor': 0.0980354227939941, 'momentum': 0.8050855805255551, 'early_stop_patience': 2, 'resize_length': 96, 'grayscale': False, 'jitter': False, 'horizontal_flip': False, 'rotation': False, 'mlp_n_layers': 5, 'mlp_layer_1_size': 256, 'mlp_layer_2_size': 512, 'mlp_layer_3_size': 512, 'mlp_activation_f': 'relu'}
Trial 12 fold 1</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 45]: : 1440it [00:04, 335.89it/s, mean train loss=1.44]
validate [batch 12]: : 384it [00:01, 289.30it/s, mean loss=1.32, macro f1=0.467]
train [epoch 2 batch 45]: : 1440it [00:04, 338.82it/s, mean train loss=0.965]
validate [batch 12]: : 384it [00:01, 284.86it/s, mean loss=1, macro f1=0.534]
train [epoch 3 batch 45]: : 1440it [00:04, 332.79it/s, mean train loss=0.834]
validate [batch 12]: : 384it [00:01, 292.14it/s, mean loss=0.911, macro f1=0.581]
train [epoch 4 batch 45]: : 1440it [00:04, 333.22it/s, mean train loss=0.7]
validate [batch 12]: : 384it [00:01, 289.65it/s, mean loss=0.85, macro f1=0.613]
train [epoch 5 batch 45]: : 1440it [00:04, 337.42it/s, mean train loss=0.662]
validate [batch 12]: : 384it [00:01, 291.34it/s, mean loss=0.789, macro f1=0.613]
train [epoch 6 batch 45]: : 1440it [00:04, 314.61it/s, mean train loss=0.591]
validate [batch 12]: : 384it [00:01, 288.48it/s, mean loss=0.767, macro f1=0.672]
[I 2024-10-19 23:53:16,211] Trial 12 pruned. </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 13 params: {'batch_size': 32, 'lr': 0.0002784740828436272, 'weight_decay': 0.0011123062199767338, 'reduce_lr_factor': 0.08429081321822332, 'momentum': 0.806176734526658, 'early_stop_patience': 2, 'resize_length': 96, 'grayscale': False, 'jitter': False, 'horizontal_flip': False, 'rotation': False, 'mlp_n_layers': 5, 'mlp_layer_1_size': 256, 'mlp_layer_2_size': 512, 'mlp_layer_3_size': 512, 'mlp_activation_f': 'relu'}
Trial 13 fold 1</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 45]: : 1440it [00:04, 337.35it/s, mean train loss=1.39]
validate [batch 12]: : 384it [00:01, 282.05it/s, mean loss=1.15, macro f1=0.566]
train [epoch 2 batch 45]: : 1440it [00:04, 336.78it/s, mean train loss=0.843]
validate [batch 12]: : 384it [00:01, 288.47it/s, mean loss=0.94, macro f1=0.588]
train [epoch 3 batch 45]: : 1440it [00:04, 333.44it/s, mean train loss=0.657]
validate [batch 12]: : 384it [00:01, 288.08it/s, mean loss=0.803, macro f1=0.645]
train [epoch 4 batch 45]: : 1440it [00:04, 336.30it/s, mean train loss=0.594]
validate [batch 12]: : 384it [00:01, 290.12it/s, mean loss=0.731, macro f1=0.643]
train [epoch 5 batch 45]: : 1440it [00:05, 272.79it/s, mean train loss=0.479]
validate [batch 12]: : 384it [00:01, 279.26it/s, mean loss=0.689, macro f1=0.651]
train [epoch 6 batch 45]: : 1440it [00:05, 261.37it/s, mean train loss=0.401]
validate [batch 12]: : 384it [00:01, 227.85it/s, mean loss=0.678, macro f1=0.62]
[I 2024-10-19 23:53:55,149] Trial 13 pruned. </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 14 params: {'batch_size': 32, 'lr': 0.0002988026568338186, 'weight_decay': 0.004920242157193899, 'reduce_lr_factor': 0.0854873294208598, 'momentum': 0.8478496511845682, 'early_stop_patience': 2, 'resize_length': 32, 'grayscale': False, 'jitter': False, 'horizontal_flip': False, 'rotation': False, 'mlp_n_layers': 5, 'mlp_layer_1_size': 256, 'mlp_layer_2_size': 512, 'mlp_layer_3_size': 1024, 'mlp_activation_f': 'relu'}
Trial 14 fold 1</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 45]: : 1440it [00:05, 271.93it/s, mean train loss=1.53]
validate [batch 12]: : 384it [00:01, 278.90it/s, mean loss=1.22, macro f1=0.445]
train [epoch 2 batch 45]: : 1440it [00:04, 339.25it/s, mean train loss=0.879]
validate [batch 12]: : 384it [00:01, 285.44it/s, mean loss=0.9, macro f1=0.57]
train [epoch 3 batch 45]: : 1440it [00:04, 340.53it/s, mean train loss=0.705]
validate [batch 12]: : 384it [00:01, 255.08it/s, mean loss=0.769, macro f1=0.583]
train [epoch 4 batch 45]: : 1440it [00:04, 328.00it/s, mean train loss=0.654]
validate [batch 12]: : 384it [00:01, 287.06it/s, mean loss=0.725, macro f1=0.602]
train [epoch 5 batch 45]: : 1440it [00:04, 337.20it/s, mean train loss=0.548]
validate [batch 12]: : 384it [00:01, 288.99it/s, mean loss=0.672, macro f1=0.661]
train [epoch 6 batch 45]: : 1440it [00:04, 337.49it/s, mean train loss=0.52]
validate [batch 12]: : 384it [00:01, 285.82it/s, mean loss=0.671, macro f1=0.642]
[I 2024-10-19 23:54:32,805] Trial 14 pruned. </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 15 params: {'batch_size': 32, 'lr': 1.9502245825872046e-05, 'weight_decay': 0.002640899275157483, 'reduce_lr_factor': 0.08642302061376703, 'momentum': 0.8317387070980279, 'early_stop_patience': 2, 'resize_length': 64, 'grayscale': False, 'jitter': False, 'horizontal_flip': False, 'rotation': False, 'mlp_n_layers': 5, 'mlp_layer_1_size': 256, 'mlp_layer_2_size': 512, 'mlp_layer_3_size': 256, 'mlp_activation_f': 'relu'}
Trial 15 fold 1</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 45]: : 1440it [00:04, 340.15it/s, mean train loss=2.05]
validate [batch 12]: : 384it [00:01, 290.17it/s, mean loss=1.85, macro f1=0.257]
train [epoch 2 batch 45]: : 1440it [00:04, 339.01it/s, mean train loss=1.71]
validate [batch 12]: : 384it [00:01, 291.67it/s, mean loss=1.6, macro f1=0.322]
train [epoch 3 batch 45]: : 1440it [00:04, 318.07it/s, mean train loss=1.54]
validate [batch 12]: : 384it [00:01, 291.75it/s, mean loss=1.51, macro f1=0.402]
train [epoch 4 batch 45]: : 1440it [00:04, 338.85it/s, mean train loss=1.44]
validate [batch 12]: : 384it [00:01, 289.53it/s, mean loss=1.4, macro f1=0.429]
train [epoch 5 batch 45]: : 1440it [00:04, 338.49it/s, mean train loss=1.33]
validate [batch 12]: : 384it [00:01, 283.95it/s, mean loss=1.36, macro f1=0.449]
train [epoch 6 batch 45]: : 1440it [00:04, 339.73it/s, mean train loss=1.3]
validate [batch 12]: : 384it [00:01, 290.92it/s, mean loss=1.29, macro f1=0.52]
[I 2024-10-19 23:55:09,032] Trial 15 pruned. </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 16 params: {'batch_size': 32, 'lr': 0.0003172086425995281, 'weight_decay': 0.0019517426716198715, 'reduce_lr_factor': 0.09943916989261473, 'momentum': 0.9433310742867136, 'early_stop_patience': 2, 'resize_length': 128, 'grayscale': False, 'jitter': False, 'horizontal_flip': False, 'rotation': False, 'mlp_n_layers': 5, 'mlp_layer_1_size': 256, 'mlp_layer_2_size': 512, 'mlp_layer_3_size': 512, 'mlp_activation_f': 'relu'}
Trial 16 fold 1</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 45]: : 1440it [00:04, 332.93it/s, mean train loss=1.13]
validate [batch 12]: : 384it [00:01, 290.09it/s, mean loss=0.872, macro f1=0.505]
train [epoch 2 batch 45]: : 1440it [00:04, 321.25it/s, mean train loss=0.686]
validate [batch 12]: : 384it [00:01, 287.15it/s, mean loss=0.682, macro f1=0.643]
train [epoch 3 batch 45]: : 1440it [00:04, 328.29it/s, mean train loss=0.489]
validate [batch 12]: : 384it [00:01, 288.87it/s, mean loss=0.619, macro f1=0.673]
train [epoch 4 batch 45]: : 1440it [00:04, 333.74it/s, mean train loss=0.398]
validate [batch 12]: : 384it [00:01, 288.06it/s, mean loss=0.624, macro f1=0.661]
train [epoch 5 batch 45]: : 1440it [00:04, 332.33it/s, mean train loss=0.325]
validate [batch 12]: : 384it [00:01, 284.98it/s, mean loss=0.541, macro f1=0.724]
train [epoch 6 batch 45]: : 1440it [00:04, 327.55it/s, mean train loss=0.256]
validate [batch 12]: : 384it [00:01, 289.78it/s, mean loss=0.56, macro f1=0.707]
train [epoch 7 batch 45]: : 1440it [00:04, 314.34it/s, mean train loss=0.22]
validate [batch 12]: : 384it [00:01, 285.15it/s, mean loss=0.517, macro f1=0.755]
train [epoch 8 batch 45]: : 1440it [00:04, 330.44it/s, mean train loss=0.173]
validate [batch 12]: : 384it [00:01, 288.93it/s, mean loss=0.477, macro f1=0.753]
train [epoch 9 batch 45]: : 1440it [00:04, 320.98it/s, mean train loss=0.156]
validate [batch 12]: : 384it [00:01, 256.67it/s, mean loss=0.471, macro f1=0.74]
train [epoch 10 batch 45]: : 1440it [00:05, 265.99it/s, mean train loss=0.132]
validate [batch 12]: : 384it [00:01, 233.04it/s, mean loss=0.521, macro f1=0.767]
train [epoch 11 batch 45]: : 1440it [00:05, 264.26it/s, mean train loss=0.11]
validate [batch 12]: : 384it [00:01, 243.51it/s, mean loss=0.611, macro f1=0.734]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Early stopping at epoch 11</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>test [batch 29]: : 928it [00:03, 305.43it/s, mean loss=0.645, macro f1=0.736]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 16 fold 2</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 45]: : 1440it [00:04, 333.69it/s, mean train loss=1.27]
validate [batch 12]: : 384it [00:01, 291.16it/s, mean loss=0.868, macro f1=0.586]
train [epoch 2 batch 45]: : 1440it [00:04, 330.52it/s, mean train loss=0.658]
validate [batch 12]: : 384it [00:01, 290.32it/s, mean loss=0.675, macro f1=0.672]
train [epoch 3 batch 45]: : 1440it [00:04, 333.49it/s, mean train loss=0.494]
validate [batch 12]: : 384it [00:01, 290.38it/s, mean loss=0.69, macro f1=0.665]
train [epoch 4 batch 45]: : 1440it [00:04, 331.04it/s, mean train loss=0.404]
validate [batch 12]: : 384it [00:01, 290.76it/s, mean loss=0.569, macro f1=0.742]
train [epoch 5 batch 45]: : 1440it [00:04, 334.50it/s, mean train loss=0.338]
validate [batch 12]: : 384it [00:01, 288.41it/s, mean loss=0.616, macro f1=0.706]
train [epoch 6 batch 45]: : 1440it [00:04, 315.71it/s, mean train loss=0.259]
validate [batch 12]: : 384it [00:01, 290.48it/s, mean loss=0.532, macro f1=0.755]
train [epoch 7 batch 45]: : 1440it [00:04, 329.93it/s, mean train loss=0.2]
validate [batch 12]: : 384it [00:01, 289.06it/s, mean loss=0.553, macro f1=0.774]
train [epoch 8 batch 45]: : 1440it [00:04, 334.14it/s, mean train loss=0.167]
validate [batch 12]: : 384it [00:01, 292.31it/s, mean loss=0.611, macro f1=0.747]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Early stopping at epoch 8</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>test [batch 29]: : 928it [00:02, 316.85it/s, mean loss=0.599, macro f1=0.741]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 16 fold 3</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 45]: : 1440it [00:04, 331.67it/s, mean train loss=1.19]
validate [batch 12]: : 384it [00:01, 291.93it/s, mean loss=0.881, macro f1=0.54]
train [epoch 2 batch 45]: : 1440it [00:04, 331.76it/s, mean train loss=0.633]
validate [batch 12]: : 384it [00:01, 258.35it/s, mean loss=0.713, macro f1=0.671]
train [epoch 3 batch 45]: : 1440it [00:04, 325.00it/s, mean train loss=0.533]
validate [batch 12]: : 384it [00:01, 290.50it/s, mean loss=0.783, macro f1=0.649]
train [epoch 4 batch 45]: : 1440it [00:04, 332.33it/s, mean train loss=0.399]
validate [batch 12]: : 384it [00:01, 290.40it/s, mean loss=0.62, macro f1=0.723]
train [epoch 5 batch 45]: : 1440it [00:04, 333.15it/s, mean train loss=0.319]
validate [batch 12]: : 384it [00:01, 287.15it/s, mean loss=0.634, macro f1=0.701]
train [epoch 6 batch 45]: : 1440it [00:04, 333.74it/s, mean train loss=0.243]
validate [batch 12]: : 384it [00:01, 288.14it/s, mean loss=0.605, macro f1=0.705]
train [epoch 7 batch 45]: : 1440it [00:04, 330.37it/s, mean train loss=0.21]
validate [batch 12]: : 384it [00:01, 289.68it/s, mean loss=0.711, macro f1=0.678]
train [epoch 8 batch 45]: : 1440it [00:04, 320.77it/s, mean train loss=0.183]
validate [batch 12]: : 384it [00:01, 290.24it/s, mean loss=0.584, macro f1=0.694]
train [epoch 9 batch 45]: : 1440it [00:04, 329.90it/s, mean train loss=0.146]
validate [batch 12]: : 384it [00:01, 286.31it/s, mean loss=0.555, macro f1=0.738]
train [epoch 10 batch 45]: : 1440it [00:04, 332.42it/s, mean train loss=0.137]
validate [batch 12]: : 384it [00:01, 291.24it/s, mean loss=0.547, macro f1=0.712]
train [epoch 11 batch 45]: : 1440it [00:04, 332.69it/s, mean train loss=0.108]
validate [batch 12]: : 384it [00:01, 278.47it/s, mean loss=0.602, macro f1=0.728]
train [epoch 12 batch 45]: : 1440it [00:05, 263.66it/s, mean train loss=0.104]
validate [batch 12]: : 384it [00:01, 227.60it/s, mean loss=0.541, macro f1=0.719]
train [epoch 13 batch 45]: : 1440it [00:05, 270.84it/s, mean train loss=0.086]
validate [batch 12]: : 384it [00:01, 276.83it/s, mean loss=0.609, macro f1=0.704]
train [epoch 14 batch 45]: : 1440it [00:04, 318.91it/s, mean train loss=0.0831]
validate [batch 12]: : 384it [00:01, 269.90it/s, mean loss=0.664, macro f1=0.693]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Early stopping at epoch 14</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>test [batch 29]: : 928it [00:02, 316.82it/s, mean loss=0.637, macro f1=0.707]
[I 2024-10-19 23:58:44,012] Trial 16 finished with value: 0.7278696750205723 and parameters: {'batch_size': 32, 'lr': 0.0003172086425995281, 'weight_decay': 0.0019517426716198715, 'reduce_lr_factor': 0.09943916989261473, 'momentum': 0.9433310742867136, 'early_stop_patience': 2, 'resize_length': 128, 'grayscale': False, 'jitter': False, 'horizontal_flip': False, 'rotation': False, 'mlp_n_layers': 5, 'mlp_layer_1_size': 256, 'mlp_layer_2_size': 512, 'mlp_layer_3_size': 512, 'mlp_activation_f': 'relu'}. Best is trial 11 with value: 0.7305797239149951.</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 17 params: {'batch_size': 24, 'lr': 0.00040797051594697656, 'weight_decay': 5.339270186246678e-06, 'reduce_lr_factor': 0.07854333142924635, 'momentum': 0.950538140281347, 'early_stop_patience': 1, 'resize_length': 128, 'grayscale': False, 'jitter': False, 'horizontal_flip': False, 'rotation': False, 'mlp_n_layers': 4, 'mlp_layer_1_size': 256, 'mlp_layer_2_size': 1024, 'mlp_activation_f': 'relu'}
Trial 17 fold 1</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 60]: : 1440it [00:04, 325.91it/s, mean train loss=1.03]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 265.74it/s, mean loss=0.726, macro f1=0.567]
train [epoch 2 batch 60]: : 1440it [00:04, 324.97it/s, mean train loss=0.589]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 267.38it/s, mean loss=0.613, macro f1=0.669]
train [epoch 3 batch 60]: : 1440it [00:04, 319.28it/s, mean train loss=0.43]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 251.53it/s, mean loss=0.545, macro f1=0.722]
train [epoch 4 batch 60]: : 1440it [00:04, 329.06it/s, mean train loss=0.334]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 268.27it/s, mean loss=0.599, macro f1=0.645]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Early stopping at epoch 4</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>test [batch 38]: : 912it [00:02, 313.15it/s, mean loss=0.627, macro f1=0.659]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 17 fold 2</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 60]: : 1440it [00:04, 325.60it/s, mean train loss=0.999]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 269.54it/s, mean loss=0.74, macro f1=0.601]
train [epoch 2 batch 60]: : 1440it [00:04, 324.99it/s, mean train loss=0.571]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 268.44it/s, mean loss=0.602, macro f1=0.61]
train [epoch 3 batch 60]: : 1440it [00:04, 328.56it/s, mean train loss=0.447]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 270.82it/s, mean loss=0.678, macro f1=0.632]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Early stopping at epoch 3</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>test [batch 38]: : 912it [00:03, 275.15it/s, mean loss=0.642, macro f1=0.671]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 17 fold 3</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 60]: : 1440it [00:04, 323.82it/s, mean train loss=1.01]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 267.66it/s, mean loss=0.758, macro f1=0.59]
train [epoch 2 batch 60]: : 1440it [00:04, 325.61it/s, mean train loss=0.557]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 267.23it/s, mean loss=0.587, macro f1=0.645]
train [epoch 3 batch 60]: : 1440it [00:04, 325.49it/s, mean train loss=0.449]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 261.71it/s, mean loss=0.607, macro f1=0.688]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Early stopping at epoch 3</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>test [batch 38]: : 912it [00:02, 311.67it/s, mean loss=0.614, macro f1=0.678]
[I 2024-10-19 23:59:56,071] Trial 17 finished with value: 0.6695770711293214 and parameters: {'batch_size': 24, 'lr': 0.00040797051594697656, 'weight_decay': 5.339270186246678e-06, 'reduce_lr_factor': 0.07854333142924635, 'momentum': 0.950538140281347, 'early_stop_patience': 1, 'resize_length': 128, 'grayscale': False, 'jitter': False, 'horizontal_flip': False, 'rotation': False, 'mlp_n_layers': 4, 'mlp_layer_1_size': 256, 'mlp_layer_2_size': 1024, 'mlp_activation_f': 'relu'}. Best is trial 11 with value: 0.7305797239149951.</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 18 params: {'batch_size': 32, 'lr': 0.0006094583766468982, 'weight_decay': 0.0017801296557513435, 'reduce_lr_factor': 0.09860499834085121, 'momentum': 0.9394135106187597, 'early_stop_patience': 1, 'resize_length': 160, 'grayscale': False, 'jitter': False, 'horizontal_flip': False, 'rotation': False, 'mlp_n_layers': 5, 'mlp_layer_1_size': 256, 'mlp_layer_2_size': 512, 'mlp_layer_3_size': 512, 'mlp_activation_f': 'relu'}
Trial 18 fold 1</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 45]: : 1440it [00:04, 327.04it/s, mean train loss=1.08]
validate [batch 12]: : 384it [00:01, 282.65it/s, mean loss=0.837, macro f1=0.512]
train [epoch 2 batch 45]: : 1440it [00:04, 315.54it/s, mean train loss=0.554]
validate [batch 12]: : 384it [00:01, 288.51it/s, mean loss=0.653, macro f1=0.667]
train [epoch 3 batch 45]: : 1440it [00:04, 325.98it/s, mean train loss=0.445]
validate [batch 12]: : 384it [00:01, 289.30it/s, mean loss=0.574, macro f1=0.724]
train [epoch 4 batch 45]: : 1440it [00:04, 328.13it/s, mean train loss=0.322]
validate [batch 12]: : 384it [00:01, 285.77it/s, mean loss=0.522, macro f1=0.749]
train [epoch 5 batch 45]: : 1440it [00:04, 325.02it/s, mean train loss=0.236]
validate [batch 12]: : 384it [00:01, 291.97it/s, mean loss=0.627, macro f1=0.668]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Early stopping at epoch 5</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>test [batch 29]: : 928it [00:02, 321.74it/s, mean loss=0.664, macro f1=0.704]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 18 fold 2</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 45]: : 1440it [00:04, 323.10it/s, mean train loss=1.07]
validate [batch 12]: : 384it [00:01, 269.82it/s, mean loss=0.782, macro f1=0.569]
train [epoch 2 batch 45]: : 1440it [00:05, 274.48it/s, mean train loss=0.577]
validate [batch 12]: : 384it [00:01, 239.93it/s, mean loss=0.707, macro f1=0.667]
train [epoch 3 batch 45]: : 1440it [00:05, 261.57it/s, mean train loss=0.499]
validate [batch 12]: : 384it [00:01, 231.58it/s, mean loss=0.585, macro f1=0.72]
train [epoch 4 batch 45]: : 1440it [00:05, 287.93it/s, mean train loss=0.354]
validate [batch 12]: : 384it [00:01, 261.13it/s, mean loss=0.581, macro f1=0.708]
train [epoch 5 batch 45]: : 1440it [00:04, 291.11it/s, mean train loss=0.315]
validate [batch 12]: : 384it [00:01, 291.49it/s, mean loss=0.578, macro f1=0.747]
train [epoch 6 batch 45]: : 1440it [00:04, 318.57it/s, mean train loss=0.227]
validate [batch 12]: : 384it [00:01, 280.47it/s, mean loss=0.516, macro f1=0.747]
train [epoch 7 batch 45]: : 1440it [00:04, 327.46it/s, mean train loss=0.165]
validate [batch 12]: : 384it [00:01, 288.64it/s, mean loss=0.566, macro f1=0.74]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Early stopping at epoch 7</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>test [batch 29]: : 928it [00:02, 320.63it/s, mean loss=0.531, macro f1=0.724]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 18 fold 3</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 45]: : 1440it [00:04, 327.74it/s, mean train loss=1.1]
validate [batch 12]: : 384it [00:01, 293.16it/s, mean loss=0.875, macro f1=0.512]
train [epoch 2 batch 45]: : 1440it [00:04, 329.63it/s, mean train loss=0.648]
validate [batch 12]: : 384it [00:01, 286.65it/s, mean loss=0.652, macro f1=0.654]
train [epoch 3 batch 45]: : 1440it [00:04, 329.26it/s, mean train loss=0.461]
validate [batch 12]: : 384it [00:01, 291.37it/s, mean loss=0.636, macro f1=0.746]
train [epoch 4 batch 45]: : 1440it [00:04, 314.83it/s, mean train loss=0.346]
validate [batch 12]: : 384it [00:01, 287.90it/s, mean loss=0.669, macro f1=0.683]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Early stopping at epoch 4</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>test [batch 29]: : 928it [00:02, 319.55it/s, mean loss=0.62, macro f1=0.676]
[I 2024-10-20 00:01:47,844] Trial 18 finished with value: 0.7014137777813739 and parameters: {'batch_size': 32, 'lr': 0.0006094583766468982, 'weight_decay': 0.0017801296557513435, 'reduce_lr_factor': 0.09860499834085121, 'momentum': 0.9394135106187597, 'early_stop_patience': 1, 'resize_length': 160, 'grayscale': False, 'jitter': False, 'horizontal_flip': False, 'rotation': False, 'mlp_n_layers': 5, 'mlp_layer_1_size': 256, 'mlp_layer_2_size': 512, 'mlp_layer_3_size': 512, 'mlp_activation_f': 'relu'}. Best is trial 11 with value: 0.7305797239149951.</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 19 params: {'batch_size': 32, 'lr': 0.0003689161140662125, 'weight_decay': 0.005486335654053626, 'reduce_lr_factor': 0.033106589596079095, 'momentum': 0.9853840928002924, 'early_stop_patience': 2, 'resize_length': 64, 'grayscale': False, 'jitter': False, 'horizontal_flip': False, 'rotation': False, 'mlp_n_layers': 5, 'mlp_layer_1_size': 256, 'mlp_layer_2_size': 512, 'mlp_layer_3_size': 256, 'mlp_activation_f': 'relu'}
Trial 19 fold 1</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 45]: : 1440it [00:04, 338.31it/s, mean train loss=1.26]
validate [batch 12]: : 384it [00:01, 292.19it/s, mean loss=0.919, macro f1=0.491]
train [epoch 2 batch 45]: : 1440it [00:04, 342.02it/s, mean train loss=0.67]
validate [batch 12]: : 384it [00:01, 290.04it/s, mean loss=0.68, macro f1=0.658]
train [epoch 3 batch 45]: : 1440it [00:04, 339.35it/s, mean train loss=0.545]
validate [batch 12]: : 384it [00:01, 292.30it/s, mean loss=0.586, macro f1=0.704]
train [epoch 4 batch 45]: : 1440it [00:04, 340.55it/s, mean train loss=0.433]
validate [batch 12]: : 384it [00:01, 288.48it/s, mean loss=0.539, macro f1=0.726]
train [epoch 5 batch 45]: : 1440it [00:04, 322.25it/s, mean train loss=0.332]
validate [batch 12]: : 384it [00:01, 287.78it/s, mean loss=0.53, macro f1=0.726]
train [epoch 6 batch 45]: : 1440it [00:04, 339.30it/s, mean train loss=0.302]
validate [batch 12]: : 384it [00:01, 292.36it/s, mean loss=0.549, macro f1=0.735]
train [epoch 7 batch 45]: : 1440it [00:04, 341.31it/s, mean train loss=0.25]
validate [batch 12]: : 384it [00:01, 287.84it/s, mean loss=0.504, macro f1=0.762]
train [epoch 8 batch 45]: : 1440it [00:04, 335.98it/s, mean train loss=0.199]
validate [batch 12]: : 384it [00:01, 290.16it/s, mean loss=0.469, macro f1=0.744]
train [epoch 9 batch 45]: : 1440it [00:04, 341.96it/s, mean train loss=0.182]
validate [batch 12]: : 384it [00:01, 291.60it/s, mean loss=0.503, macro f1=0.767]
train [epoch 10 batch 45]: : 1440it [00:04, 318.65it/s, mean train loss=0.181]
validate [batch 12]: : 384it [00:01, 290.40it/s, mean loss=0.588, macro f1=0.749]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Early stopping at epoch 10</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>test [batch 29]: : 928it [00:02, 321.55it/s, mean loss=0.723, macro f1=0.759]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 19 fold 2</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 45]: : 1440it [00:04, 337.54it/s, mean train loss=1.31]
validate [batch 12]: : 384it [00:01, 292.93it/s, mean loss=1.02, macro f1=0.516]
train [epoch 2 batch 45]: : 1440it [00:04, 310.29it/s, mean train loss=0.68]
validate [batch 12]: : 384it [00:01, 259.52it/s, mean loss=0.703, macro f1=0.641]
train [epoch 3 batch 45]: : 1440it [00:05, 262.56it/s, mean train loss=0.52]
validate [batch 12]: : 384it [00:01, 280.16it/s, mean loss=0.614, macro f1=0.66]
train [epoch 4 batch 45]: : 1440it [00:05, 279.30it/s, mean train loss=0.435]
validate [batch 12]: : 384it [00:01, 234.63it/s, mean loss=0.55, macro f1=0.71]
train [epoch 5 batch 45]: : 1440it [00:04, 320.61it/s, mean train loss=0.367]
validate [batch 12]: : 384it [00:01, 290.65it/s, mean loss=0.565, macro f1=0.76]
train [epoch 6 batch 45]: : 1440it [00:04, 340.14it/s, mean train loss=0.304]
validate [batch 12]: : 384it [00:01, 289.89it/s, mean loss=0.56, macro f1=0.748]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Early stopping at epoch 6</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>test [batch 29]: : 928it [00:02, 316.69it/s, mean loss=0.544, macro f1=0.728]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 19 fold 3</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 45]: : 1440it [00:04, 341.42it/s, mean train loss=1.25]
validate [batch 12]: : 384it [00:01, 291.45it/s, mean loss=0.995, macro f1=0.426]
train [epoch 2 batch 45]: : 1440it [00:04, 337.80it/s, mean train loss=0.742]
validate [batch 12]: : 384it [00:01, 289.41it/s, mean loss=0.712, macro f1=0.65]
train [epoch 3 batch 45]: : 1440it [00:04, 322.70it/s, mean train loss=0.513]
validate [batch 12]: : 384it [00:01, 285.94it/s, mean loss=0.627, macro f1=0.717]
train [epoch 4 batch 45]: : 1440it [00:04, 338.93it/s, mean train loss=0.407]
validate [batch 12]: : 384it [00:01, 290.89it/s, mean loss=0.505, macro f1=0.743]
train [epoch 5 batch 45]: : 1440it [00:04, 340.54it/s, mean train loss=0.364]
validate [batch 12]: : 384it [00:01, 280.68it/s, mean loss=0.581, macro f1=0.732]
train [epoch 6 batch 45]: : 1440it [00:04, 321.22it/s, mean train loss=0.276]
validate [batch 12]: : 384it [00:01, 290.06it/s, mean loss=0.562, macro f1=0.712]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Early stopping at epoch 6</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>test [batch 29]: : 928it [00:02, 321.30it/s, mean loss=0.564, macro f1=0.684]
[I 2024-10-20 00:04:11,983] Trial 19 finished with value: 0.7238061057192202 and parameters: {'batch_size': 32, 'lr': 0.0003689161140662125, 'weight_decay': 0.005486335654053626, 'reduce_lr_factor': 0.033106589596079095, 'momentum': 0.9853840928002924, 'early_stop_patience': 2, 'resize_length': 64, 'grayscale': False, 'jitter': False, 'horizontal_flip': False, 'rotation': False, 'mlp_n_layers': 5, 'mlp_layer_1_size': 256, 'mlp_layer_2_size': 512, 'mlp_layer_3_size': 256, 'mlp_activation_f': 'relu'}. Best is trial 11 with value: 0.7305797239149951.</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 20 params: {'batch_size': 24, 'lr': 2.465084092827328e-05, 'weight_decay': 0.0021603795369289007, 'reduce_lr_factor': 0.0611390638743071, 'momentum': 0.9295361802517457, 'early_stop_patience': 2, 'resize_length': 128, 'grayscale': False, 'jitter': False, 'horizontal_flip': False, 'rotation': False, 'mlp_n_layers': 4, 'mlp_layer_1_size': 256, 'mlp_layer_2_size': 512, 'mlp_activation_f': 'relu'}
Trial 20 fold 1</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 60]: : 1440it [00:04, 321.86it/s, mean train loss=1.6]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 265.36it/s, mean loss=1.19, macro f1=0.459]
train [epoch 2 batch 60]: : 1440it [00:04, 305.27it/s, mean train loss=1.07]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 267.29it/s, mean loss=1.01, macro f1=0.575]
train [epoch 3 batch 60]: : 1440it [00:04, 327.25it/s, mean train loss=0.919]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 267.31it/s, mean loss=0.91, macro f1=0.612]
train [epoch 4 batch 60]: : 1440it [00:04, 325.88it/s, mean train loss=0.789]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 266.95it/s, mean loss=0.857, macro f1=0.602]
train [epoch 5 batch 60]: : 1440it [00:04, 325.68it/s, mean train loss=0.753]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 261.91it/s, mean loss=0.792, macro f1=0.638]
train [epoch 6 batch 60]: : 1440it [00:04, 323.94it/s, mean train loss=0.668]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 264.40it/s, mean loss=0.767, macro f1=0.664]
[I 2024-10-20 00:04:49,570] Trial 20 pruned. </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 21 params: {'batch_size': 32, 'lr': 0.0003514663712832598, 'weight_decay': 0.005635507431427942, 'reduce_lr_factor': 0.029753600418150557, 'momentum': 0.9872940644377142, 'early_stop_patience': 2, 'resize_length': 64, 'grayscale': False, 'jitter': False, 'horizontal_flip': False, 'rotation': False, 'mlp_n_layers': 5, 'mlp_layer_1_size': 256, 'mlp_layer_2_size': 512, 'mlp_layer_3_size': 256, 'mlp_activation_f': 'relu'}
Trial 21 fold 1</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 45]: : 1440it [00:04, 304.34it/s, mean train loss=1.35]
validate [batch 12]: : 384it [00:01, 286.40it/s, mean loss=1.03, macro f1=0.421]
train [epoch 2 batch 45]: : 1440it [00:04, 336.89it/s, mean train loss=0.719]
validate [batch 12]: : 384it [00:01, 280.27it/s, mean loss=0.727, macro f1=0.582]
train [epoch 3 batch 45]: : 1440it [00:04, 337.93it/s, mean train loss=0.557]
validate [batch 12]: : 384it [00:01, 289.77it/s, mean loss=0.619, macro f1=0.659]
train [epoch 4 batch 45]: : 1440it [00:04, 336.96it/s, mean train loss=0.406]
validate [batch 12]: : 384it [00:01, 290.80it/s, mean loss=0.601, macro f1=0.68]
train [epoch 5 batch 45]: : 1440it [00:04, 338.75it/s, mean train loss=0.325]
validate [batch 12]: : 384it [00:01, 287.99it/s, mean loss=0.489, macro f1=0.716]
train [epoch 6 batch 45]: : 1440it [00:04, 290.40it/s, mean train loss=0.304]
validate [batch 12]: : 384it [00:01, 257.36it/s, mean loss=0.485, macro f1=0.754]
train [epoch 7 batch 45]: : 1440it [00:05, 283.92it/s, mean train loss=0.277]
validate [batch 12]: : 384it [00:01, 232.25it/s, mean loss=0.514, macro f1=0.737]
train [epoch 8 batch 45]: : 1440it [00:05, 266.56it/s, mean train loss=0.227]
validate [batch 12]: : 384it [00:01, 277.39it/s, mean loss=0.467, macro f1=0.759]
train [epoch 9 batch 45]: : 1440it [00:04, 328.69it/s, mean train loss=0.191]
validate [batch 12]: : 384it [00:01, 289.07it/s, mean loss=0.415, macro f1=0.772]
train [epoch 10 batch 45]: : 1440it [00:04, 336.91it/s, mean train loss=0.171]
validate [batch 12]: : 384it [00:01, 276.86it/s, mean loss=0.42, macro f1=0.779]
train [epoch 11 batch 45]: : 1440it [00:04, 326.34it/s, mean train loss=0.122]
validate [batch 12]: : 384it [00:01, 285.62it/s, mean loss=0.515, macro f1=0.787]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Early stopping at epoch 11</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>test [batch 29]: : 928it [00:02, 320.06it/s, mean loss=0.586, macro f1=0.772]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 21 fold 2</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 45]: : 1440it [00:04, 336.93it/s, mean train loss=1.35]
validate [batch 12]: : 384it [00:01, 283.44it/s, mean loss=1.01, macro f1=0.491]
train [epoch 2 batch 45]: : 1440it [00:04, 336.27it/s, mean train loss=0.821]
validate [batch 12]: : 384it [00:01, 290.17it/s, mean loss=0.669, macro f1=0.609]
train [epoch 3 batch 45]: : 1440it [00:04, 336.83it/s, mean train loss=0.539]
validate [batch 12]: : 384it [00:01, 292.29it/s, mean loss=0.646, macro f1=0.649]
train [epoch 4 batch 45]: : 1440it [00:04, 337.70it/s, mean train loss=0.479]
validate [batch 12]: : 384it [00:01, 290.91it/s, mean loss=0.539, macro f1=0.685]
train [epoch 5 batch 45]: : 1440it [00:04, 315.52it/s, mean train loss=0.432]
validate [batch 12]: : 384it [00:01, 284.92it/s, mean loss=0.567, macro f1=0.722]
train [epoch 6 batch 45]: : 1440it [00:04, 341.37it/s, mean train loss=0.349]
validate [batch 12]: : 384it [00:01, 291.38it/s, mean loss=0.477, macro f1=0.776]
train [epoch 7 batch 45]: : 1440it [00:04, 338.89it/s, mean train loss=0.263]
validate [batch 12]: : 384it [00:01, 290.67it/s, mean loss=0.532, macro f1=0.753]
train [epoch 8 batch 45]: : 1440it [00:04, 340.17it/s, mean train loss=0.206]
validate [batch 12]: : 384it [00:01, 291.87it/s, mean loss=0.549, macro f1=0.759]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Early stopping at epoch 8</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>test [batch 29]: : 928it [00:02, 315.86it/s, mean loss=0.536, macro f1=0.749]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 21 fold 3</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 45]: : 1440it [00:04, 339.85it/s, mean train loss=1.3]
validate [batch 12]: : 384it [00:01, 254.36it/s, mean loss=1.1, macro f1=0.336]
train [epoch 2 batch 45]: : 1440it [00:04, 328.76it/s, mean train loss=0.72]
validate [batch 12]: : 384it [00:01, 292.60it/s, mean loss=0.752, macro f1=0.572]
train [epoch 3 batch 45]: : 1440it [00:04, 341.06it/s, mean train loss=0.565]
validate [batch 12]: : 384it [00:01, 292.54it/s, mean loss=0.63, macro f1=0.716]
train [epoch 4 batch 45]: : 1440it [00:04, 336.56it/s, mean train loss=0.42]
validate [batch 12]: : 384it [00:01, 292.08it/s, mean loss=0.539, macro f1=0.705]
train [epoch 5 batch 45]: : 1440it [00:04, 338.37it/s, mean train loss=0.37]
validate [batch 12]: : 384it [00:01, 284.94it/s, mean loss=0.548, macro f1=0.712]
train [epoch 6 batch 45]: : 1440it [00:04, 340.20it/s, mean train loss=0.299]
validate [batch 12]: : 384it [00:01, 291.07it/s, mean loss=0.489, macro f1=0.77]
train [epoch 7 batch 45]: : 1440it [00:04, 322.16it/s, mean train loss=0.248]
validate [batch 12]: : 384it [00:01, 290.99it/s, mean loss=0.53, macro f1=0.704]
train [epoch 8 batch 45]: : 1440it [00:04, 342.25it/s, mean train loss=0.236]
validate [batch 12]: : 384it [00:01, 291.01it/s, mean loss=0.557, macro f1=0.747]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Early stopping at epoch 8</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>test [batch 29]: : 928it [00:03, 279.59it/s, mean loss=0.522, macro f1=0.746]
[I 2024-10-20 00:07:44,438] Trial 21 finished with value: 0.7554649753224488 and parameters: {'batch_size': 32, 'lr': 0.0003514663712832598, 'weight_decay': 0.005635507431427942, 'reduce_lr_factor': 0.029753600418150557, 'momentum': 0.9872940644377142, 'early_stop_patience': 2, 'resize_length': 64, 'grayscale': False, 'jitter': False, 'horizontal_flip': False, 'rotation': False, 'mlp_n_layers': 5, 'mlp_layer_1_size': 256, 'mlp_layer_2_size': 512, 'mlp_layer_3_size': 256, 'mlp_activation_f': 'relu'}. Best is trial 21 with value: 0.7554649753224488.</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 22 params: {'batch_size': 32, 'lr': 0.000202737946258828, 'weight_decay': 0.009833824537831395, 'reduce_lr_factor': 0.02851445389188707, 'momentum': 0.9865118683640303, 'early_stop_patience': 2, 'resize_length': 64, 'grayscale': False, 'jitter': False, 'horizontal_flip': False, 'rotation': False, 'mlp_n_layers': 5, 'mlp_layer_1_size': 256, 'mlp_layer_2_size': 512, 'mlp_layer_3_size': 256, 'mlp_activation_f': 'relu'}
Trial 22 fold 1</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 45]: : 1440it [00:04, 323.38it/s, mean train loss=1.44]
validate [batch 12]: : 384it [00:01, 237.98it/s, mean loss=1.03, macro f1=0.447]
train [epoch 2 batch 45]: : 1440it [00:05, 260.76it/s, mean train loss=0.818]
validate [batch 12]: : 384it [00:01, 234.38it/s, mean loss=0.751, macro f1=0.589]
train [epoch 3 batch 45]: : 1440it [00:05, 269.62it/s, mean train loss=0.59]
validate [batch 12]: : 384it [00:01, 290.13it/s, mean loss=0.644, macro f1=0.695]
train [epoch 4 batch 45]: : 1440it [00:04, 338.90it/s, mean train loss=0.48]
validate [batch 12]: : 384it [00:01, 289.29it/s, mean loss=0.556, macro f1=0.705]
train [epoch 5 batch 45]: : 1440it [00:04, 338.29it/s, mean train loss=0.423]
validate [batch 12]: : 384it [00:01, 287.31it/s, mean loss=0.544, macro f1=0.731]
train [epoch 6 batch 45]: : 1440it [00:04, 340.61it/s, mean train loss=0.372]
validate [batch 12]: : 384it [00:01, 284.78it/s, mean loss=0.519, macro f1=0.735]
train [epoch 7 batch 45]: : 1440it [00:04, 339.96it/s, mean train loss=0.277]
validate [batch 12]: : 384it [00:01, 288.48it/s, mean loss=0.506, macro f1=0.729]
train [epoch 8 batch 45]: : 1440it [00:04, 337.61it/s, mean train loss=0.243]
validate [batch 12]: : 384it [00:01, 266.14it/s, mean loss=0.539, macro f1=0.745]
train [epoch 9 batch 45]: : 1440it [00:04, 338.21it/s, mean train loss=0.215]
validate [batch 12]: : 384it [00:01, 288.87it/s, mean loss=0.43, macro f1=0.765]
train [epoch 10 batch 45]: : 1440it [00:04, 336.71it/s, mean train loss=0.184]
validate [batch 12]: : 384it [00:01, 289.53it/s, mean loss=0.456, macro f1=0.757]
train [epoch 11 batch 45]: : 1440it [00:04, 339.61it/s, mean train loss=0.158]
validate [batch 12]: : 384it [00:01, 283.95it/s, mean loss=0.441, macro f1=0.779]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Early stopping at epoch 11</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>test [batch 29]: : 928it [00:02, 319.66it/s, mean loss=0.581, macro f1=0.749]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 22 fold 2</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 45]: : 1440it [00:04, 341.32it/s, mean train loss=1.43]
validate [batch 12]: : 384it [00:01, 286.00it/s, mean loss=1.08, macro f1=0.447]
train [epoch 2 batch 45]: : 1440it [00:04, 319.26it/s, mean train loss=0.839]
validate [batch 12]: : 384it [00:01, 269.00it/s, mean loss=0.775, macro f1=0.621]
train [epoch 3 batch 45]: : 1440it [00:04, 333.94it/s, mean train loss=0.613]
validate [batch 12]: : 384it [00:01, 288.86it/s, mean loss=0.666, macro f1=0.657]
train [epoch 4 batch 45]: : 1440it [00:04, 336.47it/s, mean train loss=0.562]
validate [batch 12]: : 384it [00:01, 289.34it/s, mean loss=0.58, macro f1=0.73]
train [epoch 5 batch 45]: : 1440it [00:04, 336.25it/s, mean train loss=0.456]
validate [batch 12]: : 384it [00:01, 290.24it/s, mean loss=0.525, macro f1=0.714]
train [epoch 6 batch 45]: : 1440it [00:04, 340.02it/s, mean train loss=0.342]
validate [batch 12]: : 384it [00:01, 287.17it/s, mean loss=0.607, macro f1=0.741]
train [epoch 7 batch 45]: : 1440it [00:04, 336.92it/s, mean train loss=0.277]
validate [batch 12]: : 384it [00:01, 275.38it/s, mean loss=0.525, macro f1=0.776]
train [epoch 8 batch 45]: : 1440it [00:04, 319.44it/s, mean train loss=0.26]
validate [batch 12]: : 384it [00:01, 292.10it/s, mean loss=0.526, macro f1=0.749]
train [epoch 9 batch 45]: : 1440it [00:04, 338.48it/s, mean train loss=0.222]
validate [batch 12]: : 384it [00:01, 291.73it/s, mean loss=0.553, macro f1=0.757]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Early stopping at epoch 9</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>test [batch 29]: : 928it [00:02, 318.64it/s, mean loss=0.509, macro f1=0.744]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 22 fold 3</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 45]: : 1440it [00:04, 339.79it/s, mean train loss=1.41]
validate [batch 12]: : 384it [00:01, 287.29it/s, mean loss=1.02, macro f1=0.402]
train [epoch 2 batch 45]: : 1440it [00:04, 336.77it/s, mean train loss=0.777]
validate [batch 12]: : 384it [00:01, 228.00it/s, mean loss=0.778, macro f1=0.587]
train [epoch 3 batch 45]: : 1440it [00:05, 262.03it/s, mean train loss=0.602]
validate [batch 12]: : 384it [00:01, 245.85it/s, mean loss=0.68, macro f1=0.634]
train [epoch 4 batch 45]: : 1440it [00:04, 312.71it/s, mean train loss=0.5]
validate [batch 12]: : 384it [00:01, 235.10it/s, mean loss=0.585, macro f1=0.681]
train [epoch 5 batch 45]: : 1440it [00:05, 283.48it/s, mean train loss=0.412]
validate [batch 12]: : 384it [00:01, 268.14it/s, mean loss=0.544, macro f1=0.707]
train [epoch 6 batch 45]: : 1440it [00:04, 340.32it/s, mean train loss=0.334]
validate [batch 12]: : 384it [00:01, 286.41it/s, mean loss=0.53, macro f1=0.746]
train [epoch 7 batch 45]: : 1440it [00:04, 338.34it/s, mean train loss=0.294]
validate [batch 12]: : 384it [00:01, 288.59it/s, mean loss=0.521, macro f1=0.716]
train [epoch 8 batch 45]: : 1440it [00:04, 321.23it/s, mean train loss=0.228]
validate [batch 12]: : 384it [00:01, 289.19it/s, mean loss=0.538, macro f1=0.721]
train [epoch 9 batch 45]: : 1440it [00:04, 337.19it/s, mean train loss=0.219]
validate [batch 12]: : 384it [00:01, 289.31it/s, mean loss=0.489, macro f1=0.759]
train [epoch 10 batch 45]: : 1440it [00:04, 335.63it/s, mean train loss=0.179]
validate [batch 12]: : 384it [00:01, 289.85it/s, mean loss=0.559, macro f1=0.742]
train [epoch 11 batch 45]: : 1440it [00:04, 338.04it/s, mean train loss=0.159]
validate [batch 12]: : 384it [00:01, 290.46it/s, mean loss=0.609, macro f1=0.772]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Early stopping at epoch 11</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>test [batch 29]: : 928it [00:02, 313.30it/s, mean loss=0.503, macro f1=0.749]
[I 2024-10-20 00:11:05,377] Trial 22 finished with value: 0.7474029635781916 and parameters: {'batch_size': 32, 'lr': 0.000202737946258828, 'weight_decay': 0.009833824537831395, 'reduce_lr_factor': 0.02851445389188707, 'momentum': 0.9865118683640303, 'early_stop_patience': 2, 'resize_length': 64, 'grayscale': False, 'jitter': False, 'horizontal_flip': False, 'rotation': False, 'mlp_n_layers': 5, 'mlp_layer_1_size': 256, 'mlp_layer_2_size': 512, 'mlp_layer_3_size': 256, 'mlp_activation_f': 'relu'}. Best is trial 21 with value: 0.7554649753224488.</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 23 params: {'batch_size': 32, 'lr': 0.00017571495299283302, 'weight_decay': 0.009726305764949416, 'reduce_lr_factor': 0.02894806164879947, 'momentum': 0.9737434356784762, 'early_stop_patience': 2, 'resize_length': 64, 'grayscale': False, 'jitter': False, 'horizontal_flip': False, 'rotation': False, 'mlp_n_layers': 5, 'mlp_layer_1_size': 256, 'mlp_layer_2_size': 512, 'mlp_layer_3_size': 256, 'mlp_activation_f': 'relu'}
Trial 23 fold 1</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 45]: : 1440it [00:04, 338.24it/s, mean train loss=1.42]
validate [batch 12]: : 384it [00:01, 290.04it/s, mean loss=1.02, macro f1=0.478]
train [epoch 2 batch 45]: : 1440it [00:04, 322.06it/s, mean train loss=0.859]
validate [batch 12]: : 384it [00:01, 283.55it/s, mean loss=0.775, macro f1=0.582]
train [epoch 3 batch 45]: : 1440it [00:04, 339.55it/s, mean train loss=0.652]
validate [batch 12]: : 384it [00:01, 289.28it/s, mean loss=0.682, macro f1=0.667]
train [epoch 4 batch 45]: : 1440it [00:04, 340.80it/s, mean train loss=0.497]
validate [batch 12]: : 384it [00:01, 289.77it/s, mean loss=0.637, macro f1=0.686]
train [epoch 5 batch 45]: : 1440it [00:04, 339.12it/s, mean train loss=0.425]
validate [batch 12]: : 384it [00:01, 290.76it/s, mean loss=0.584, macro f1=0.724]
train [epoch 6 batch 45]: : 1440it [00:04, 339.53it/s, mean train loss=0.345]
validate [batch 12]: : 384it [00:01, 291.77it/s, mean loss=0.559, macro f1=0.74]
train [epoch 7 batch 45]: : 1440it [00:04, 329.29it/s, mean train loss=0.298]
validate [batch 12]: : 384it [00:01, 280.47it/s, mean loss=0.522, macro f1=0.716]
train [epoch 8 batch 45]: : 1440it [00:04, 339.76it/s, mean train loss=0.241]
validate [batch 12]: : 384it [00:01, 286.05it/s, mean loss=0.524, macro f1=0.764]
train [epoch 9 batch 45]: : 1440it [00:04, 339.84it/s, mean train loss=0.233]
validate [batch 12]: : 384it [00:01, 290.45it/s, mean loss=0.497, macro f1=0.743]
train [epoch 10 batch 45]: : 1440it [00:04, 336.80it/s, mean train loss=0.218]
validate [batch 12]: : 384it [00:01, 290.46it/s, mean loss=0.444, macro f1=0.805]
train [epoch 11 batch 45]: : 1440it [00:04, 339.22it/s, mean train loss=0.2]
validate [batch 12]: : 384it [00:01, 289.99it/s, mean loss=0.528, macro f1=0.773]
train [epoch 12 batch 45]: : 1440it [00:04, 336.77it/s, mean train loss=0.15]
validate [batch 12]: : 384it [00:01, 290.61it/s, mean loss=0.508, macro f1=0.767]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Early stopping at epoch 12</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>test [batch 29]: : 928it [00:03, 279.97it/s, mean loss=0.57, macro f1=0.758]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 23 fold 2</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 45]: : 1440it [00:04, 335.34it/s, mean train loss=1.51]
validate [batch 12]: : 384it [00:01, 254.09it/s, mean loss=1.09, macro f1=0.437]
train [epoch 2 batch 45]: : 1440it [00:04, 298.29it/s, mean train loss=0.864]
validate [batch 12]: : 384it [00:01, 238.32it/s, mean loss=0.809, macro f1=0.571]
train [epoch 3 batch 45]: : 1440it [00:05, 266.27it/s, mean train loss=0.679]
validate [batch 12]: : 384it [00:01, 233.46it/s, mean loss=0.694, macro f1=0.661]
train [epoch 4 batch 45]: : 1440it [00:04, 297.27it/s, mean train loss=0.552]
validate [batch 12]: : 384it [00:01, 281.14it/s, mean loss=0.62, macro f1=0.65]
train [epoch 5 batch 45]: : 1440it [00:04, 306.57it/s, mean train loss=0.482]
validate [batch 12]: : 384it [00:01, 291.46it/s, mean loss=0.622, macro f1=0.668]
train [epoch 6 batch 45]: : 1440it [00:04, 338.67it/s, mean train loss=0.379]
validate [batch 12]: : 384it [00:01, 292.26it/s, mean loss=0.632, macro f1=0.745]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Early stopping at epoch 6</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>test [batch 29]: : 928it [00:02, 316.50it/s, mean loss=0.581, macro f1=0.722]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 23 fold 3</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 45]: : 1440it [00:04, 339.08it/s, mean train loss=1.49]
validate [batch 12]: : 384it [00:01, 291.83it/s, mean loss=1.11, macro f1=0.437]
train [epoch 2 batch 45]: : 1440it [00:04, 338.02it/s, mean train loss=0.811]
validate [batch 12]: : 384it [00:01, 290.40it/s, mean loss=0.764, macro f1=0.566]
train [epoch 3 batch 45]: : 1440it [00:04, 337.19it/s, mean train loss=0.632]
validate [batch 12]: : 384it [00:01, 291.03it/s, mean loss=0.682, macro f1=0.635]
train [epoch 4 batch 45]: : 1440it [00:04, 302.42it/s, mean train loss=0.545]
validate [batch 12]: : 384it [00:01, 283.37it/s, mean loss=0.674, macro f1=0.715]
train [epoch 5 batch 45]: : 1440it [00:04, 337.40it/s, mean train loss=0.428]
validate [batch 12]: : 384it [00:01, 277.56it/s, mean loss=0.648, macro f1=0.673]
train [epoch 6 batch 45]: : 1440it [00:04, 315.50it/s, mean train loss=0.362]
validate [batch 12]: : 384it [00:01, 275.64it/s, mean loss=0.567, macro f1=0.733]
train [epoch 7 batch 45]: : 1440it [00:04, 330.34it/s, mean train loss=0.328]
validate [batch 12]: : 384it [00:01, 274.43it/s, mean loss=0.625, macro f1=0.705]
train [epoch 8 batch 45]: : 1440it [00:04, 323.70it/s, mean train loss=0.281]
validate [batch 12]: : 384it [00:01, 268.20it/s, mean loss=0.517, macro f1=0.747]
train [epoch 9 batch 45]: : 1440it [00:04, 303.44it/s, mean train loss=0.241]
validate [batch 12]: : 384it [00:01, 276.61it/s, mean loss=0.522, macro f1=0.744]
train [epoch 10 batch 45]: : 1440it [00:04, 326.51it/s, mean train loss=0.189]
validate [batch 12]: : 384it [00:01, 277.63it/s, mean loss=0.482, macro f1=0.769]
train [epoch 11 batch 45]: : 1440it [00:04, 330.01it/s, mean train loss=0.205]
validate [batch 12]: : 384it [00:01, 280.44it/s, mean loss=0.561, macro f1=0.715]
train [epoch 12 batch 45]: : 1440it [00:04, 329.34it/s, mean train loss=0.152]
validate [batch 12]: : 384it [00:01, 274.07it/s, mean loss=0.508, macro f1=0.754]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Early stopping at epoch 12</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>test [batch 29]: : 928it [00:03, 308.06it/s, mean loss=0.524, macro f1=0.742]
[I 2024-10-20 00:14:19,623] Trial 23 finished with value: 0.7407869084118612 and parameters: {'batch_size': 32, 'lr': 0.00017571495299283302, 'weight_decay': 0.009726305764949416, 'reduce_lr_factor': 0.02894806164879947, 'momentum': 0.9737434356784762, 'early_stop_patience': 2, 'resize_length': 64, 'grayscale': False, 'jitter': False, 'horizontal_flip': False, 'rotation': False, 'mlp_n_layers': 5, 'mlp_layer_1_size': 256, 'mlp_layer_2_size': 512, 'mlp_layer_3_size': 256, 'mlp_activation_f': 'relu'}. Best is trial 21 with value: 0.7554649753224488.</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 24 params: {'batch_size': 32, 'lr': 0.00011760046913718452, 'weight_decay': 0.0095020853707793, 'reduce_lr_factor': 0.027691349431280918, 'momentum': 0.9891129573812659, 'early_stop_patience': 2, 'resize_length': 32, 'grayscale': False, 'jitter': False, 'horizontal_flip': False, 'rotation': False, 'mlp_n_layers': 5, 'mlp_layer_1_size': 256, 'mlp_layer_2_size': 1024, 'mlp_layer_3_size': 256, 'mlp_activation_f': 'relu'}
Trial 24 fold 1</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 45]: : 1440it [00:04, 321.38it/s, mean train loss=1.7]
validate [batch 12]: : 384it [00:01, 272.59it/s, mean loss=1.25, macro f1=0.409]
train [epoch 2 batch 45]: : 1440it [00:04, 329.51it/s, mean train loss=0.913]
validate [batch 12]: : 384it [00:01, 274.09it/s, mean loss=0.843, macro f1=0.58]
train [epoch 3 batch 45]: : 1440it [00:04, 332.47it/s, mean train loss=0.718]
validate [batch 12]: : 384it [00:01, 275.59it/s, mean loss=0.711, macro f1=0.613]
train [epoch 4 batch 45]: : 1440it [00:04, 329.29it/s, mean train loss=0.622]
validate [batch 12]: : 384it [00:01, 280.73it/s, mean loss=0.686, macro f1=0.648]
train [epoch 5 batch 45]: : 1440it [00:04, 335.76it/s, mean train loss=0.51]
validate [batch 12]: : 384it [00:01, 270.25it/s, mean loss=0.594, macro f1=0.675]
train [epoch 6 batch 45]: : 1440it [00:04, 314.30it/s, mean train loss=0.433]
validate [batch 12]: : 384it [00:01, 222.25it/s, mean loss=0.56, macro f1=0.717]
train [epoch 7 batch 45]: : 1440it [00:05, 262.02it/s, mean train loss=0.392]
validate [batch 12]: : 384it [00:01, 263.97it/s, mean loss=0.542, macro f1=0.718]
train [epoch 8 batch 45]: : 1440it [00:04, 308.88it/s, mean train loss=0.359]
validate [batch 12]: : 384it [00:01, 269.02it/s, mean loss=0.533, macro f1=0.724]
[I 2024-10-20 00:15:11,184] Trial 24 pruned. </code></pre>
</div>
</div>
</div>
<div id="0948f321" class="cell">
<div class="sourceCode cell-code" id="cb272"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb272-1"><a href="#cb272-1" aria-hidden="true" tabindex="-1"></a>mlp_study.best_params</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-container">
<div class="cell-output cell-output-display" data-execution_count="44">
<pre><code>{'batch_size': 32,
 'lr': 0.0003514663712832598,
 'weight_decay': 0.005635507431427942,
 'reduce_lr_factor': 0.029753600418150557,
 'momentum': 0.9872940644377142,
 'early_stop_patience': 2,
 'resize_length': 64,
 'grayscale': False,
 'jitter': False,
 'horizontal_flip': False,
 'rotation': False,
 'mlp_n_layers': 5,
 'mlp_layer_1_size': 256,
 'mlp_layer_2_size': 512,
 'mlp_layer_3_size': 256,
 'mlp_activation_f': 'relu'}</code></pre>
</div>
</div>
</div>
<p>Interestingly, the hyperparameter tuning results indicate that the MLP performed best without performing any data augmentations. However, images had to be resized to 64 by 64. The size of each layer also seems rather small.</p>
</section>
<section id="vgg-16-1" class="level3">
<h3 class="anchored" data-anchor-id="vgg-16-1">VGG-16</h3>
<p>For the custom VGG16 model, there is only one model-specific hyperparameter to tune since the model’s architecture is mostly fixed. The hyperparameter is whether to use batch normalisation. As mentioned earlier, I found that the model struggled to learn (loss would not decrease) without applying batch normalisation after each convolutional layer. The reason could be because the VGG-16 architecture is deep, so it is prone to the vanishing / exploding gradient problem. Batch normalisation may have helped to stabilise the gradients.</p>
<div id="66668f18" class="cell">
<div class="sourceCode cell-code" id="cb274"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb274-1"><a href="#cb274-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> suggest_vgg16_params(trial: optuna.trial.Trial):</span>
<span id="cb274-2"><a href="#cb274-2" aria-hidden="true" tabindex="-1"></a>    trial.suggest_categorical(<span class="st">"vgg16_batch_norm"</span>, [<span class="va">True</span>, <span class="va">False</span>])</span>
<span id="cb274-3"><a href="#cb274-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb274-4"><a href="#cb274-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> vgg16_from_params(params: <span class="bu">dict</span>) <span class="op">-&gt;</span> VGG16:</span>
<span id="cb274-5"><a href="#cb274-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> VGG16(<span class="dv">10</span>, batch_norm<span class="op">=</span>params[<span class="st">"vgg16_batch_norm"</span>])</span>
<span id="cb274-6"><a href="#cb274-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb274-7"><a href="#cb274-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb274-8"><a href="#cb274-8" aria-hidden="true" tabindex="-1"></a>vgg16_study <span class="op">=</span> optuna.create_study(</span>
<span id="cb274-9"><a href="#cb274-9" aria-hidden="true" tabindex="-1"></a>    study_name<span class="op">=</span><span class="st">"VGG16 study"</span>,</span>
<span id="cb274-10"><a href="#cb274-10" aria-hidden="true" tabindex="-1"></a>    sampler<span class="op">=</span>optuna.samplers.TPESampler(seed<span class="op">=</span><span class="dv">0</span>),</span>
<span id="cb274-11"><a href="#cb274-11" aria-hidden="true" tabindex="-1"></a>    pruner<span class="op">=</span>optuna.pruners.MedianPruner(n_startup_trials<span class="op">=</span><span class="dv">3</span>, n_warmup_steps<span class="op">=</span><span class="dv">5</span>),</span>
<span id="cb274-12"><a href="#cb274-12" aria-hidden="true" tabindex="-1"></a>    directions<span class="op">=</span>[<span class="st">"maximize"</span>],</span>
<span id="cb274-13"><a href="#cb274-13" aria-hidden="true" tabindex="-1"></a>    storage<span class="op">=</span>optuna.storages.JournalStorage(</span>
<span id="cb274-14"><a href="#cb274-14" aria-hidden="true" tabindex="-1"></a>        optuna.storages.journal.JournalFileBackend(<span class="st">"vgg16_journal.log"</span>),</span>
<span id="cb274-15"><a href="#cb274-15" aria-hidden="true" tabindex="-1"></a>    ),</span>
<span id="cb274-16"><a href="#cb274-16" aria-hidden="true" tabindex="-1"></a>    load_if_exists<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb274-17"><a href="#cb274-17" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb274-18"><a href="#cb274-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb274-19"><a href="#cb274-19" aria-hidden="true" tabindex="-1"></a>vgg16_study.optimize(</span>
<span id="cb274-20"><a href="#cb274-20" aria-hidden="true" tabindex="-1"></a>    make_objective(</span>
<span id="cb274-21"><a href="#cb274-21" aria-hidden="true" tabindex="-1"></a>        suggest_vgg16_params,</span>
<span id="cb274-22"><a href="#cb274-22" aria-hidden="true" tabindex="-1"></a>        vgg16_from_params,</span>
<span id="cb274-23"><a href="#cb274-23" aria-hidden="true" tabindex="-1"></a>        ds_train,</span>
<span id="cb274-24"><a href="#cb274-24" aria-hidden="true" tabindex="-1"></a>    ),</span>
<span id="cb274-25"><a href="#cb274-25" aria-hidden="true" tabindex="-1"></a>    n_trials<span class="op">=</span>N_TRIALS,</span>
<span id="cb274-26"><a href="#cb274-26" aria-hidden="true" tabindex="-1"></a>    gc_after_trial<span class="op">=</span><span class="va">True</span></span>
<span id="cb274-27"><a href="#cb274-27" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-container">
<div class="cell-output cell-output-stderr">
<pre><code>[I 2024-10-19 19:11:00,993] A new study created in Journal with name: VGG16 study</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 0 params: {'batch_size': 32, 'lr': 0.0007180374727086954, 'weight_decay': 0.006027633760716439, 'reduce_lr_factor': 0.05903948646972072, 'momentum': 0.8804944118743919, 'early_stop_patience': 2, 'resize_length': 128, 'grayscale': False, 'jitter': False, 'horizontal_flip': False, 'rotation': True, 'vgg16_batch_norm': True}
Trial 0 fold 1</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 45]: : 1440it [00:14, 100.87it/s, mean train loss=1.67]
validate [batch 12]: : 384it [00:02, 170.65it/s, mean loss=1.89, macro f1=0.0796]
train [epoch 2 batch 45]: : 1440it [00:12, 116.36it/s, mean train loss=1.14]
validate [batch 12]: : 384it [00:01, 198.92it/s, mean loss=0.905, macro f1=0.4]
train [epoch 3 batch 45]: : 1440it [00:12, 117.19it/s, mean train loss=0.754]
validate [batch 12]: : 384it [00:01, 199.23it/s, mean loss=0.579, macro f1=0.696]
train [epoch 4 batch 45]: : 1440it [00:12, 116.30it/s, mean train loss=0.511]
validate [batch 12]: : 384it [00:01, 196.39it/s, mean loss=0.438, macro f1=0.775]
train [epoch 5 batch 45]: : 1440it [00:12, 116.73it/s, mean train loss=0.432]
validate [batch 12]: : 384it [00:01, 201.75it/s, mean loss=0.449, macro f1=0.763]
train [epoch 6 batch 45]: : 1440it [00:12, 115.05it/s, mean train loss=0.389]
validate [batch 12]: : 384it [00:01, 199.72it/s, mean loss=0.426, macro f1=0.803]
train [epoch 7 batch 45]: : 1440it [00:12, 115.94it/s, mean train loss=0.337]
validate [batch 12]: : 384it [00:01, 201.13it/s, mean loss=0.485, macro f1=0.781]
train [epoch 8 batch 45]: : 1440it [00:12, 114.43it/s, mean train loss=0.29]
validate [batch 12]: : 384it [00:01, 199.99it/s, mean loss=0.592, macro f1=0.748]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Early stopping at epoch 8</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>test [batch 29]: : 928it [00:04, 213.72it/s, mean loss=0.498, macro f1=0.78]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 0 fold 2</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 45]: : 1440it [00:12, 114.99it/s, mean train loss=1.65]
validate [batch 12]: : 384it [00:01, 193.17it/s, mean loss=1.63, macro f1=0.124]
train [epoch 2 batch 45]: : 1440it [00:12, 112.77it/s, mean train loss=1.17]
validate [batch 12]: : 384it [00:01, 197.98it/s, mean loss=0.934, macro f1=0.442]
train [epoch 3 batch 45]: : 1440it [00:12, 113.72it/s, mean train loss=0.766]
validate [batch 12]: : 384it [00:01, 192.40it/s, mean loss=0.517, macro f1=0.685]
train [epoch 4 batch 45]: : 1440it [00:12, 112.18it/s, mean train loss=0.534]
validate [batch 12]: : 384it [00:01, 199.63it/s, mean loss=0.499, macro f1=0.73]
train [epoch 5 batch 45]: : 1440it [00:12, 112.55it/s, mean train loss=0.485]
validate [batch 12]: : 384it [00:01, 198.74it/s, mean loss=0.591, macro f1=0.784]
train [epoch 6 batch 45]: : 1440it [00:13, 109.01it/s, mean train loss=0.386]
validate [batch 12]: : 384it [00:01, 198.99it/s, mean loss=0.43, macro f1=0.811]
train [epoch 7 batch 45]: : 1440it [00:12, 112.68it/s, mean train loss=0.31]
validate [batch 12]: : 384it [00:01, 194.11it/s, mean loss=0.68, macro f1=0.785]
train [epoch 8 batch 45]: : 1440it [00:12, 111.20it/s, mean train loss=0.319]
validate [batch 12]: : 384it [00:01, 200.87it/s, mean loss=0.386, macro f1=0.833]
train [epoch 9 batch 45]: : 1440it [00:12, 113.16it/s, mean train loss=0.257]
validate [batch 12]: : 384it [00:01, 195.65it/s, mean loss=0.471, macro f1=0.857]
train [epoch 10 batch 45]: : 1440it [00:12, 111.59it/s, mean train loss=0.261]
validate [batch 12]: : 384it [00:02, 189.58it/s, mean loss=0.459, macro f1=0.831]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Early stopping at epoch 10</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>test [batch 29]: : 928it [00:04, 211.92it/s, mean loss=0.438, macro f1=0.815]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 0 fold 3</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 45]: : 1440it [00:12, 112.76it/s, mean train loss=1.62]
validate [batch 12]: : 384it [00:01, 195.99it/s, mean loss=1.72, macro f1=0.0926]
train [epoch 2 batch 45]: : 1440it [00:12, 111.21it/s, mean train loss=1.13]
validate [batch 12]: : 384it [00:01, 192.86it/s, mean loss=0.829, macro f1=0.463]
train [epoch 3 batch 45]: : 1440it [00:12, 112.39it/s, mean train loss=0.72]
validate [batch 12]: : 384it [00:01, 194.04it/s, mean loss=0.556, macro f1=0.627]
train [epoch 4 batch 45]: : 1440it [00:12, 111.76it/s, mean train loss=0.533]
validate [batch 12]: : 384it [00:02, 191.32it/s, mean loss=0.522, macro f1=0.709]
train [epoch 5 batch 45]: : 1440it [00:12, 112.57it/s, mean train loss=0.476]
validate [batch 12]: : 384it [00:01, 193.47it/s, mean loss=0.452, macro f1=0.743]
train [epoch 6 batch 45]: : 1440it [00:12, 111.67it/s, mean train loss=0.362]
validate [batch 12]: : 384it [00:01, 193.37it/s, mean loss=0.44, macro f1=0.809]
train [epoch 7 batch 45]: : 1440it [00:12, 112.56it/s, mean train loss=0.338]
validate [batch 12]: : 384it [00:01, 194.23it/s, mean loss=0.347, macro f1=0.819]
train [epoch 8 batch 45]: : 1440it [00:12, 111.76it/s, mean train loss=0.314]
validate [batch 12]: : 384it [00:01, 193.10it/s, mean loss=0.536, macro f1=0.794]
train [epoch 9 batch 45]: : 1440it [00:12, 112.79it/s, mean train loss=0.263]
validate [batch 12]: : 384it [00:01, 197.93it/s, mean loss=0.965, macro f1=0.628]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Early stopping at epoch 9</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>test [batch 29]: : 928it [00:04, 211.59it/s, mean loss=1.01, macro f1=0.662]
[I 2024-10-19 19:18:03,075] Trial 0 finished with value: 0.7523225350223695 and parameters: {'batch_size': 32, 'lr': 0.0007180374727086954, 'weight_decay': 0.006027633760716439, 'reduce_lr_factor': 0.05903948646972072, 'momentum': 0.8804944118743919, 'early_stop_patience': 2, 'resize_length': 128, 'grayscale': False, 'jitter': False, 'horizontal_flip': False, 'rotation': True, 'vgg16_batch_norm': True}. Best is trial 0 with value: 0.7523225350223695.</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 1 params: {'batch_size': 32, 'lr': 0.000780375183440352, 'weight_decay': 0.008700121482468192, 'reduce_lr_factor': 0.09807565080094877, 'momentum': 0.9518401272011775, 'early_stop_patience': 2, 'resize_length': 192, 'grayscale': False, 'jitter': False, 'horizontal_flip': True, 'rotation': False, 'vgg16_batch_norm': False}
Trial 1 fold 1</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 45]: : 1440it [00:19, 73.40it/s, mean train loss=2.28]
validate [batch 12]: : 384it [00:02, 143.77it/s, mean loss=2.25, macro f1=0.0581]
train [epoch 2 batch 45]: : 1440it [00:19, 73.40it/s, mean train loss=2.22]
validate [batch 12]: : 384it [00:02, 144.38it/s, mean loss=2.18, macro f1=0.0581]
train [epoch 3 batch 45]: : 1440it [00:19, 73.52it/s, mean train loss=2.16]
validate [batch 12]: : 384it [00:02, 145.34it/s, mean loss=2.12, macro f1=0.0581]
train [epoch 4 batch 45]: : 1440it [00:19, 75.23it/s, mean train loss=2.1]
validate [batch 12]: : 384it [00:02, 142.81it/s, mean loss=2.06, macro f1=0.0581]
train [epoch 5 batch 45]: : 1440it [00:19, 75.19it/s, mean train loss=2.06]
validate [batch 12]: : 384it [00:02, 144.68it/s, mean loss=2.02, macro f1=0.0581]
[I 2024-10-19 19:19:55,855] Trial 1 pruned. Mean validation loss is still above or equal to 1.5 at the 5th epoch.</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 2 params: {'batch_size': 24, 'lr': 0.0006214591421051183, 'weight_decay': 0.006120957227224214, 'reduce_lr_factor': 0.06552405971872813, 'momentum': 0.9793121349177786, 'early_stop_patience': 3, 'resize_length': 96, 'grayscale': False, 'jitter': False, 'horizontal_flip': True, 'rotation': False, 'vgg16_batch_norm': False}
Trial 2 fold 1</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 60]: : 1440it [00:09, 144.38it/s, mean train loss=2.27]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 199.36it/s, mean loss=2.21, macro f1=0.0581]
train [epoch 2 batch 60]: : 1440it [00:10, 142.60it/s, mean train loss=2.16]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 196.57it/s, mean loss=2.08, macro f1=0.0581]
train [epoch 3 batch 60]: : 1440it [00:09, 144.39it/s, mean train loss=2.03]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 207.55it/s, mean loss=1.98, macro f1=0.0581]
train [epoch 4 batch 60]: : 1440it [00:10, 143.66it/s, mean train loss=1.94]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 201.52it/s, mean loss=1.93, macro f1=0.0581]
train [epoch 5 batch 60]: : 1440it [00:10, 141.98it/s, mean train loss=1.93]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 210.07it/s, mean loss=1.92, macro f1=0.0581]
[I 2024-10-19 19:20:57,225] Trial 2 pruned. Mean validation loss is still above or equal to 1.5 at the 5th epoch.</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 3 params: {'batch_size': 24, 'lr': 0.000988490099678634, 'weight_decay': 0.0010204481074802807, 'reduce_lr_factor': 0.028798908048535125, 'momentum': 0.8306488083981494, 'early_stop_patience': 2, 'resize_length': 64, 'grayscale': True, 'jitter': True, 'horizontal_flip': True, 'rotation': False, 'vgg16_batch_norm': True}
Trial 3 fold 1</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 60]: : 1440it [00:09, 146.73it/s, mean train loss=1.49]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 203.88it/s, mean loss=1.06, macro f1=0.348]
train [epoch 2 batch 60]: : 1440it [00:10, 143.80it/s, mean train loss=0.836]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 201.12it/s, mean loss=0.727, macro f1=0.466]
train [epoch 3 batch 60]: : 1440it [00:09, 147.44it/s, mean train loss=0.599]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 237.76it/s, mean loss=0.604, macro f1=0.615]
train [epoch 4 batch 60]: : 1440it [00:08, 170.02it/s, mean train loss=0.505]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 234.33it/s, mean loss=0.581, macro f1=0.644]
train [epoch 5 batch 60]: : 1440it [00:08, 167.54it/s, mean train loss=0.47]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 213.05it/s, mean loss=0.534, macro f1=0.738]
train [epoch 6 batch 60]: : 1440it [00:09, 148.33it/s, mean train loss=0.359]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 218.60it/s, mean loss=0.396, macro f1=0.807]
train [epoch 7 batch 60]: : 1440it [00:09, 151.78it/s, mean train loss=0.306]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 215.62it/s, mean loss=0.63, macro f1=0.681]
train [epoch 8 batch 60]: : 1440it [00:09, 155.29it/s, mean train loss=0.301]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 226.55it/s, mean loss=0.5, macro f1=0.807]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Early stopping at epoch 8</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>test [batch 38]: : 912it [00:03, 241.26it/s, mean loss=0.492, macro f1=0.788]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 3 fold 2</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 60]: : 1440it [00:09, 159.14it/s, mean train loss=1.48]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 203.72it/s, mean loss=1.04, macro f1=0.283]
train [epoch 2 batch 60]: : 1440it [00:08, 160.07it/s, mean train loss=0.965]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 231.34it/s, mean loss=0.75, macro f1=0.524]
train [epoch 3 batch 60]: : 1440it [00:09, 158.21it/s, mean train loss=0.664]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 206.40it/s, mean loss=0.647, macro f1=0.608]
train [epoch 4 batch 60]: : 1440it [00:09, 156.54it/s, mean train loss=0.55]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 229.36it/s, mean loss=0.463, macro f1=0.661]
train [epoch 5 batch 60]: : 1440it [00:09, 159.86it/s, mean train loss=0.45]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 225.62it/s, mean loss=0.966, macro f1=0.569]
train [epoch 6 batch 60]: : 1440it [00:09, 154.53it/s, mean train loss=0.354]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 221.09it/s, mean loss=0.361, macro f1=0.818]
train [epoch 7 batch 60]: : 1440it [00:09, 158.33it/s, mean train loss=0.279]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 226.56it/s, mean loss=0.435, macro f1=0.787]
train [epoch 8 batch 60]: : 1440it [00:08, 160.70it/s, mean train loss=0.259]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 205.83it/s, mean loss=0.343, macro f1=0.786]
train [epoch 9 batch 60]: : 1440it [00:09, 150.06it/s, mean train loss=0.206]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 225.38it/s, mean loss=0.771, macro f1=0.687]
train [epoch 10 batch 60]: : 1440it [00:09, 155.78it/s, mean train loss=0.21]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 219.03it/s, mean loss=0.31, macro f1=0.845]
train [epoch 11 batch 60]: : 1440it [00:09, 144.30it/s, mean train loss=0.154]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 200.78it/s, mean loss=0.641, macro f1=0.769]
train [epoch 12 batch 60]: : 1440it [00:09, 154.75it/s, mean train loss=0.179]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 228.11it/s, mean loss=0.505, macro f1=0.81]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Early stopping at epoch 12</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>test [batch 38]: : 912it [00:04, 223.73it/s, mean loss=0.562, macro f1=0.791]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 3 fold 3</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 60]: : 1440it [00:09, 146.65it/s, mean train loss=1.45]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 184.34it/s, mean loss=1.11, macro f1=0.267]
train [epoch 2 batch 60]: : 1440it [00:09, 147.96it/s, mean train loss=0.903]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 236.92it/s, mean loss=0.671, macro f1=0.554]
train [epoch 3 batch 60]: : 1440it [00:08, 162.29it/s, mean train loss=0.65]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 254.00it/s, mean loss=0.592, macro f1=0.582]
train [epoch 4 batch 60]: : 1440it [00:08, 160.81it/s, mean train loss=0.523]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 211.14it/s, mean loss=0.563, macro f1=0.672]
train [epoch 5 batch 60]: : 1440it [00:09, 156.72it/s, mean train loss=0.412]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 221.52it/s, mean loss=0.74, macro f1=0.645]
train [epoch 6 batch 60]: : 1440it [00:09, 147.93it/s, mean train loss=0.384]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 237.38it/s, mean loss=0.469, macro f1=0.74]
train [epoch 7 batch 60]: : 1440it [00:09, 144.91it/s, mean train loss=0.351]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 200.00it/s, mean loss=0.625, macro f1=0.666]
train [epoch 8 batch 60]: : 1440it [00:09, 145.31it/s, mean train loss=0.302]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 202.47it/s, mean loss=0.414, macro f1=0.767]
train [epoch 9 batch 60]: : 1440it [00:09, 146.82it/s, mean train loss=0.241]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 241.98it/s, mean loss=0.523, macro f1=0.807]
train [epoch 10 batch 60]: : 1440it [00:08, 165.72it/s, mean train loss=0.195]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 236.47it/s, mean loss=0.337, macro f1=0.849]
train [epoch 11 batch 60]: : 1440it [00:09, 154.30it/s, mean train loss=0.146]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 243.67it/s, mean loss=0.398, macro f1=0.841]
train [epoch 12 batch 60]: : 1440it [00:08, 162.94it/s, mean train loss=0.196]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 236.83it/s, mean loss=0.497, macro f1=0.806]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Early stopping at epoch 12</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>test [batch 38]: : 912it [00:04, 213.17it/s, mean loss=0.482, macro f1=0.816]
[I 2024-10-19 19:27:12,999] Trial 3 finished with value: 0.7984415820297991 and parameters: {'batch_size': 24, 'lr': 0.000988490099678634, 'weight_decay': 0.0010204481074802807, 'reduce_lr_factor': 0.028798908048535125, 'momentum': 0.8306488083981494, 'early_stop_patience': 2, 'resize_length': 64, 'grayscale': True, 'jitter': True, 'horizontal_flip': True, 'rotation': False, 'vgg16_batch_norm': True}. Best is trial 3 with value: 0.7984415820297991.</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 4 params: {'batch_size': 32, 'lr': 0.00010513742381502343, 'weight_decay': 0.009764594650133959, 'reduce_lr_factor': 0.05217860814829315, 'momentum': 0.985584606756164, 'early_stop_patience': 2, 'resize_length': 192, 'grayscale': False, 'jitter': False, 'horizontal_flip': False, 'rotation': True, 'vgg16_batch_norm': True}
Trial 4 fold 1</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 45]: : 1440it [00:21, 66.17it/s, mean train loss=1.99]
validate [batch 12]: : 384it [00:02, 142.38it/s, mean loss=1.89, macro f1=0.0624]
train [epoch 2 batch 45]: : 1440it [00:21, 65.77it/s, mean train loss=1.5]
validate [batch 12]: : 384it [00:02, 142.00it/s, mean loss=1.26, macro f1=0.227]
train [epoch 3 batch 45]: : 1440it [00:21, 66.62it/s, mean train loss=1.21]
validate [batch 12]: : 384it [00:02, 142.28it/s, mean loss=0.99, macro f1=0.392]
train [epoch 4 batch 45]: : 1440it [00:21, 66.09it/s, mean train loss=0.926]
validate [batch 12]: : 384it [00:02, 142.99it/s, mean loss=0.735, macro f1=0.469]
train [epoch 5 batch 45]: : 1440it [00:21, 66.28it/s, mean train loss=0.703]
validate [batch 12]: : 384it [00:02, 140.95it/s, mean loss=0.597, macro f1=0.697]
train [epoch 6 batch 45]: : 1440it [00:21, 66.35it/s, mean train loss=0.525]
validate [batch 12]: : 384it [00:02, 143.14it/s, mean loss=0.517, macro f1=0.744]
train [epoch 7 batch 45]: : 1440it [00:21, 66.30it/s, mean train loss=0.503]
validate [batch 12]: : 384it [00:02, 144.00it/s, mean loss=0.477, macro f1=0.72]
train [epoch 8 batch 45]: : 1440it [00:21, 66.19it/s, mean train loss=0.445]
validate [batch 12]: : 384it [00:02, 142.65it/s, mean loss=0.491, macro f1=0.71]
train [epoch 9 batch 45]: : 1440it [00:21, 66.44it/s, mean train loss=0.369]
validate [batch 12]: : 384it [00:02, 143.21it/s, mean loss=0.447, macro f1=0.749]
train [epoch 10 batch 45]: : 1440it [00:21, 66.60it/s, mean train loss=0.365]
validate [batch 12]: : 384it [00:02, 143.87it/s, mean loss=0.563, macro f1=0.721]
train [epoch 11 batch 45]: : 1440it [00:21, 66.41it/s, mean train loss=0.356]
validate [batch 12]: : 384it [00:02, 142.29it/s, mean loss=0.477, macro f1=0.734]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Early stopping at epoch 11</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>test [batch 29]: : 928it [00:06, 144.26it/s, mean loss=0.475, macro f1=0.737]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 4 fold 2</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 45]: : 1440it [00:22, 65.30it/s, mean train loss=1.98]
validate [batch 12]: : 384it [00:02, 139.44it/s, mean loss=1.87, macro f1=0.0509]
train [epoch 2 batch 45]: : 1440it [00:22, 65.18it/s, mean train loss=1.55]
validate [batch 12]: : 384it [00:02, 128.87it/s, mean loss=1.26, macro f1=0.182]
train [epoch 3 batch 45]: : 1440it [00:22, 65.41it/s, mean train loss=1.19]
validate [batch 12]: : 384it [00:02, 138.24it/s, mean loss=1.08, macro f1=0.277]
train [epoch 4 batch 45]: : 1440it [00:22, 64.80it/s, mean train loss=1.03]
validate [batch 12]: : 384it [00:02, 136.42it/s, mean loss=0.805, macro f1=0.575]
train [epoch 5 batch 45]: : 1440it [00:21, 65.92it/s, mean train loss=0.768]
validate [batch 12]: : 384it [00:02, 143.70it/s, mean loss=0.667, macro f1=0.611]
train [epoch 6 batch 45]: : 1440it [00:21, 66.43it/s, mean train loss=0.591]
validate [batch 12]: : 384it [00:02, 139.04it/s, mean loss=0.573, macro f1=0.653]
train [epoch 7 batch 45]: : 1440it [00:21, 65.66it/s, mean train loss=0.536]
validate [batch 12]: : 384it [00:02, 133.96it/s, mean loss=0.5, macro f1=0.753]
train [epoch 8 batch 45]: : 1440it [00:22, 65.03it/s, mean train loss=0.441]
validate [batch 12]: : 384it [00:02, 132.92it/s, mean loss=0.439, macro f1=0.733]
train [epoch 9 batch 45]: : 1440it [00:22, 64.81it/s, mean train loss=0.391]
validate [batch 12]: : 384it [00:02, 134.53it/s, mean loss=0.509, macro f1=0.776]
train [epoch 10 batch 45]: : 1440it [00:21, 65.91it/s, mean train loss=0.365]
validate [batch 12]: : 384it [00:02, 142.00it/s, mean loss=0.443, macro f1=0.771]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Early stopping at epoch 10</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>test [batch 29]: : 928it [00:06, 142.47it/s, mean loss=0.44, macro f1=0.739]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 4 fold 3</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 45]: : 1440it [00:21, 65.69it/s, mean train loss=1.96]
validate [batch 12]: : 384it [00:02, 134.33it/s, mean loss=1.87, macro f1=0.151]
train [epoch 2 batch 45]: : 1440it [00:22, 65.19it/s, mean train loss=1.51]
validate [batch 12]: : 384it [00:02, 135.61it/s, mean loss=1.27, macro f1=0.186]
train [epoch 3 batch 45]: : 1440it [00:21, 65.57it/s, mean train loss=1.12]
validate [batch 12]: : 384it [00:02, 135.01it/s, mean loss=1.05, macro f1=0.29]
train [epoch 4 batch 45]: : 1440it [00:22, 65.37it/s, mean train loss=0.955]
validate [batch 12]: : 384it [00:02, 141.50it/s, mean loss=0.81, macro f1=0.491]
train [epoch 5 batch 45]: : 1440it [00:21, 66.59it/s, mean train loss=0.72]
validate [batch 12]: : 384it [00:02, 140.94it/s, mean loss=0.631, macro f1=0.602]
train [epoch 6 batch 45]: : 1440it [00:21, 65.70it/s, mean train loss=0.618]
validate [batch 12]: : 384it [00:02, 140.75it/s, mean loss=0.528, macro f1=0.67]
train [epoch 7 batch 45]: : 1440it [00:22, 65.08it/s, mean train loss=0.512]
validate [batch 12]: : 384it [00:02, 137.98it/s, mean loss=0.526, macro f1=0.626]
train [epoch 8 batch 45]: : 1440it [00:21, 66.01it/s, mean train loss=0.449]
validate [batch 12]: : 384it [00:02, 141.96it/s, mean loss=0.488, macro f1=0.65]
train [epoch 9 batch 45]: : 1440it [00:21, 65.88it/s, mean train loss=0.424]
validate [batch 12]: : 384it [00:02, 133.52it/s, mean loss=0.437, macro f1=0.769]
train [epoch 10 batch 45]: : 1440it [00:22, 65.30it/s, mean train loss=0.402]
validate [batch 12]: : 384it [00:02, 135.72it/s, mean loss=0.456, macro f1=0.726]
train [epoch 11 batch 45]: : 1440it [00:22, 65.41it/s, mean train loss=0.347]
validate [batch 12]: : 384it [00:02, 141.40it/s, mean loss=0.505, macro f1=0.681]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Early stopping at epoch 11</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>test [batch 29]: : 928it [00:06, 145.51it/s, mean loss=0.513, macro f1=0.722]
[I 2024-10-19 19:40:53,521] Trial 4 finished with value: 0.7328167133703358 and parameters: {'batch_size': 32, 'lr': 0.00010513742381502343, 'weight_decay': 0.009764594650133959, 'reduce_lr_factor': 0.05217860814829315, 'momentum': 0.985584606756164, 'early_stop_patience': 2, 'resize_length': 192, 'grayscale': False, 'jitter': False, 'horizontal_flip': False, 'rotation': True, 'vgg16_batch_norm': True}. Best is trial 3 with value: 0.7984415820297991.</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 5 params: {'batch_size': 24, 'lr': 0.0005280155729320327, 'weight_decay': 0.0009394051075844168, 'reduce_lr_factor': 0.061835184600056145, 'momentum': 0.9765662775394807, 'early_stop_patience': 1, 'resize_length': 160, 'grayscale': False, 'jitter': True, 'horizontal_flip': True, 'rotation': True, 'vgg16_batch_norm': True}
Trial 5 fold 1</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 60]: : 1440it [00:19, 72.31it/s, mean train loss=1.73]
validate [batch 15]: 100%|██████████| 360/360 [00:02&lt;00:00, 146.71it/s, mean loss=1.13, macro f1=0.342]
train [epoch 2 batch 60]: : 1440it [00:19, 72.67it/s, mean train loss=1.05]
validate [batch 15]: 100%|██████████| 360/360 [00:02&lt;00:00, 150.47it/s, mean loss=1.59, macro f1=0.477]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Early stopping at epoch 2</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>test [batch 38]: : 912it [00:05, 154.55it/s, mean loss=1.55, macro f1=0.475]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 5 fold 2</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 60]: : 1440it [00:19, 72.23it/s, mean train loss=1.69]
validate [batch 15]: 100%|██████████| 360/360 [00:02&lt;00:00, 147.72it/s, mean loss=1.42, macro f1=0.23]
train [epoch 2 batch 60]: : 1440it [00:20, 71.90it/s, mean train loss=1.08]
validate [batch 15]: 100%|██████████| 360/360 [00:02&lt;00:00, 139.10it/s, mean loss=0.786, macro f1=0.572]
train [epoch 3 batch 60]: : 1440it [00:19, 72.12it/s, mean train loss=0.64]
validate [batch 15]: 100%|██████████| 360/360 [00:02&lt;00:00, 149.50it/s, mean loss=0.565, macro f1=0.692]
train [epoch 4 batch 60]: : 1440it [00:19, 72.48it/s, mean train loss=0.531]
validate [batch 15]: 100%|██████████| 360/360 [00:02&lt;00:00, 151.28it/s, mean loss=0.597, macro f1=0.669]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Early stopping at epoch 4</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>test [batch 38]: : 912it [00:05, 152.39it/s, mean loss=0.564, macro f1=0.715]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 5 fold 3</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 60]: : 1440it [00:19, 73.51it/s, mean train loss=1.68]
validate [batch 15]: 100%|██████████| 360/360 [00:02&lt;00:00, 149.51it/s, mean loss=1.19, macro f1=0.268]
train [epoch 2 batch 60]: : 1440it [00:19, 73.40it/s, mean train loss=0.96]
validate [batch 15]: 100%|██████████| 360/360 [00:02&lt;00:00, 149.25it/s, mean loss=0.729, macro f1=0.593]
train [epoch 3 batch 60]: : 1440it [00:19, 73.45it/s, mean train loss=0.669]
validate [batch 15]: 100%|██████████| 360/360 [00:02&lt;00:00, 150.29it/s, mean loss=0.672, macro f1=0.629]
train [epoch 4 batch 60]: : 1440it [00:19, 73.64it/s, mean train loss=0.527]
validate [batch 15]: 100%|██████████| 360/360 [00:02&lt;00:00, 142.75it/s, mean loss=0.527, macro f1=0.702]
train [epoch 5 batch 60]: : 1440it [00:19, 73.12it/s, mean train loss=0.527]
validate [batch 15]: 100%|██████████| 360/360 [00:02&lt;00:00, 149.07it/s, mean loss=0.495, macro f1=0.671]
train [epoch 6 batch 60]: : 1440it [00:19, 73.57it/s, mean train loss=0.542]
validate [batch 15]: 100%|██████████| 360/360 [00:02&lt;00:00, 151.03it/s, mean loss=0.627, macro f1=0.708]
[I 2024-10-19 19:45:37,597] Trial 5 pruned. </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 6 params: {'batch_size': 32, 'lr': 0.0009625666596662639, 'weight_decay': 0.0024875314351995802, 'reduce_lr_factor': 0.06185416009760533, 'momentum': 0.9124879669416495, 'early_stop_patience': 2, 'resize_length': 64, 'grayscale': True, 'jitter': True, 'horizontal_flip': False, 'rotation': False, 'vgg16_batch_norm': False}
Trial 6 fold 1</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 45]: : 1440it [00:08, 160.41it/s, mean train loss=2.29]
validate [batch 12]: : 384it [00:01, 223.18it/s, mean loss=2.26, macro f1=0.0581]
train [epoch 2 batch 45]: : 1440it [00:08, 178.33it/s, mean train loss=2.23]
validate [batch 12]: : 384it [00:01, 245.00it/s, mean loss=2.2, macro f1=0.0581]
train [epoch 3 batch 45]: : 1440it [00:08, 173.17it/s, mean train loss=2.19]
validate [batch 12]: : 384it [00:01, 237.41it/s, mean loss=2.16, macro f1=0.0581]
train [epoch 4 batch 45]: : 1440it [00:08, 173.86it/s, mean train loss=2.14]
validate [batch 12]: : 384it [00:01, 234.44it/s, mean loss=2.11, macro f1=0.0581]
train [epoch 5 batch 45]: : 1440it [00:08, 167.28it/s, mean train loss=2.12]
validate [batch 12]: : 384it [00:01, 235.14it/s, mean loss=2.08, macro f1=0.0581]
[I 2024-10-19 19:46:30,353] Trial 6 pruned. Mean validation loss is still above or equal to 1.5 at the 5th epoch.</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 7 params: {'batch_size': 32, 'lr': 0.0007280017370214441, 'weight_decay': 0.005013243819267023, 'reduce_lr_factor': 0.09604752712509015, 'momentum': 0.9223581378536311, 'early_stop_patience': 2, 'resize_length': 160, 'grayscale': False, 'jitter': True, 'horizontal_flip': True, 'rotation': False, 'vgg16_batch_norm': False}
Trial 7 fold 1</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 45]: : 1440it [00:16, 87.35it/s, mean train loss=2.28]
validate [batch 12]: : 384it [00:02, 166.54it/s, mean loss=2.26, macro f1=0.0581]
train [epoch 2 batch 45]: : 1440it [00:16, 88.73it/s, mean train loss=2.24]
validate [batch 12]: : 384it [00:02, 163.50it/s, mean loss=2.22, macro f1=0.0581]
train [epoch 3 batch 45]: : 1440it [00:16, 89.29it/s, mean train loss=2.19]
validate [batch 12]: : 384it [00:02, 163.31it/s, mean loss=2.17, macro f1=0.0581]
train [epoch 4 batch 45]: : 1440it [00:16, 87.08it/s, mean train loss=2.16]
validate [batch 12]: : 384it [00:02, 166.39it/s, mean loss=2.14, macro f1=0.0581]
train [epoch 5 batch 45]: : 1440it [00:16, 87.84it/s, mean train loss=2.14]
validate [batch 12]: : 384it [00:02, 160.36it/s, mean loss=2.1, macro f1=0.0581]
[I 2024-10-19 19:48:06,141] Trial 7 pruned. Mean validation loss is still above or equal to 1.5 at the 5th epoch.</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 8 params: {'batch_size': 32, 'lr': 0.0006566688116585623, 'weight_decay': 0.006521032700016889, 'reduce_lr_factor': 0.04882765918905766, 'momentum': 0.9703438532117019, 'early_stop_patience': 2, 'resize_length': 128, 'grayscale': True, 'jitter': True, 'horizontal_flip': True, 'rotation': True, 'vgg16_batch_norm': True}
Trial 8 fold 1</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 45]: : 1440it [00:14, 101.02it/s, mean train loss=1.69]
validate [batch 12]: : 384it [00:01, 196.74it/s, mean loss=1.48, macro f1=0.188]
train [epoch 2 batch 45]: : 1440it [00:14, 102.26it/s, mean train loss=1.18]
validate [batch 12]: : 384it [00:02, 188.35it/s, mean loss=1.04, macro f1=0.334]
train [epoch 3 batch 45]: : 1440it [00:14, 99.35it/s, mean train loss=0.88] 
validate [batch 12]: : 384it [00:02, 183.23it/s, mean loss=0.798, macro f1=0.463]
train [epoch 4 batch 45]: : 1440it [00:14, 100.90it/s, mean train loss=0.778]
validate [batch 12]: : 384it [00:02, 181.89it/s, mean loss=0.91, macro f1=0.401]
train [epoch 5 batch 45]: : 1440it [00:14, 102.74it/s, mean train loss=0.723]
validate [batch 12]: : 384it [00:02, 189.26it/s, mean loss=0.826, macro f1=0.565]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Early stopping at epoch 5</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>test [batch 29]: : 928it [00:04, 203.14it/s, mean loss=0.892, macro f1=0.546]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 8 fold 2</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 45]: : 1440it [00:14, 99.63it/s, mean train loss=1.7]
validate [batch 12]: : 384it [00:02, 187.85it/s, mean loss=1.31, macro f1=0.22]
train [epoch 2 batch 45]: : 1440it [00:14, 102.25it/s, mean train loss=1.26]
validate [batch 12]: : 384it [00:02, 191.05it/s, mean loss=0.945, macro f1=0.399]
train [epoch 3 batch 45]: : 1440it [00:13, 105.75it/s, mean train loss=0.911]
validate [batch 12]: : 384it [00:01, 196.32it/s, mean loss=0.812, macro f1=0.434]
[I 2024-10-19 19:50:24,152] Trial 8 pruned. </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 9 params: {'batch_size': 32, 'lr': 0.0001325817830209471, 'weight_decay': 0.008480082293222344, 'reduce_lr_factor': 0.08265870628525096, 'momentum': 0.9081291403367727, 'early_stop_patience': 2, 'resize_length': 32, 'grayscale': True, 'jitter': False, 'horizontal_flip': True, 'rotation': False, 'vgg16_batch_norm': True}
Trial 9 fold 1</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 45]: : 1440it [00:06, 208.49it/s, mean train loss=1.9]
validate [batch 12]: : 384it [00:01, 247.77it/s, mean loss=1.77, macro f1=0.0744]
train [epoch 2 batch 45]: : 1440it [00:06, 217.25it/s, mean train loss=1.39]
validate [batch 12]: : 384it [00:01, 248.65it/s, mean loss=1.26, macro f1=0.191]
train [epoch 3 batch 45]: : 1440it [00:06, 216.26it/s, mean train loss=1.15]
validate [batch 12]: : 384it [00:01, 240.61it/s, mean loss=1.04, macro f1=0.487]
train [epoch 4 batch 45]: : 1440it [00:07, 182.90it/s, mean train loss=0.944]
validate [batch 12]: : 384it [00:01, 227.63it/s, mean loss=0.886, macro f1=0.434]
train [epoch 5 batch 45]: : 1440it [00:07, 205.13it/s, mean train loss=0.803]
validate [batch 12]: : 384it [00:01, 247.98it/s, mean loss=0.698, macro f1=0.562]
train [epoch 6 batch 45]: : 1440it [00:06, 217.90it/s, mean train loss=0.625]
validate [batch 12]: : 384it [00:01, 241.69it/s, mean loss=0.628, macro f1=0.539]
[I 2024-10-19 19:51:18,034] Trial 9 pruned. </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 10 params: {'batch_size': 24, 'lr': 0.0003311487468197181, 'weight_decay': 0.00020768834766933357, 'reduce_lr_factor': 0.015244949070858688, 'momentum': 0.8213427298202189, 'early_stop_patience': 3, 'resize_length': 32, 'grayscale': True, 'jitter': True, 'horizontal_flip': False, 'rotation': False, 'vgg16_batch_norm': True}
Trial 10 fold 1</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 60]: : 1440it [00:07, 182.11it/s, mean train loss=1.68]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 204.31it/s, mean loss=1.3, macro f1=0.192]
train [epoch 2 batch 60]: : 1440it [00:08, 177.95it/s, mean train loss=1.15]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 241.21it/s, mean loss=0.968, macro f1=0.388]
train [epoch 3 batch 60]: : 1440it [00:07, 184.46it/s, mean train loss=0.835]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 250.22it/s, mean loss=0.699, macro f1=0.558]
train [epoch 4 batch 60]: : 1440it [00:07, 184.33it/s, mean train loss=0.592]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 253.27it/s, mean loss=0.615, macro f1=0.522]
train [epoch 5 batch 60]: : 1440it [00:08, 178.58it/s, mean train loss=0.437]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 270.69it/s, mean loss=0.679, macro f1=0.632]
train [epoch 6 batch 60]: : 1440it [00:07, 202.59it/s, mean train loss=0.392]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 239.06it/s, mean loss=0.565, macro f1=0.665]
[I 2024-10-19 19:52:16,445] Trial 10 pruned. </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 11 params: {'batch_size': 24, 'lr': 0.0009785389379804816, 'weight_decay': 0.003223015461764898, 'reduce_lr_factor': 0.03006433458512642, 'momentum': 0.8470825958578355, 'early_stop_patience': 1, 'resize_length': 96, 'grayscale': True, 'jitter': False, 'horizontal_flip': False, 'rotation': True, 'vgg16_batch_norm': True}
Trial 11 fold 1</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 60]: : 1440it [00:10, 134.19it/s, mean train loss=1.59]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 202.72it/s, mean loss=1.23, macro f1=0.301]
train [epoch 2 batch 60]: : 1440it [00:10, 134.06it/s, mean train loss=1.16]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 206.54it/s, mean loss=0.807, macro f1=0.472]
train [epoch 3 batch 60]: : 1440it [00:10, 133.92it/s, mean train loss=0.763]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 211.59it/s, mean loss=0.627, macro f1=0.559]
train [epoch 4 batch 60]: : 1440it [00:10, 132.98it/s, mean train loss=0.628]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 196.06it/s, mean loss=0.614, macro f1=0.685]
train [epoch 5 batch 60]: : 1440it [00:10, 139.06it/s, mean train loss=0.564]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 217.30it/s, mean loss=0.53, macro f1=0.698]
train [epoch 6 batch 60]: : 1440it [00:10, 139.93it/s, mean train loss=0.446]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 215.87it/s, mean loss=0.473, macro f1=0.765]
[I 2024-10-19 19:53:33,229] Trial 11 pruned. </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 12 params: {'batch_size': 24, 'lr': 0.0008410583816770148, 'weight_decay': 0.00337328767144783, 'reduce_lr_factor': 0.035082533355625806, 'momentum': 0.8617079018609524, 'early_stop_patience': 1, 'resize_length': 96, 'grayscale': False, 'jitter': False, 'horizontal_flip': False, 'rotation': True, 'vgg16_batch_norm': True}
Trial 12 fold 1</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 60]: : 1440it [00:10, 137.29it/s, mean train loss=1.66]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 209.48it/s, mean loss=1.24, macro f1=0.286]
train [epoch 2 batch 60]: : 1440it [00:10, 138.87it/s, mean train loss=1.04]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 214.08it/s, mean loss=0.734, macro f1=0.656]
train [epoch 3 batch 60]: : 1440it [00:10, 138.31it/s, mean train loss=0.663]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 200.52it/s, mean loss=0.68, macro f1=0.599]
train [epoch 4 batch 60]: : 1440it [00:10, 136.52it/s, mean train loss=0.414]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 200.82it/s, mean loss=0.525, macro f1=0.762]
train [epoch 5 batch 60]: : 1440it [00:10, 137.73it/s, mean train loss=0.44]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 210.41it/s, mean loss=0.468, macro f1=0.757]
train [epoch 6 batch 60]: : 1440it [00:10, 135.44it/s, mean train loss=0.382]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 201.08it/s, mean loss=0.496, macro f1=0.784]
[I 2024-10-19 19:54:49,350] Trial 12 pruned. </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 13 params: {'batch_size': 24, 'lr': 0.0008576302353547708, 'weight_decay': 0.006391681065068446, 'reduce_lr_factor': 0.011662590535982367, 'momentum': 0.8025557342473153, 'early_stop_patience': 3, 'resize_length': 224, 'grayscale': True, 'jitter': True, 'horizontal_flip': False, 'rotation': True, 'vgg16_batch_norm': True}
Trial 13 fold 1</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 60]: : 1440it [00:31, 46.19it/s, mean train loss=1.6]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 106.73it/s, mean loss=1.27, macro f1=0.267]
train [epoch 2 batch 60]: : 1440it [00:31, 46.32it/s, mean train loss=1.11]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 107.08it/s, mean loss=0.877, macro f1=0.457]
train [epoch 3 batch 60]: : 1440it [00:31, 46.39it/s, mean train loss=0.831]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 106.30it/s, mean loss=0.727, macro f1=0.522]
train [epoch 4 batch 60]: : 1440it [00:31, 46.45it/s, mean train loss=0.727]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 107.48it/s, mean loss=0.616, macro f1=0.609]
train [epoch 5 batch 60]: : 1440it [00:31, 46.44it/s, mean train loss=0.569]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 106.09it/s, mean loss=0.51, macro f1=0.746]
train [epoch 6 batch 60]: : 1440it [00:31, 46.29it/s, mean train loss=0.509]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 106.96it/s, mean loss=0.474, macro f1=0.761]
[I 2024-10-19 19:58:18,714] Trial 13 pruned. </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 14 params: {'batch_size': 32, 'lr': 0.0003732888404852332, 'weight_decay': 0.0018890159484641644, 'reduce_lr_factor': 0.03510400357330958, 'momentum': 0.8673167170090301, 'early_stop_patience': 2, 'resize_length': 64, 'grayscale': True, 'jitter': True, 'horizontal_flip': True, 'rotation': False, 'vgg16_batch_norm': True}
Trial 14 fold 1</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 45]: : 1440it [00:08, 171.21it/s, mean train loss=1.77]
validate [batch 12]: : 384it [00:01, 252.89it/s, mean loss=1.85, macro f1=0.0509]
train [epoch 2 batch 45]: : 1440it [00:08, 173.91it/s, mean train loss=1.2]
validate [batch 12]: : 384it [00:01, 211.11it/s, mean loss=0.994, macro f1=0.371]
train [epoch 3 batch 45]: : 1440it [00:08, 173.95it/s, mean train loss=0.97]
validate [batch 12]: : 384it [00:01, 245.78it/s, mean loss=0.792, macro f1=0.479]
train [epoch 4 batch 45]: : 1440it [00:08, 171.31it/s, mean train loss=0.715]
validate [batch 12]: : 384it [00:01, 251.80it/s, mean loss=0.688, macro f1=0.484]
train [epoch 5 batch 45]: : 1440it [00:08, 175.26it/s, mean train loss=0.589]
validate [batch 12]: : 384it [00:01, 246.78it/s, mean loss=0.592, macro f1=0.596]
train [epoch 6 batch 45]: : 1440it [00:08, 166.68it/s, mean train loss=0.548]
validate [batch 12]: : 384it [00:01, 251.51it/s, mean loss=0.54, macro f1=0.658]
[I 2024-10-19 19:59:21,146] Trial 14 pruned. </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 15 params: {'batch_size': 24, 'lr': 0.00047572214685467765, 'weight_decay': 0.004737713732629837, 'reduce_lr_factor': 0.07534940915322018, 'momentum': 0.8825874139599189, 'early_stop_patience': 3, 'resize_length': 128, 'grayscale': False, 'jitter': False, 'horizontal_flip': False, 'rotation': True, 'vgg16_batch_norm': True}
Trial 15 fold 1</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 60]: : 1440it [00:14, 102.72it/s, mean train loss=1.67]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 182.65it/s, mean loss=1.3, macro f1=0.202]
train [epoch 2 batch 60]: : 1440it [00:14, 101.37it/s, mean train loss=1.19]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 182.43it/s, mean loss=0.89, macro f1=0.377]
train [epoch 3 batch 60]: : 1440it [00:13, 103.24it/s, mean train loss=0.761]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 181.82it/s, mean loss=0.624, macro f1=0.623]
train [epoch 4 batch 60]: : 1440it [00:14, 102.55it/s, mean train loss=0.592]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 183.40it/s, mean loss=0.458, macro f1=0.745]
train [epoch 5 batch 60]: : 1440it [00:13, 103.48it/s, mean train loss=0.453]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 180.82it/s, mean loss=0.425, macro f1=0.768]
train [epoch 6 batch 60]: : 1440it [00:14, 101.79it/s, mean train loss=0.416]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 181.83it/s, mean loss=0.447, macro f1=0.781]
[I 2024-10-19 20:00:59,958] Trial 15 pruned. </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 16 params: {'batch_size': 24, 'lr': 0.0008731032162838153, 'weight_decay': 0.004471853039748174, 'reduce_lr_factor': 0.0429246070738525, 'momentum': 0.8376014150056645, 'early_stop_patience': 1, 'resize_length': 64, 'grayscale': False, 'jitter': True, 'horizontal_flip': False, 'rotation': False, 'vgg16_batch_norm': True}
Trial 16 fold 1</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 60]: : 1440it [00:09, 157.47it/s, mean train loss=1.45]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 229.93it/s, mean loss=1.02, macro f1=0.295]
train [epoch 2 batch 60]: : 1440it [00:08, 161.15it/s, mean train loss=0.762]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 224.79it/s, mean loss=0.747, macro f1=0.622]
train [epoch 3 batch 60]: : 1440it [00:09, 153.31it/s, mean train loss=0.475]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 203.95it/s, mean loss=0.447, macro f1=0.731]
train [epoch 4 batch 60]: : 1440it [00:08, 161.07it/s, mean train loss=0.333]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 230.01it/s, mean loss=0.485, macro f1=0.75]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Early stopping at epoch 4</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>test [batch 38]: : 912it [00:03, 248.36it/s, mean loss=0.462, macro f1=0.786]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 16 fold 2</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 60]: : 1440it [00:08, 160.16it/s, mean train loss=1.52]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 218.59it/s, mean loss=1.05, macro f1=0.307]
train [epoch 2 batch 60]: : 1440it [00:09, 154.12it/s, mean train loss=0.836]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 230.88it/s, mean loss=0.7, macro f1=0.571]
train [epoch 3 batch 60]: : 1440it [00:08, 161.31it/s, mean train loss=0.505]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 231.74it/s, mean loss=0.512, macro f1=0.687]
train [epoch 4 batch 60]: : 1440it [00:08, 161.10it/s, mean train loss=0.317]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 224.79it/s, mean loss=0.464, macro f1=0.821]
train [epoch 5 batch 60]: : 1440it [00:09, 152.06it/s, mean train loss=0.248]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 229.22it/s, mean loss=0.544, macro f1=0.789]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Early stopping at epoch 5</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>test [batch 38]: : 912it [00:03, 246.18it/s, mean loss=0.507, macro f1=0.765]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 16 fold 3</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 60]: : 1440it [00:08, 160.63it/s, mean train loss=1.43]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 227.05it/s, mean loss=1.02, macro f1=0.367]
train [epoch 2 batch 60]: : 1440it [00:09, 157.80it/s, mean train loss=0.761]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 195.16it/s, mean loss=0.572, macro f1=0.691]
train [epoch 3 batch 60]: : 1440it [00:09, 158.07it/s, mean train loss=0.451]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 231.37it/s, mean loss=0.511, macro f1=0.706]
train [epoch 4 batch 60]: : 1440it [00:08, 160.68it/s, mean train loss=0.34]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 229.84it/s, mean loss=0.379, macro f1=0.787]
train [epoch 5 batch 60]: : 1440it [00:09, 159.20it/s, mean train loss=0.262]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 197.62it/s, mean loss=0.396, macro f1=0.761]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Early stopping at epoch 5</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>test [batch 38]: : 912it [00:03, 229.26it/s, mean loss=0.422, macro f1=0.803]
[I 2024-10-19 20:03:48,241] Trial 16 finished with value: 0.7846964692207749 and parameters: {'batch_size': 24, 'lr': 0.0008731032162838153, 'weight_decay': 0.004471853039748174, 'reduce_lr_factor': 0.0429246070738525, 'momentum': 0.8376014150056645, 'early_stop_patience': 1, 'resize_length': 64, 'grayscale': False, 'jitter': True, 'horizontal_flip': False, 'rotation': False, 'vgg16_batch_norm': True}. Best is trial 3 with value: 0.7984415820297991.</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 17 params: {'batch_size': 24, 'lr': 0.0009916011134837074, 'weight_decay': 0.0043502690778127765, 'reduce_lr_factor': 0.025162260173005026, 'momentum': 0.8374267532215826, 'early_stop_patience': 1, 'resize_length': 64, 'grayscale': True, 'jitter': True, 'horizontal_flip': True, 'rotation': False, 'vgg16_batch_norm': True}
Trial 17 fold 1</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 60]: : 1440it [00:08, 160.16it/s, mean train loss=1.51]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 230.41it/s, mean loss=1.05, macro f1=0.324]
train [epoch 2 batch 60]: : 1440it [00:08, 160.87it/s, mean train loss=0.95]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 228.07it/s, mean loss=0.757, macro f1=0.533]
train [epoch 3 batch 60]: : 1440it [00:09, 156.36it/s, mean train loss=0.646]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 228.66it/s, mean loss=0.631, macro f1=0.555]
train [epoch 4 batch 60]: : 1440it [00:08, 160.45it/s, mean train loss=0.554]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 227.19it/s, mean loss=0.573, macro f1=0.557]
train [epoch 5 batch 60]: : 1440it [00:08, 160.90it/s, mean train loss=0.407]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 221.05it/s, mean loss=0.445, macro f1=0.74]
train [epoch 6 batch 60]: : 1440it [00:09, 150.36it/s, mean train loss=0.376]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 217.21it/s, mean loss=0.447, macro f1=0.757]
[I 2024-10-19 20:04:55,227] Trial 17 pruned. </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 18 params: {'batch_size': 24, 'lr': 0.0008682908052384192, 'weight_decay': 0.0009480799177705037, 'reduce_lr_factor': 0.04269622508194973, 'momentum': 0.8061197489142833, 'early_stop_patience': 1, 'resize_length': 32, 'grayscale': True, 'jitter': True, 'horizontal_flip': True, 'rotation': False, 'vgg16_batch_norm': False}
Trial 18 fold 1</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 60]: : 1440it [00:07, 183.70it/s, mean train loss=2.29]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 257.15it/s, mean loss=2.27, macro f1=0.0581]
train [epoch 2 batch 60]: : 1440it [00:07, 185.40it/s, mean train loss=2.26]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 259.55it/s, mean loss=2.24, macro f1=0.0581]
train [epoch 3 batch 60]: : 1440it [00:07, 180.87it/s, mean train loss=2.23]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 241.28it/s, mean loss=2.21, macro f1=0.0581]
train [epoch 4 batch 60]: : 1440it [00:08, 177.00it/s, mean train loss=2.2]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 261.68it/s, mean loss=2.19, macro f1=0.0581]
train [epoch 5 batch 60]: : 1440it [00:07, 186.95it/s, mean train loss=2.18]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 255.71it/s, mean loss=2.16, macro f1=0.0581]
[I 2024-10-19 20:05:44,075] Trial 18 pruned. Mean validation loss is still above or equal to 1.5 at the 5th epoch.</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 19 params: {'batch_size': 24, 'lr': 0.0008848104981850575, 'weight_decay': 0.0018407051447151832, 'reduce_lr_factor': 0.020811431988625707, 'momentum': 0.834584433021492, 'early_stop_patience': 1, 'resize_length': 64, 'grayscale': False, 'jitter': True, 'horizontal_flip': False, 'rotation': False, 'vgg16_batch_norm': True}
Trial 19 fold 1</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 60]: : 1440it [00:08, 162.12it/s, mean train loss=1.49]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 236.92it/s, mean loss=1.08, macro f1=0.452]
train [epoch 2 batch 60]: : 1440it [00:09, 156.33it/s, mean train loss=0.76]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 235.72it/s, mean loss=0.771, macro f1=0.6]
train [epoch 3 batch 60]: : 1440it [00:08, 167.14it/s, mean train loss=0.435]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 234.66it/s, mean loss=0.552, macro f1=0.727]
train [epoch 4 batch 60]: : 1440it [00:09, 158.00it/s, mean train loss=0.32]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 237.24it/s, mean loss=0.434, macro f1=0.779]
train [epoch 5 batch 60]: : 1440it [00:09, 158.78it/s, mean train loss=0.261]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 234.50it/s, mean loss=0.473, macro f1=0.707]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Early stopping at epoch 5</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>test [batch 38]: : 912it [00:03, 245.15it/s, mean loss=0.509, macro f1=0.725]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 19 fold 2</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 60]: : 1440it [00:09, 157.47it/s, mean train loss=1.53]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 237.00it/s, mean loss=1.11, macro f1=0.393]
train [epoch 2 batch 60]: : 1440it [00:08, 161.41it/s, mean train loss=0.888]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 237.84it/s, mean loss=0.761, macro f1=0.551]
train [epoch 3 batch 60]: : 1440it [00:08, 164.49it/s, mean train loss=0.544]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 244.43it/s, mean loss=0.569, macro f1=0.698]
train [epoch 4 batch 60]: : 1440it [00:09, 158.39it/s, mean train loss=0.326]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 240.81it/s, mean loss=0.45, macro f1=0.768]
train [epoch 5 batch 60]: : 1440it [00:08, 167.72it/s, mean train loss=0.319]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 235.98it/s, mean loss=0.404, macro f1=0.838]
train [epoch 6 batch 60]: : 1440it [00:09, 158.64it/s, mean train loss=0.196]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 233.37it/s, mean loss=0.36, macro f1=0.847]
train [epoch 7 batch 60]: : 1440it [00:09, 157.94it/s, mean train loss=0.176]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 231.07it/s, mean loss=0.466, macro f1=0.8]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Early stopping at epoch 7</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>test [batch 38]: : 912it [00:03, 257.24it/s, mean loss=0.546, macro f1=0.765]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 19 fold 3</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 60]: : 1440it [00:09, 159.91it/s, mean train loss=1.48]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 233.60it/s, mean loss=1.1, macro f1=0.435]
train [epoch 2 batch 60]: : 1440it [00:09, 154.02it/s, mean train loss=0.778]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 231.30it/s, mean loss=0.631, macro f1=0.611]
train [epoch 3 batch 60]: : 1440it [00:08, 163.50it/s, mean train loss=0.475]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 247.78it/s, mean loss=0.53, macro f1=0.744]
train [epoch 4 batch 60]: : 1440it [00:08, 160.74it/s, mean train loss=0.355]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 240.03it/s, mean loss=0.492, macro f1=0.745]
train [epoch 5 batch 60]: : 1440it [00:09, 158.38it/s, mean train loss=0.25]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 238.98it/s, mean loss=0.377, macro f1=0.826]
train [epoch 6 batch 60]: : 1440it [00:09, 156.92it/s, mean train loss=0.189]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 240.47it/s, mean loss=0.331, macro f1=0.835]
train [epoch 7 batch 60]: : 1440it [00:08, 161.46it/s, mean train loss=0.217]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 237.31it/s, mean loss=0.352, macro f1=0.84]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Early stopping at epoch 7</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>test [batch 38]: : 912it [00:03, 250.36it/s, mean loss=0.446, macro f1=0.807]
[I 2024-10-19 20:09:23,031] Trial 19 finished with value: 0.7655126149356413 and parameters: {'batch_size': 24, 'lr': 0.0008848104981850575, 'weight_decay': 0.0018407051447151832, 'reduce_lr_factor': 0.020811431988625707, 'momentum': 0.834584433021492, 'early_stop_patience': 1, 'resize_length': 64, 'grayscale': False, 'jitter': True, 'horizontal_flip': False, 'rotation': False, 'vgg16_batch_norm': True}. Best is trial 3 with value: 0.7984415820297991.</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 20 params: {'batch_size': 24, 'lr': 1.061325571229696e-05, 'weight_decay': 0.00012321089739614753, 'reduce_lr_factor': 0.042649330962282274, 'momentum': 0.8200846878990533, 'early_stop_patience': 1, 'resize_length': 96, 'grayscale': True, 'jitter': True, 'horizontal_flip': False, 'rotation': False, 'vgg16_batch_norm': True}
Trial 20 fold 1</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 60]: : 1440it [00:11, 120.04it/s, mean train loss=2.23]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 206.10it/s, mean loss=2.16, macro f1=0.156]
train [epoch 2 batch 60]: : 1440it [00:11, 126.53it/s, mean train loss=2.12]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 205.53it/s, mean loss=2.01, macro f1=0.158]
train [epoch 3 batch 60]: : 1440it [00:11, 122.54it/s, mean train loss=2.01]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 188.30it/s, mean loss=1.94, macro f1=0.176]
train [epoch 4 batch 60]: : 1440it [00:11, 126.71it/s, mean train loss=1.94]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 207.82it/s, mean loss=1.87, macro f1=0.19]
train [epoch 5 batch 60]: : 1440it [00:11, 126.80it/s, mean train loss=1.88]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 206.49it/s, mean loss=1.83, macro f1=0.19]
[I 2024-10-19 20:10:32,179] Trial 20 pruned. Mean validation loss is still above or equal to 1.5 at the 5th epoch.</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 21 params: {'batch_size': 24, 'lr': 0.0009151928451481278, 'weight_decay': 0.001643161022766956, 'reduce_lr_factor': 0.01695784221558099, 'momentum': 0.8389391528297435, 'early_stop_patience': 1, 'resize_length': 64, 'grayscale': False, 'jitter': True, 'horizontal_flip': False, 'rotation': False, 'vgg16_batch_norm': True}
Trial 21 fold 1</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 60]: : 1440it [00:09, 150.63it/s, mean train loss=1.45]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 226.62it/s, mean loss=0.983, macro f1=0.522]
train [epoch 2 batch 60]: : 1440it [00:08, 160.89it/s, mean train loss=0.726]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 231.20it/s, mean loss=0.665, macro f1=0.693]
train [epoch 3 batch 60]: : 1440it [00:08, 161.09it/s, mean train loss=0.429]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 216.35it/s, mean loss=0.519, macro f1=0.756]
train [epoch 4 batch 60]: : 1440it [00:09, 151.21it/s, mean train loss=0.334]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 229.33it/s, mean loss=0.43, macro f1=0.762]
train [epoch 5 batch 60]: : 1440it [00:08, 161.05it/s, mean train loss=0.275]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 230.47it/s, mean loss=0.428, macro f1=0.795]
train [epoch 6 batch 60]: : 1440it [00:08, 160.57it/s, mean train loss=0.156]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 223.72it/s, mean loss=0.475, macro f1=0.807]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Early stopping at epoch 6</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>test [batch 38]: : 912it [00:03, 247.95it/s, mean loss=0.417, macro f1=0.836]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 21 fold 2</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 60]: : 1440it [00:09, 149.76it/s, mean train loss=1.47]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 238.43it/s, mean loss=1.07, macro f1=0.304]
train [epoch 2 batch 60]: : 1440it [00:08, 166.54it/s, mean train loss=0.837]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 245.84it/s, mean loss=0.604, macro f1=0.689]
train [epoch 3 batch 60]: : 1440it [00:08, 164.09it/s, mean train loss=0.513]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 243.11it/s, mean loss=0.822, macro f1=0.669]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Early stopping at epoch 3</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>test [batch 38]: : 912it [00:03, 255.95it/s, mean loss=0.779, macro f1=0.645]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 21 fold 3</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 60]: : 1440it [00:09, 155.86it/s, mean train loss=1.46]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 233.65it/s, mean loss=0.992, macro f1=0.608]
train [epoch 2 batch 60]: : 1440it [00:08, 161.35it/s, mean train loss=0.751]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 204.15it/s, mean loss=0.59, macro f1=0.696]
train [epoch 3 batch 60]: : 1440it [00:09, 159.60it/s, mean train loss=0.483]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 229.42it/s, mean loss=0.429, macro f1=0.768]
train [epoch 4 batch 60]: : 1440it [00:09, 158.07it/s, mean train loss=0.308]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 238.76it/s, mean loss=0.495, macro f1=0.763]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Early stopping at epoch 4</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>test [batch 38]: : 912it [00:03, 260.45it/s, mean loss=0.576, macro f1=0.763]
[I 2024-10-19 20:13:07,988] Trial 21 finished with value: 0.7478920106660203 and parameters: {'batch_size': 24, 'lr': 0.0009151928451481278, 'weight_decay': 0.001643161022766956, 'reduce_lr_factor': 0.01695784221558099, 'momentum': 0.8389391528297435, 'early_stop_patience': 1, 'resize_length': 64, 'grayscale': False, 'jitter': True, 'horizontal_flip': False, 'rotation': False, 'vgg16_batch_norm': True}. Best is trial 3 with value: 0.7984415820297991.</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 22 params: {'batch_size': 24, 'lr': 0.000807275411396522, 'weight_decay': 0.003030434542243136, 'reduce_lr_factor': 0.0233790072538175, 'momentum': 0.8539383714917498, 'early_stop_patience': 1, 'resize_length': 64, 'grayscale': False, 'jitter': True, 'horizontal_flip': False, 'rotation': False, 'vgg16_batch_norm': True}
Trial 22 fold 1</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 60]: : 1440it [00:09, 157.83it/s, mean train loss=1.47]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 233.52it/s, mean loss=1.07, macro f1=0.476]
train [epoch 2 batch 60]: : 1440it [00:08, 167.07it/s, mean train loss=0.759]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 202.36it/s, mean loss=0.572, macro f1=0.668]
train [epoch 3 batch 60]: : 1440it [00:08, 161.72it/s, mean train loss=0.477]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 236.76it/s, mean loss=0.492, macro f1=0.735]
train [epoch 4 batch 60]: : 1440it [00:08, 165.37it/s, mean train loss=0.343]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 233.48it/s, mean loss=0.531, macro f1=0.709]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Early stopping at epoch 4</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>test [batch 38]: : 912it [00:03, 256.19it/s, mean loss=0.52, macro f1=0.715]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 22 fold 2</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 60]: : 1440it [00:09, 157.39it/s, mean train loss=1.57]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 186.62it/s, mean loss=1.06, macro f1=0.45]
train [epoch 2 batch 60]: : 1440it [00:09, 156.04it/s, mean train loss=0.868]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 214.11it/s, mean loss=0.804, macro f1=0.492]
train [epoch 3 batch 60]: : 1440it [00:08, 161.00it/s, mean train loss=0.519]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 232.43it/s, mean loss=0.564, macro f1=0.662]
train [epoch 4 batch 60]: : 1440it [00:08, 161.19it/s, mean train loss=0.361]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 214.25it/s, mean loss=0.522, macro f1=0.712]
train [epoch 5 batch 60]: : 1440it [00:09, 155.10it/s, mean train loss=0.249]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 229.17it/s, mean loss=0.515, macro f1=0.808]
train [epoch 6 batch 60]: : 1440it [00:08, 160.86it/s, mean train loss=0.26]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 229.36it/s, mean loss=0.473, macro f1=0.805]
[I 2024-10-19 20:15:02,347] Trial 22 pruned. </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 23 params: {'batch_size': 24, 'lr': 0.0008994724021985357, 'weight_decay': 0.00398890739787271, 'reduce_lr_factor': 0.02268743414914747, 'momentum': 0.8216647476937027, 'early_stop_patience': 1, 'resize_length': 32, 'grayscale': False, 'jitter': True, 'horizontal_flip': False, 'rotation': False, 'vgg16_batch_norm': True}
Trial 23 fold 1</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 60]: : 1440it [00:07, 183.93it/s, mean train loss=1.41]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 239.79it/s, mean loss=1.11, macro f1=0.336]
train [epoch 2 batch 60]: : 1440it [00:08, 176.84it/s, mean train loss=0.73]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 227.36it/s, mean loss=0.799, macro f1=0.587]
train [epoch 3 batch 60]: : 1440it [00:07, 183.45it/s, mean train loss=0.437]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 240.45it/s, mean loss=0.775, macro f1=0.658]
train [epoch 4 batch 60]: : 1440it [00:07, 183.56it/s, mean train loss=0.34]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 242.91it/s, mean loss=0.542, macro f1=0.752]
train [epoch 5 batch 60]: : 1440it [00:07, 183.21it/s, mean train loss=0.284]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 228.38it/s, mean loss=0.669, macro f1=0.706]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Early stopping at epoch 5</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>test [batch 38]: : 912it [00:04, 209.84it/s, mean loss=0.653, macro f1=0.722]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 23 fold 2</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 60]: : 1440it [00:07, 181.85it/s, mean train loss=1.4]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 243.83it/s, mean loss=0.916, macro f1=0.523]
train [epoch 2 batch 60]: : 1440it [00:07, 183.08it/s, mean train loss=0.625]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 244.55it/s, mean loss=0.624, macro f1=0.579]
train [epoch 3 batch 60]: : 1440it [00:07, 182.69it/s, mean train loss=0.451]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 236.83it/s, mean loss=0.803, macro f1=0.672]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Early stopping at epoch 3</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>test [batch 38]: : 912it [00:04, 226.29it/s, mean loss=0.731, macro f1=0.697]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 23 fold 3</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 60]: : 1440it [00:08, 179.22it/s, mean train loss=1.39]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 245.18it/s, mean loss=0.889, macro f1=0.37]
train [epoch 2 batch 60]: : 1440it [00:07, 182.83it/s, mean train loss=0.698]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 244.52it/s, mean loss=0.652, macro f1=0.646]
train [epoch 3 batch 60]: : 1440it [00:08, 177.93it/s, mean train loss=0.48]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 245.99it/s, mean loss=0.579, macro f1=0.772]
train [epoch 4 batch 60]: : 1440it [00:08, 171.50it/s, mean train loss=0.347]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 242.08it/s, mean loss=0.641, macro f1=0.747]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Early stopping at epoch 4</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>test [batch 38]: : 912it [00:03, 261.42it/s, mean loss=0.62, macro f1=0.739]
[I 2024-10-19 20:17:14,048] Trial 23 finished with value: 0.7192742660990045 and parameters: {'batch_size': 24, 'lr': 0.0008994724021985357, 'weight_decay': 0.00398890739787271, 'reduce_lr_factor': 0.02268743414914747, 'momentum': 0.8216647476937027, 'early_stop_patience': 1, 'resize_length': 32, 'grayscale': False, 'jitter': True, 'horizontal_flip': False, 'rotation': False, 'vgg16_batch_norm': True}. Best is trial 3 with value: 0.7984415820297991.</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 24 params: {'batch_size': 24, 'lr': 0.0005885471433678505, 'weight_decay': 0.0021420265945451033, 'reduce_lr_factor': 0.04064712585336279, 'momentum': 0.8321911216465314, 'early_stop_patience': 1, 'resize_length': 96, 'grayscale': False, 'jitter': True, 'horizontal_flip': False, 'rotation': False, 'vgg16_batch_norm': True}
Trial 24 fold 1</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 60]: : 1440it [00:11, 126.65it/s, mean train loss=1.57]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 197.80it/s, mean loss=1.22, macro f1=0.242]
train [epoch 2 batch 60]: : 1440it [00:11, 124.82it/s, mean train loss=0.987]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 198.43it/s, mean loss=0.809, macro f1=0.58]
train [epoch 3 batch 60]: : 1440it [00:11, 125.21it/s, mean train loss=0.661]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 206.84it/s, mean loss=0.579, macro f1=0.67]
train [epoch 4 batch 60]: : 1440it [00:11, 125.34it/s, mean train loss=0.466]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 205.72it/s, mean loss=0.49, macro f1=0.779]
train [epoch 5 batch 60]: : 1440it [00:12, 119.67it/s, mean train loss=0.343]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 199.78it/s, mean loss=0.482, macro f1=0.738]
train [epoch 6 batch 60]: : 1440it [00:11, 125.32it/s, mean train loss=0.254]
validate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 206.65it/s, mean loss=0.494, macro f1=0.769]
[I 2024-10-19 20:18:36,863] Trial 24 pruned. </code></pre>
</div>
</div>
</div>
<div id="d5e9ca23" class="cell">
<div class="sourceCode cell-code" id="cb412"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb412-1"><a href="#cb412-1" aria-hidden="true" tabindex="-1"></a>vgg16_study.best_params</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-container">
<div class="cell-output cell-output-display" data-execution_count="39">
<pre><code>{'batch_size': 24,
 'lr': 0.000988490099678634,
 'weight_decay': 0.0010204481074802807,
 'reduce_lr_factor': 0.028798908048535125,
 'momentum': 0.8306488083981494,
 'early_stop_patience': 2,
 'resize_length': 64,
 'grayscale': True,
 'jitter': True,
 'horizontal_flip': True,
 'rotation': False,
 'vgg16_batch_norm': True}</code></pre>
</div>
</div>
</div>
<p>Like the MLP, the best hyperparameters include resizing of the images to 64 by 64. However, this time, VGG-16 seems to perform best when some data augmentation is applied (colour jittering and random horizontal flip). The results also indicate that batch normalisation should be used.</p>
</section>
<section id="vgg-16-pretrained-1" class="level3">
<h3 class="anchored" data-anchor-id="vgg-16-pretrained-1">VGG-16 Pretrained</h3>
<p>No model-specific hyperparameters were tuned for the pre-trained VGG-16 model since its architecture is fixed. However, in the future, I may want to consider whether to freeze the convolutional layers.</p>
<p>As mentioned earlier, no resizing or grayscaling was applied to the images since the pretrained model requires using the same transformations applied when it was trained on ImageNet. However, whether to use data augmentation is still part of the search space.</p>
<div id="ac6d0719" class="cell">
<div class="sourceCode cell-code" id="cb414"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb414-1"><a href="#cb414-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> vgg16_pretrained_from_params(params: <span class="bu">dict</span>) <span class="op">-&gt;</span> torchvision.models.vgg.VGG:</span>
<span id="cb414-2"><a href="#cb414-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> make_vgg16_pretrained()</span>
<span id="cb414-3"><a href="#cb414-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb414-4"><a href="#cb414-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb414-5"><a href="#cb414-5" aria-hidden="true" tabindex="-1"></a>vgg16_pretrained_study <span class="op">=</span> optuna.create_study(</span>
<span id="cb414-6"><a href="#cb414-6" aria-hidden="true" tabindex="-1"></a>    study_name<span class="op">=</span><span class="st">"VGG16 Pretrained study"</span>,</span>
<span id="cb414-7"><a href="#cb414-7" aria-hidden="true" tabindex="-1"></a>    sampler<span class="op">=</span>optuna.samplers.TPESampler(seed<span class="op">=</span><span class="dv">0</span>),</span>
<span id="cb414-8"><a href="#cb414-8" aria-hidden="true" tabindex="-1"></a>    pruner<span class="op">=</span>optuna.pruners.MedianPruner(n_startup_trials<span class="op">=</span><span class="dv">3</span>, n_warmup_steps<span class="op">=</span><span class="dv">5</span>),</span>
<span id="cb414-9"><a href="#cb414-9" aria-hidden="true" tabindex="-1"></a>    directions<span class="op">=</span>[<span class="st">"maximize"</span>],</span>
<span id="cb414-10"><a href="#cb414-10" aria-hidden="true" tabindex="-1"></a>    storage<span class="op">=</span>optuna.storages.JournalStorage(</span>
<span id="cb414-11"><a href="#cb414-11" aria-hidden="true" tabindex="-1"></a>        optuna.storages.journal.JournalFileBackend(<span class="st">"vgg16_pretrained_journal.log"</span>),</span>
<span id="cb414-12"><a href="#cb414-12" aria-hidden="true" tabindex="-1"></a>    ),</span>
<span id="cb414-13"><a href="#cb414-13" aria-hidden="true" tabindex="-1"></a>    load_if_exists<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb414-14"><a href="#cb414-14" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb414-15"><a href="#cb414-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb414-16"><a href="#cb414-16" aria-hidden="true" tabindex="-1"></a>vgg16_pretrained_study.optimize(</span>
<span id="cb414-17"><a href="#cb414-17" aria-hidden="true" tabindex="-1"></a>    make_objective(</span>
<span id="cb414-18"><a href="#cb414-18" aria-hidden="true" tabindex="-1"></a>        <span class="kw">lambda</span> trial: <span class="va">None</span>,</span>
<span id="cb414-19"><a href="#cb414-19" aria-hidden="true" tabindex="-1"></a>        vgg16_pretrained_from_params,</span>
<span id="cb414-20"><a href="#cb414-20" aria-hidden="true" tabindex="-1"></a>        ds_train,</span>
<span id="cb414-21"><a href="#cb414-21" aria-hidden="true" tabindex="-1"></a>        transforms_override<span class="op">=</span>torchvision.models.VGG16_Weights.IMAGENET1K_V1.transforms(),</span>
<span id="cb414-22"><a href="#cb414-22" aria-hidden="true" tabindex="-1"></a>    ),</span>
<span id="cb414-23"><a href="#cb414-23" aria-hidden="true" tabindex="-1"></a>    n_trials<span class="op">=</span>N_TRIALS,</span>
<span id="cb414-24"><a href="#cb414-24" aria-hidden="true" tabindex="-1"></a>    gc_after_trial<span class="op">=</span><span class="va">True</span></span>
<span id="cb414-25"><a href="#cb414-25" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-container">
<div class="cell-output cell-output-stderr">
<pre><code>[I 2024-10-19 20:19:05,248] A new study created in Journal with name: VGG16 Pretrained study</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 0 params: {'batch_size': 32, 'lr': 0.0007180374727086954, 'weight_decay': 0.006027633760716439, 'reduce_lr_factor': 0.05903948646972072, 'momentum': 0.8804944118743919, 'early_stop_patience': 2, 'jitter': False, 'horizontal_flip': True, 'rotation': True}
Trial 0 fold 1</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 45]: : 1440it [00:24, 59.23it/s, mean train loss=1.61]
validate [batch 12]: : 384it [00:03, 122.75it/s, mean loss=1.08, macro f1=0.287]
train [epoch 2 batch 45]: : 1440it [00:24, 57.80it/s, mean train loss=0.903]
validate [batch 12]: : 384it [00:03, 125.08it/s, mean loss=0.706, macro f1=0.724]
train [epoch 3 batch 45]: : 1440it [00:24, 58.55it/s, mean train loss=0.582]
validate [batch 12]: : 384it [00:03, 127.50it/s, mean loss=0.481, macro f1=0.782]
train [epoch 4 batch 45]: : 1440it [00:24, 58.39it/s, mean train loss=0.411]
validate [batch 12]: : 384it [00:03, 126.71it/s, mean loss=0.373, macro f1=0.836]
train [epoch 5 batch 45]: : 1440it [00:24, 58.35it/s, mean train loss=0.319]
validate [batch 12]: : 384it [00:03, 119.95it/s, mean loss=0.356, macro f1=0.856]
train [epoch 6 batch 45]: : 1440it [00:24, 58.67it/s, mean train loss=0.232]
validate [batch 12]: : 384it [00:03, 127.35it/s, mean loss=0.275, macro f1=0.846]
train [epoch 7 batch 45]: : 1440it [00:24, 58.40it/s, mean train loss=0.208]
validate [batch 12]: : 384it [00:03, 125.92it/s, mean loss=0.392, macro f1=0.841]
train [epoch 8 batch 45]: : 1440it [00:24, 58.06it/s, mean train loss=0.185]
validate [batch 12]: : 384it [00:03, 125.74it/s, mean loss=0.328, macro f1=0.855]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Early stopping at epoch 8</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>test [batch 29]: : 928it [00:07, 131.38it/s, mean loss=0.237, macro f1=0.886]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 0 fold 2</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 45]: : 1440it [00:24, 58.28it/s, mean train loss=1.64]
validate [batch 12]: : 384it [00:03, 126.31it/s, mean loss=1.18, macro f1=0.482]
train [epoch 2 batch 45]: : 1440it [00:24, 58.14it/s, mean train loss=0.914]
validate [batch 12]: : 384it [00:02, 128.37it/s, mean loss=0.646, macro f1=0.725]
train [epoch 3 batch 45]: : 1440it [00:24, 58.52it/s, mean train loss=0.56]
validate [batch 12]: : 384it [00:03, 123.05it/s, mean loss=0.421, macro f1=0.814]
train [epoch 4 batch 45]: : 1440it [00:24, 58.69it/s, mean train loss=0.444]
validate [batch 12]: : 384it [00:03, 122.46it/s, mean loss=0.384, macro f1=0.824]
train [epoch 5 batch 45]: : 1440it [00:24, 58.27it/s, mean train loss=0.302]
validate [batch 12]: : 384it [00:02, 128.27it/s, mean loss=0.331, macro f1=0.869]
train [epoch 6 batch 45]: : 1440it [00:24, 58.43it/s, mean train loss=0.269]
validate [batch 12]: : 384it [00:02, 128.06it/s, mean loss=0.278, macro f1=0.876]
train [epoch 7 batch 45]: : 1440it [00:24, 58.21it/s, mean train loss=0.235]
validate [batch 12]: : 384it [00:03, 127.73it/s, mean loss=0.33, macro f1=0.85]
train [epoch 8 batch 45]: : 1440it [00:24, 58.33it/s, mean train loss=0.23]
validate [batch 12]: : 384it [00:03, 115.43it/s, mean loss=0.309, macro f1=0.869]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Early stopping at epoch 8</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>test [batch 29]: : 928it [00:07, 131.49it/s, mean loss=0.365, macro f1=0.853]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 0 fold 3</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 45]: : 1440it [00:24, 58.75it/s, mean train loss=1.61]
validate [batch 12]: : 384it [00:03, 124.03it/s, mean loss=1.07, macro f1=0.446]
train [epoch 2 batch 45]: : 1440it [00:24, 58.70it/s, mean train loss=0.768]
validate [batch 12]: : 384it [00:03, 127.65it/s, mean loss=0.655, macro f1=0.689]
train [epoch 3 batch 45]: : 1440it [00:24, 58.08it/s, mean train loss=0.462]
validate [batch 12]: : 384it [00:03, 126.18it/s, mean loss=0.429, macro f1=0.786]
train [epoch 4 batch 45]: : 1440it [00:24, 58.28it/s, mean train loss=0.371]
validate [batch 12]: : 384it [00:03, 126.48it/s, mean loss=0.397, macro f1=0.807]
train [epoch 5 batch 45]: : 1440it [00:24, 58.26it/s, mean train loss=0.288]
validate [batch 12]: : 384it [00:03, 127.55it/s, mean loss=0.311, macro f1=0.808]
train [epoch 6 batch 45]: : 1440it [00:24, 58.58it/s, mean train loss=0.243]
validate [batch 12]: : 384it [00:03, 124.19it/s, mean loss=0.287, macro f1=0.852]
train [epoch 7 batch 45]: : 1440it [00:24, 58.62it/s, mean train loss=0.18]
validate [batch 12]: : 384it [00:03, 125.50it/s, mean loss=0.309, macro f1=0.859]
train [epoch 8 batch 45]: : 1440it [00:24, 58.31it/s, mean train loss=0.136]
validate [batch 12]: : 384it [00:03, 127.37it/s, mean loss=0.313, macro f1=0.843]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Early stopping at epoch 8</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>test [batch 29]: : 928it [00:07, 128.64it/s, mean loss=0.27, macro f1=0.879]
[I 2024-10-19 20:30:46,032] Trial 0 finished with value: 0.8728714092677149 and parameters: {'batch_size': 32, 'lr': 0.0007180374727086954, 'weight_decay': 0.006027633760716439, 'reduce_lr_factor': 0.05903948646972072, 'momentum': 0.8804944118743919, 'early_stop_patience': 2, 'jitter': False, 'horizontal_flip': True, 'rotation': True}. Best is trial 0 with value: 0.8728714092677149.</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 1 params: {'batch_size': 32, 'lr': 0.0009263406719097344, 'weight_decay': 0.0007103605819788694, 'reduce_lr_factor': 0.017841636973138664, 'momentum': 0.8038414955136619, 'early_stop_patience': 3, 'jitter': False, 'horizontal_flip': True, 'rotation': False}
Trial 1 fold 1</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 45]: : 1440it [00:24, 58.54it/s, mean train loss=1.49]
validate [batch 12]: : 384it [00:03, 123.96it/s, mean loss=0.883, macro f1=0.565]
train [epoch 2 batch 45]: : 1440it [00:24, 58.66it/s, mean train loss=0.625]
validate [batch 12]: : 384it [00:03, 124.68it/s, mean loss=0.541, macro f1=0.732]
train [epoch 3 batch 45]: : 1440it [00:24, 58.63it/s, mean train loss=0.396]
validate [batch 12]: : 384it [00:03, 126.55it/s, mean loss=0.442, macro f1=0.809]
train [epoch 4 batch 45]: : 1440it [00:24, 59.04it/s, mean train loss=0.288]
validate [batch 12]: : 384it [00:03, 118.42it/s, mean loss=0.346, macro f1=0.851]
train [epoch 5 batch 45]: : 1440it [00:24, 58.47it/s, mean train loss=0.223]
validate [batch 12]: : 384it [00:03, 126.15it/s, mean loss=0.365, macro f1=0.839]
train [epoch 6 batch 45]: : 1440it [00:24, 58.43it/s, mean train loss=0.176]
validate [batch 12]: : 384it [00:03, 127.23it/s, mean loss=0.363, macro f1=0.847]
train [epoch 7 batch 45]: : 1440it [00:24, 58.42it/s, mean train loss=0.166]
validate [batch 12]: : 384it [00:03, 126.87it/s, mean loss=0.423, macro f1=0.789]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Early stopping at epoch 7</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>test [batch 29]: : 928it [00:07, 131.18it/s, mean loss=0.371, macro f1=0.837]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 1 fold 2</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 45]: : 1440it [00:24, 58.65it/s, mean train loss=1.55]
validate [batch 12]: : 384it [00:03, 126.84it/s, mean loss=1.09, macro f1=0.373]
train [epoch 2 batch 45]: : 1440it [00:24, 58.78it/s, mean train loss=0.955]
validate [batch 12]: : 384it [00:03, 127.56it/s, mean loss=0.698, macro f1=0.697]
train [epoch 3 batch 45]: : 1440it [00:24, 59.04it/s, mean train loss=0.612]
validate [batch 12]: : 384it [00:03, 117.46it/s, mean loss=0.483, macro f1=0.779]
train [epoch 4 batch 45]: : 1440it [00:24, 58.58it/s, mean train loss=0.472]
validate [batch 12]: : 384it [00:03, 126.37it/s, mean loss=0.434, macro f1=0.759]
train [epoch 5 batch 45]: : 1440it [00:24, 59.07it/s, mean train loss=0.384]
validate [batch 12]: : 384it [00:03, 126.49it/s, mean loss=0.364, macro f1=0.833]
train [epoch 6 batch 45]: : 1440it [00:24, 59.03it/s, mean train loss=0.299]
validate [batch 12]: : 384it [00:03, 125.63it/s, mean loss=0.296, macro f1=0.87]
train [epoch 7 batch 45]: : 1440it [00:24, 58.87it/s, mean train loss=0.261]
validate [batch 12]: : 384it [00:03, 116.36it/s, mean loss=0.228, macro f1=0.891]
train [epoch 8 batch 45]: : 1440it [00:24, 58.93it/s, mean train loss=0.199]
validate [batch 12]: : 384it [00:03, 127.54it/s, mean loss=0.305, macro f1=0.897]
train [epoch 9 batch 45]: : 1440it [00:24, 58.69it/s, mean train loss=0.157]
validate [batch 12]: : 384it [00:03, 126.17it/s, mean loss=0.237, macro f1=0.894]
train [epoch 10 batch 45]: : 1440it [00:24, 58.24it/s, mean train loss=0.141]
validate [batch 12]: : 384it [00:03, 126.22it/s, mean loss=0.236, macro f1=0.902]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Early stopping at epoch 10</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>test [batch 29]: : 928it [00:07, 131.14it/s, mean loss=0.225, macro f1=0.882]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 1 fold 3</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 45]: : 1440it [00:24, 58.50it/s, mean train loss=1.41]
validate [batch 12]: : 384it [00:03, 126.73it/s, mean loss=0.872, macro f1=0.709]
train [epoch 2 batch 45]: : 1440it [00:24, 58.56it/s, mean train loss=0.656]
validate [batch 12]: : 384it [00:03, 126.81it/s, mean loss=0.455, macro f1=0.75]
train [epoch 3 batch 45]: : 1440it [00:24, 59.08it/s, mean train loss=0.396]
validate [batch 12]: : 384it [00:03, 118.01it/s, mean loss=0.35, macro f1=0.826]
train [epoch 4 batch 45]: : 1440it [00:24, 58.71it/s, mean train loss=0.299]
validate [batch 12]: : 384it [00:03, 126.23it/s, mean loss=0.328, macro f1=0.841]
train [epoch 5 batch 45]: : 1440it [00:24, 58.53it/s, mean train loss=0.245]
validate [batch 12]: : 384it [00:03, 126.72it/s, mean loss=0.3, macro f1=0.871]
train [epoch 6 batch 45]: : 1440it [00:24, 58.49it/s, mean train loss=0.195]
validate [batch 12]: : 384it [00:03, 123.62it/s, mean loss=0.28, macro f1=0.868]
train [epoch 7 batch 45]: : 1440it [00:24, 59.09it/s, mean train loss=0.165]
validate [batch 12]: : 384it [00:03, 116.89it/s, mean loss=0.306, macro f1=0.88]
train [epoch 8 batch 45]: : 1440it [00:24, 58.84it/s, mean train loss=0.176]
validate [batch 12]: : 384it [00:03, 127.88it/s, mean loss=0.262, macro f1=0.897]
train [epoch 9 batch 45]: : 1440it [00:24, 58.48it/s, mean train loss=0.13]
validate [batch 12]: : 384it [00:03, 127.73it/s, mean loss=0.263, macro f1=0.886]
train [epoch 10 batch 45]: : 1440it [00:24, 58.57it/s, mean train loss=0.11]
validate [batch 12]: : 384it [00:03, 127.99it/s, mean loss=0.2, macro f1=0.899]
train [epoch 11 batch 45]: : 1440it [00:24, 58.56it/s, mean train loss=0.115]
validate [batch 12]: : 384it [00:03, 125.72it/s, mean loss=0.182, macro f1=0.889]
train [epoch 12 batch 45]: : 1440it [00:24, 58.85it/s, mean train loss=0.0943]
validate [batch 12]: : 384it [00:02, 128.17it/s, mean loss=0.213, macro f1=0.876]
train [epoch 13 batch 45]: : 1440it [00:24, 58.42it/s, mean train loss=0.081]
validate [batch 12]: : 384it [00:02, 128.01it/s, mean loss=0.217, macro f1=0.891]
train [epoch 14 batch 45]: : 1440it [00:24, 58.64it/s, mean train loss=0.067]
validate [batch 12]: : 384it [00:03, 127.61it/s, mean loss=0.178, macro f1=0.888]
train [epoch 15 batch 45]: : 1440it [00:24, 58.61it/s, mean train loss=0.0602]
validate [batch 12]: : 384it [00:03, 125.73it/s, mean loss=0.232, macro f1=0.88]
train [epoch 16 batch 45]: : 1440it [00:24, 59.07it/s, mean train loss=0.0645]
validate [batch 12]: : 384it [00:03, 114.82it/s, mean loss=0.253, macro f1=0.897]
train [epoch 17 batch 45]: : 1440it [00:24, 58.55it/s, mean train loss=0.0456]
validate [batch 12]: : 384it [00:03, 127.57it/s, mean loss=0.236, macro f1=0.9]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Early stopping at epoch 17</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>test [batch 29]: : 928it [00:07, 128.53it/s, mean loss=0.236, macro f1=0.909]
[I 2024-10-19 20:47:03,893] Trial 1 finished with value: 0.875893723287808 and parameters: {'batch_size': 32, 'lr': 0.0009263406719097344, 'weight_decay': 0.0007103605819788694, 'reduce_lr_factor': 0.017841636973138664, 'momentum': 0.8038414955136619, 'early_stop_patience': 3, 'jitter': False, 'horizontal_flip': True, 'rotation': False}. Best is trial 1 with value: 0.875893723287808.</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 2 params: {'batch_size': 24, 'lr': 0.0006435218111142486, 'weight_decay': 0.001433532874090464, 'reduce_lr_factor': 0.09502020253446256, 'momentum': 0.8991511811325137, 'early_stop_patience': 2, 'jitter': False, 'horizontal_flip': False, 'rotation': False}
Trial 2 fold 1</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 60]: : 1440it [00:26, 53.77it/s, mean train loss=1.41]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 111.16it/s, mean loss=0.803, macro f1=0.616]
train [epoch 2 batch 60]: : 1440it [00:26, 53.62it/s, mean train loss=0.62]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 112.70it/s, mean loss=0.568, macro f1=0.759]
train [epoch 3 batch 60]: : 1440it [00:26, 53.48it/s, mean train loss=0.375]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.53it/s, mean loss=0.495, macro f1=0.765]
train [epoch 4 batch 60]: : 1440it [00:26, 53.33it/s, mean train loss=0.29]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.87it/s, mean loss=0.372, macro f1=0.798]
train [epoch 5 batch 60]: : 1440it [00:26, 53.41it/s, mean train loss=0.204]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.32it/s, mean loss=0.357, macro f1=0.845]
train [epoch 6 batch 60]: : 1440it [00:26, 53.69it/s, mean train loss=0.146]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 104.24it/s, mean loss=0.413, macro f1=0.849]
train [epoch 7 batch 60]: : 1440it [00:26, 53.61it/s, mean train loss=0.144]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.71it/s, mean loss=0.358, macro f1=0.856]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Early stopping at epoch 7</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>test [batch 38]: : 912it [00:08, 113.67it/s, mean loss=0.251, macro f1=0.9]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 2 fold 2</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 60]: : 1440it [00:26, 53.87it/s, mean train loss=1.35]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 104.45it/s, mean loss=0.646, macro f1=0.691]
train [epoch 2 batch 60]: : 1440it [00:26, 53.51it/s, mean train loss=0.528]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 111.30it/s, mean loss=0.354, macro f1=0.863]
train [epoch 3 batch 60]: : 1440it [00:27, 53.24it/s, mean train loss=0.258]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 111.33it/s, mean loss=0.263, macro f1=0.88]
train [epoch 4 batch 60]: : 1440it [00:27, 53.21it/s, mean train loss=0.156]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.70it/s, mean loss=0.206, macro f1=0.913]
train [epoch 5 batch 60]: : 1440it [00:26, 53.35it/s, mean train loss=0.129]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.38it/s, mean loss=0.241, macro f1=0.898]
train [epoch 6 batch 60]: : 1440it [00:27, 53.21it/s, mean train loss=0.0876]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.28it/s, mean loss=0.3, macro f1=0.899]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Early stopping at epoch 6</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>test [batch 38]: : 912it [00:07, 119.76it/s, mean loss=0.306, macro f1=0.884]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 2 fold 3</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 60]: : 1440it [00:27, 53.22it/s, mean train loss=1.37]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 112.60it/s, mean loss=0.77, macro f1=0.545]
train [epoch 2 batch 60]: : 1440it [00:26, 53.50it/s, mean train loss=0.502]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 110.95it/s, mean loss=0.496, macro f1=0.738]
train [epoch 3 batch 60]: : 1440it [00:26, 53.66it/s, mean train loss=0.282]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 105.41it/s, mean loss=0.345, macro f1=0.852]
train [epoch 4 batch 60]: : 1440it [00:27, 53.25it/s, mean train loss=0.22]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 107.32it/s, mean loss=0.257, macro f1=0.858]
train [epoch 5 batch 60]: : 1440it [00:26, 53.46it/s, mean train loss=0.101]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 111.30it/s, mean loss=0.438, macro f1=0.804]
train [epoch 6 batch 60]: : 1440it [00:26, 53.34it/s, mean train loss=0.104]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.06it/s, mean loss=0.321, macro f1=0.858]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Early stopping at epoch 6</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>test [batch 38]: : 912it [00:07, 119.63it/s, mean loss=0.252, macro f1=0.9]
[I 2024-10-19 20:57:13,387] Trial 2 finished with value: 0.8944816106117478 and parameters: {'batch_size': 24, 'lr': 0.0006435218111142486, 'weight_decay': 0.001433532874090464, 'reduce_lr_factor': 0.09502020253446256, 'momentum': 0.8991511811325137, 'early_stop_patience': 2, 'jitter': False, 'horizontal_flip': False, 'rotation': False}. Best is trial 2 with value: 0.8944816106117478.</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 3 params: {'batch_size': 32, 'lr': 0.0006207646569060094, 'weight_decay': 0.009437480785146241, 'reduce_lr_factor': 0.0713638269193135, 'momentum': 0.8683065011090194, 'early_stop_patience': 2, 'jitter': True, 'horizontal_flip': False, 'rotation': True}
Trial 3 fold 1</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 45]: : 1440it [00:25, 57.20it/s, mean train loss=1.71]
validate [batch 12]: : 384it [00:02, 129.49it/s, mean loss=1.21, macro f1=0.35]
train [epoch 2 batch 45]: : 1440it [00:25, 56.97it/s, mean train loss=1.01]
validate [batch 12]: : 384it [00:03, 122.05it/s, mean loss=0.673, macro f1=0.666]
train [epoch 3 batch 45]: : 1440it [00:25, 57.45it/s, mean train loss=0.746]
validate [batch 12]: : 384it [00:03, 117.92it/s, mean loss=0.677, macro f1=0.713]
train [epoch 4 batch 45]: : 1440it [00:25, 57.26it/s, mean train loss=0.467]
validate [batch 12]: : 384it [00:02, 129.56it/s, mean loss=0.469, macro f1=0.79]
train [epoch 5 batch 45]: : 1440it [00:25, 56.89it/s, mean train loss=0.37]
validate [batch 12]: : 384it [00:02, 129.32it/s, mean loss=0.365, macro f1=0.802]
train [epoch 6 batch 45]: : 1440it [00:25, 56.73it/s, mean train loss=0.351]
validate [batch 12]: : 384it [00:02, 129.46it/s, mean loss=0.331, macro f1=0.834]
[I 2024-10-19 21:00:07,033] Trial 3 pruned. </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 4 params: {'batch_size': 24, 'lr': 0.0003700736632331964, 'weight_decay': 0.0057019677041787965, 'reduce_lr_factor': 0.049474136211608837, 'momentum': 0.987791029231253, 'early_stop_patience': 1, 'jitter': True, 'horizontal_flip': True, 'rotation': True}
Trial 4 fold 1</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 60]: : 1440it [00:27, 51.71it/s, mean train loss=1.59]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.16it/s, mean loss=1.61, macro f1=0.435]
train [epoch 2 batch 60]: : 1440it [00:27, 51.79it/s, mean train loss=0.938]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 107.55it/s, mean loss=0.667, macro f1=0.705]
train [epoch 3 batch 60]: : 1440it [00:27, 51.95it/s, mean train loss=0.568]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 109.71it/s, mean loss=0.513, macro f1=0.747]
train [epoch 4 batch 60]: : 1440it [00:27, 51.83it/s, mean train loss=0.398]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 112.44it/s, mean loss=0.501, macro f1=0.811]
train [epoch 5 batch 60]: : 1440it [00:27, 51.46it/s, mean train loss=0.355]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.16it/s, mean loss=0.418, macro f1=0.791]
train [epoch 6 batch 60]: : 1440it [00:27, 51.59it/s, mean train loss=0.285]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 112.85it/s, mean loss=0.261, macro f1=0.885]
train [epoch 7 batch 60]: : 1440it [00:27, 51.63it/s, mean train loss=0.184]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 112.90it/s, mean loss=0.238, macro f1=0.896]
train [epoch 8 batch 60]: : 1440it [00:27, 51.67it/s, mean train loss=0.198]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.21it/s, mean loss=0.199, macro f1=0.903]
train [epoch 9 batch 60]: : 1440it [00:27, 51.89it/s, mean train loss=0.156]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 104.40it/s, mean loss=0.253, macro f1=0.913]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Early stopping at epoch 9</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>test [batch 38]: : 912it [00:07, 119.49it/s, mean loss=0.265, macro f1=0.909]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 4 fold 2</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 60]: : 1440it [00:27, 51.54it/s, mean train loss=1.53]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.45it/s, mean loss=0.998, macro f1=0.379]
train [epoch 2 batch 60]: : 1440it [00:27, 51.82it/s, mean train loss=0.928]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 105.14it/s, mean loss=0.548, macro f1=0.734]
train [epoch 3 batch 60]: : 1440it [00:27, 51.82it/s, mean train loss=0.637]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.26it/s, mean loss=0.544, macro f1=0.673]
train [epoch 4 batch 60]: : 1440it [00:27, 51.52it/s, mean train loss=0.481]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 112.23it/s, mean loss=0.421, macro f1=0.822]
train [epoch 5 batch 60]: : 1440it [00:28, 51.30it/s, mean train loss=0.394]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.17it/s, mean loss=0.336, macro f1=0.833]
train [epoch 6 batch 60]: : 1440it [00:27, 51.72it/s, mean train loss=0.336]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.36it/s, mean loss=0.277, macro f1=0.88]
train [epoch 7 batch 60]: : 1440it [00:27, 51.55it/s, mean train loss=0.355]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.04it/s, mean loss=0.284, macro f1=0.88]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Early stopping at epoch 7</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>test [batch 38]: : 912it [00:07, 120.09it/s, mean loss=0.318, macro f1=0.819]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 4 fold 3</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 60]: : 1440it [00:27, 51.83it/s, mean train loss=1.52]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 112.81it/s, mean loss=0.837, macro f1=0.619]
train [epoch 2 batch 60]: : 1440it [00:27, 51.59it/s, mean train loss=0.734]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 112.56it/s, mean loss=0.56, macro f1=0.717]
train [epoch 3 batch 60]: : 1440it [00:27, 51.88it/s, mean train loss=0.56]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 103.18it/s, mean loss=0.439, macro f1=0.798]
train [epoch 4 batch 60]: : 1440it [00:27, 51.87it/s, mean train loss=0.329]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 112.94it/s, mean loss=0.481, macro f1=0.81]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Early stopping at epoch 4</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>test [batch 38]: : 912it [00:07, 121.76it/s, mean loss=0.438, macro f1=0.85]
[I 2024-10-19 21:11:04,546] Trial 4 finished with value: 0.8593910636015544 and parameters: {'batch_size': 24, 'lr': 0.0003700736632331964, 'weight_decay': 0.0057019677041787965, 'reduce_lr_factor': 0.049474136211608837, 'momentum': 0.987791029231253, 'early_stop_patience': 1, 'jitter': True, 'horizontal_flip': True, 'rotation': True}. Best is trial 2 with value: 0.8944816106117478.</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 5 params: {'batch_size': 24, 'lr': 0.00011927138975266209, 'weight_decay': 0.006563295894652734, 'reduce_lr_factor': 0.022436465621375246, 'momentum': 0.8373506487192102, 'early_stop_patience': 2, 'jitter': True, 'horizontal_flip': True, 'rotation': True}
Trial 5 fold 1</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 60]: : 1440it [00:27, 51.70it/s, mean train loss=1.99]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 112.72it/s, mean loss=1.79, macro f1=0.155]
train [epoch 2 batch 60]: : 1440it [00:27, 52.04it/s, mean train loss=1.66]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.30it/s, mean loss=1.59, macro f1=0.151]
train [epoch 3 batch 60]: : 1440it [00:27, 52.04it/s, mean train loss=1.47]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 104.71it/s, mean loss=1.32, macro f1=0.185]
train [epoch 4 batch 60]: : 1440it [00:27, 51.89it/s, mean train loss=1.38]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.18it/s, mean loss=1.23, macro f1=0.228]
train [epoch 5 batch 60]: : 1440it [00:27, 51.92it/s, mean train loss=1.21]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 111.25it/s, mean loss=1.05, macro f1=0.457]
train [epoch 6 batch 60]: : 1440it [00:27, 51.68it/s, mean train loss=1.08]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 112.89it/s, mean loss=0.891, macro f1=0.584]
[I 2024-10-19 21:14:14,601] Trial 5 pruned. </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 6 params: {'batch_size': 32, 'lr': 0.0006087970645475956, 'weight_decay': 0.007392635793983017, 'reduce_lr_factor': 0.01352690130288886, 'momentum': 0.8537333228895179, 'early_stop_patience': 1, 'jitter': True, 'horizontal_flip': False, 'rotation': False}
Trial 6 fold 1</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 45]: : 1440it [00:24, 57.67it/s, mean train loss=1.67]
validate [batch 12]: : 384it [00:03, 127.57it/s, mean loss=1.18, macro f1=0.371]
train [epoch 2 batch 45]: : 1440it [00:25, 57.05it/s, mean train loss=0.899]
validate [batch 12]: : 384it [00:03, 126.63it/s, mean loss=0.607, macro f1=0.694]
train [epoch 3 batch 45]: : 1440it [00:25, 57.58it/s, mean train loss=0.518]
validate [batch 12]: : 384it [00:02, 128.15it/s, mean loss=0.456, macro f1=0.762]
train [epoch 4 batch 45]: : 1440it [00:25, 57.33it/s, mean train loss=0.395]
validate [batch 12]: : 384it [00:03, 126.34it/s, mean loss=0.346, macro f1=0.812]
train [epoch 5 batch 45]: : 1440it [00:25, 57.50it/s, mean train loss=0.268]
validate [batch 12]: : 384it [00:03, 127.33it/s, mean loss=0.318, macro f1=0.846]
train [epoch 6 batch 45]: : 1440it [00:25, 57.29it/s, mean train loss=0.193]
validate [batch 12]: : 384it [00:03, 125.01it/s, mean loss=0.298, macro f1=0.844]
[I 2024-10-19 21:17:07,417] Trial 6 pruned. </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 7 params: {'batch_size': 32, 'lr': 0.00027273559603005097, 'weight_decay': 0.005232480534666997, 'reduce_lr_factor': 0.018454645968259752, 'momentum': 0.9094298341556741, 'early_stop_patience': 3, 'jitter': False, 'horizontal_flip': False, 'rotation': True}
Trial 7 fold 1</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 45]: : 1440it [00:24, 58.91it/s, mean train loss=1.9]
validate [batch 12]: : 384it [00:03, 127.55it/s, mean loss=1.37, macro f1=0.182]
train [epoch 2 batch 45]: : 1440it [00:24, 59.18it/s, mean train loss=1.2]
validate [batch 12]: : 384it [00:03, 126.79it/s, mean loss=0.796, macro f1=0.603]
train [epoch 3 batch 45]: : 1440it [00:24, 58.39it/s, mean train loss=0.794]
validate [batch 12]: : 384it [00:03, 126.50it/s, mean loss=0.597, macro f1=0.665]
train [epoch 4 batch 45]: : 1440it [00:24, 59.03it/s, mean train loss=0.612]
validate [batch 12]: : 384it [00:03, 127.63it/s, mean loss=0.533, macro f1=0.727]
train [epoch 5 batch 45]: : 1440it [00:24, 58.59it/s, mean train loss=0.449]
validate [batch 12]: : 384it [00:03, 124.87it/s, mean loss=0.403, macro f1=0.808]
train [epoch 6 batch 45]: : 1440it [00:24, 59.03it/s, mean train loss=0.375]
validate [batch 12]: : 384it [00:03, 127.36it/s, mean loss=0.344, macro f1=0.794]
[I 2024-10-19 21:19:56,656] Trial 7 pruned. </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 8 params: {'batch_size': 32, 'lr': 2.9906470725618615e-05, 'weight_decay': 0.008289400292173631, 'reduce_lr_factor': 0.010422592857329237, 'momentum': 0.9287851419912837, 'early_stop_patience': 1, 'jitter': False, 'horizontal_flip': False, 'rotation': True}
Trial 8 fold 1</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 45]: : 1440it [00:24, 58.88it/s, mean train loss=2.15]
validate [batch 12]: : 384it [00:03, 127.36it/s, mean loss=1.93, macro f1=0.148]
train [epoch 2 batch 45]: : 1440it [00:24, 58.44it/s, mean train loss=1.84]
validate [batch 12]: : 384it [00:03, 118.95it/s, mean loss=1.68, macro f1=0.173]
train [epoch 3 batch 45]: : 1440it [00:24, 59.31it/s, mean train loss=1.65]
validate [batch 12]: : 384it [00:03, 126.62it/s, mean loss=1.49, macro f1=0.179]
train [epoch 4 batch 45]: : 1440it [00:24, 58.74it/s, mean train loss=1.46]
validate [batch 12]: : 384it [00:03, 127.47it/s, mean loss=1.36, macro f1=0.182]
train [epoch 5 batch 45]: : 1440it [00:24, 58.99it/s, mean train loss=1.36]
validate [batch 12]: : 384it [00:03, 127.77it/s, mean loss=1.23, macro f1=0.188]
train [epoch 6 batch 45]: : 1440it [00:24, 58.89it/s, mean train loss=1.29]
validate [batch 12]: : 384it [00:03, 125.03it/s, mean loss=1.15, macro f1=0.239]
[I 2024-10-19 21:22:45,935] Trial 8 pruned. </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 9 params: {'batch_size': 24, 'lr': 0.0009532215214018151, 'weight_decay': 0.004471253786176274, 'reduce_lr_factor': 0.0861767805224015, 'momentum': 0.9329010623103258, 'early_stop_patience': 1, 'jitter': True, 'horizontal_flip': True, 'rotation': True}
Trial 9 fold 1</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 60]: : 1440it [00:27, 51.82it/s, mean train loss=1.45]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.38it/s, mean loss=0.697, macro f1=0.65]
train [epoch 2 batch 60]: : 1440it [00:27, 51.85it/s, mean train loss=0.714]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 109.31it/s, mean loss=0.493, macro f1=0.772]
train [epoch 3 batch 60]: : 1440it [00:27, 51.75it/s, mean train loss=0.45]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 112.85it/s, mean loss=0.313, macro f1=0.835]
train [epoch 4 batch 60]: : 1440it [00:27, 51.73it/s, mean train loss=0.317]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 112.24it/s, mean loss=0.297, macro f1=0.859]
train [epoch 5 batch 60]: : 1440it [00:28, 51.42it/s, mean train loss=0.216]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 112.08it/s, mean loss=0.324, macro f1=0.845]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Early stopping at epoch 5</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>test [batch 38]: : 912it [00:07, 121.50it/s, mean loss=0.25, macro f1=0.876]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 9 fold 2</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 60]: : 1440it [00:27, 51.85it/s, mean train loss=1.4]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 111.74it/s, mean loss=0.947, macro f1=0.615]
train [epoch 2 batch 60]: : 1440it [00:28, 51.42it/s, mean train loss=0.806]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 112.31it/s, mean loss=0.511, macro f1=0.705]
train [epoch 3 batch 60]: : 1440it [00:27, 51.77it/s, mean train loss=0.435]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 112.96it/s, mean loss=0.312, macro f1=0.847]
train [epoch 4 batch 60]: : 1440it [00:28, 51.28it/s, mean train loss=0.354]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 112.51it/s, mean loss=0.251, macro f1=0.869]
train [epoch 5 batch 60]: : 1440it [00:27, 51.60it/s, mean train loss=0.272]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 112.78it/s, mean loss=0.253, macro f1=0.893]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Early stopping at epoch 5</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>test [batch 38]: : 912it [00:07, 118.57it/s, mean loss=0.267, macro f1=0.873]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 9 fold 3</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 60]: : 1440it [00:28, 51.36it/s, mean train loss=1.27]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.18it/s, mean loss=0.64, macro f1=0.662]
train [epoch 2 batch 60]: : 1440it [00:27, 51.75it/s, mean train loss=0.615]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 112.82it/s, mean loss=0.376, macro f1=0.807]
train [epoch 3 batch 60]: : 1440it [00:28, 51.41it/s, mean train loss=0.331]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.02it/s, mean loss=0.285, macro f1=0.845]
train [epoch 4 batch 60]: : 1440it [00:27, 51.72it/s, mean train loss=0.217]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 111.86it/s, mean loss=0.352, macro f1=0.857]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Early stopping at epoch 4</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>test [batch 38]: : 912it [00:07, 120.99it/s, mean loss=0.236, macro f1=0.911]
[I 2024-10-19 21:30:35,179] Trial 9 finished with value: 0.8864634164154767 and parameters: {'batch_size': 24, 'lr': 0.0009532215214018151, 'weight_decay': 0.004471253786176274, 'reduce_lr_factor': 0.0861767805224015, 'momentum': 0.9329010623103258, 'early_stop_patience': 1, 'jitter': True, 'horizontal_flip': True, 'rotation': True}. Best is trial 2 with value: 0.8944816106117478.</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 10 params: {'batch_size': 24, 'lr': 0.00041926610680704515, 'weight_decay': 0.00020768834766933357, 'reduce_lr_factor': 0.09366763195250474, 'momentum': 0.9731649469005796, 'early_stop_patience': 2, 'jitter': False, 'horizontal_flip': False, 'rotation': False}
Trial 10 fold 1</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 60]: : 1440it [00:27, 53.19it/s, mean train loss=1.33]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.15it/s, mean loss=0.698, macro f1=0.68]
train [epoch 2 batch 60]: : 1440it [00:26, 53.58it/s, mean train loss=0.482]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 112.03it/s, mean loss=0.452, macro f1=0.778]
train [epoch 3 batch 60]: : 1440it [00:27, 53.29it/s, mean train loss=0.313]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 112.12it/s, mean loss=0.499, macro f1=0.791]
train [epoch 4 batch 60]: : 1440it [00:26, 53.64it/s, mean train loss=0.256]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 112.06it/s, mean loss=0.279, macro f1=0.851]
train [epoch 5 batch 60]: : 1440it [00:27, 53.13it/s, mean train loss=0.133]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 112.96it/s, mean loss=0.316, macro f1=0.842]
train [epoch 6 batch 60]: : 1440it [00:26, 53.61it/s, mean train loss=0.102]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 114.01it/s, mean loss=0.291, macro f1=0.874]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Early stopping at epoch 6</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>test [batch 38]: : 912it [00:07, 120.98it/s, mean loss=0.24, macro f1=0.893]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 10 fold 2</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 60]: : 1440it [00:27, 53.08it/s, mean train loss=1.41]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 112.18it/s, mean loss=0.652, macro f1=0.703]
train [epoch 2 batch 60]: : 1440it [00:26, 53.49it/s, mean train loss=0.534]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 112.75it/s, mean loss=0.333, macro f1=0.825]
train [epoch 3 batch 60]: : 1440it [00:27, 53.21it/s, mean train loss=0.28]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.18it/s, mean loss=0.252, macro f1=0.913]
train [epoch 4 batch 60]: : 1440it [00:26, 53.57it/s, mean train loss=0.145]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.08it/s, mean loss=0.187, macro f1=0.918]
train [epoch 5 batch 60]: : 1440it [00:26, 53.46it/s, mean train loss=0.12]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 103.63it/s, mean loss=0.249, macro f1=0.905]
train [epoch 6 batch 60]: : 1440it [00:27, 53.26it/s, mean train loss=0.0894]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.29it/s, mean loss=0.287, macro f1=0.878]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Early stopping at epoch 6</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>test [batch 38]: : 912it [00:07, 121.04it/s, mean loss=0.282, macro f1=0.9]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 10 fold 3</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 60]: : 1440it [00:26, 53.49it/s, mean train loss=1.36]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 109.36it/s, mean loss=0.725, macro f1=0.657]
train [epoch 2 batch 60]: : 1440it [00:26, 53.38it/s, mean train loss=0.505]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.76it/s, mean loss=0.435, macro f1=0.768]
train [epoch 3 batch 60]: : 1440it [00:26, 53.51it/s, mean train loss=0.312]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.48it/s, mean loss=0.482, macro f1=0.776]
train [epoch 4 batch 60]: : 1440it [00:27, 52.99it/s, mean train loss=0.223]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 111.53it/s, mean loss=0.27, macro f1=0.86]
train [epoch 5 batch 60]: : 1440it [00:26, 53.49it/s, mean train loss=0.165]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.27it/s, mean loss=0.3, macro f1=0.854]
train [epoch 6 batch 60]: : 1440it [00:27, 53.20it/s, mean train loss=0.106]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 114.08it/s, mean loss=0.236, macro f1=0.912]
train [epoch 7 batch 60]: : 1440it [00:26, 53.75it/s, mean train loss=0.0983]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.58it/s, mean loss=0.332, macro f1=0.834]
train [epoch 8 batch 60]: : 1440it [00:27, 53.23it/s, mean train loss=0.0587]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.06it/s, mean loss=0.229, macro f1=0.904]
train [epoch 9 batch 60]: : 1440it [00:26, 53.48it/s, mean train loss=0.0529]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.51it/s, mean loss=0.299, macro f1=0.854]
train [epoch 10 batch 60]: : 1440it [00:27, 53.24it/s, mean train loss=0.0761]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 105.10it/s, mean loss=0.258, macro f1=0.889]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Early stopping at epoch 10</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>test [batch 38]: : 912it [00:07, 121.06it/s, mean loss=0.266, macro f1=0.895]
[I 2024-10-19 21:42:15,827] Trial 10 finished with value: 0.8959995960548577 and parameters: {'batch_size': 24, 'lr': 0.00041926610680704515, 'weight_decay': 0.00020768834766933357, 'reduce_lr_factor': 0.09366763195250474, 'momentum': 0.9731649469005796, 'early_stop_patience': 2, 'jitter': False, 'horizontal_flip': False, 'rotation': False}. Best is trial 10 with value: 0.8959995960548577.</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 11 params: {'batch_size': 24, 'lr': 0.0004494679962846208, 'weight_decay': 0.00030386197288080987, 'reduce_lr_factor': 0.09992933757507363, 'momentum': 0.9858812103194864, 'early_stop_patience': 2, 'jitter': False, 'horizontal_flip': False, 'rotation': False}
Trial 11 fold 1</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 60]: : 1440it [00:26, 53.56it/s, mean train loss=1.48]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.25it/s, mean loss=0.85, macro f1=0.554]
train [epoch 2 batch 60]: : 1440it [00:27, 53.13it/s, mean train loss=0.679]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 112.21it/s, mean loss=0.509, macro f1=0.738]
train [epoch 3 batch 60]: : 1440it [00:26, 53.73it/s, mean train loss=0.378]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.07it/s, mean loss=0.456, macro f1=0.748]
train [epoch 4 batch 60]: : 1440it [00:26, 53.65it/s, mean train loss=0.272]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 105.53it/s, mean loss=0.507, macro f1=0.816]
train [epoch 5 batch 60]: : 1440it [00:26, 53.41it/s, mean train loss=0.277]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.49it/s, mean loss=0.368, macro f1=0.859]
train [epoch 6 batch 60]: : 1440it [00:26, 53.65it/s, mean train loss=0.209]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.81it/s, mean loss=0.435, macro f1=0.83]
train [epoch 7 batch 60]: : 1440it [00:27, 53.19it/s, mean train loss=0.153]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.27it/s, mean loss=0.229, macro f1=0.904]
train [epoch 8 batch 60]: : 1440it [00:26, 53.83it/s, mean train loss=0.0816]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 114.07it/s, mean loss=0.445, macro f1=0.86]
train [epoch 9 batch 60]: : 1440it [00:27, 53.30it/s, mean train loss=0.0898]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.63it/s, mean loss=0.4, macro f1=0.84]
[I 2024-10-19 21:46:51,882] Trial 11 pruned. </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 12 params: {'batch_size': 24, 'lr': 0.0007418366656250753, 'weight_decay': 0.0023652436805426938, 'reduce_lr_factor': 0.09810883725002961, 'momentum': 0.9482279084641699, 'early_stop_patience': 3, 'jitter': False, 'horizontal_flip': False, 'rotation': False}
Trial 12 fold 1</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 60]: : 1440it [00:26, 53.69it/s, mean train loss=1.3]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 112.68it/s, mean loss=0.777, macro f1=0.664]
train [epoch 2 batch 60]: : 1440it [00:27, 53.06it/s, mean train loss=0.474]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.52it/s, mean loss=0.46, macro f1=0.784]
train [epoch 3 batch 60]: : 1440it [00:26, 53.57it/s, mean train loss=0.371]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 112.73it/s, mean loss=0.352, macro f1=0.818]
train [epoch 4 batch 60]: : 1440it [00:26, 53.45it/s, mean train loss=0.238]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 103.10it/s, mean loss=0.247, macro f1=0.861]
train [epoch 5 batch 60]: : 1440it [00:27, 53.32it/s, mean train loss=0.195]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 110.55it/s, mean loss=0.341, macro f1=0.872]
train [epoch 6 batch 60]: : 1440it [00:26, 53.60it/s, mean train loss=0.112]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 106.21it/s, mean loss=0.357, macro f1=0.826]
train [epoch 7 batch 60]: : 1440it [00:27, 53.07it/s, mean train loss=0.108]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 110.31it/s, mean loss=0.293, macro f1=0.869]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Early stopping at epoch 7</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>test [batch 38]: : 912it [00:07, 121.49it/s, mean loss=0.198, macro f1=0.914]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 12 fold 2</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 60]: : 1440it [00:26, 53.67it/s, mean train loss=1.39]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.13it/s, mean loss=0.715, macro f1=0.524]
train [epoch 2 batch 60]: : 1440it [00:27, 53.21it/s, mean train loss=0.532]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.98it/s, mean loss=0.346, macro f1=0.818]
train [epoch 3 batch 60]: : 1440it [00:26, 53.69it/s, mean train loss=0.311]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.53it/s, mean loss=0.342, macro f1=0.852]
train [epoch 4 batch 60]: : 1440it [00:26, 53.37it/s, mean train loss=0.199]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.62it/s, mean loss=0.314, macro f1=0.869]
train [epoch 5 batch 60]: : 1440it [00:26, 53.68it/s, mean train loss=0.162]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.10it/s, mean loss=0.308, macro f1=0.893]
train [epoch 6 batch 60]: : 1440it [00:27, 53.06it/s, mean train loss=0.102]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.25it/s, mean loss=0.349, macro f1=0.853]
train [epoch 7 batch 60]: : 1440it [00:26, 53.70it/s, mean train loss=0.115]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 108.52it/s, mean loss=0.219, macro f1=0.904]
train [epoch 8 batch 60]: : 1440it [00:26, 53.36it/s, mean train loss=0.0609]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 103.65it/s, mean loss=0.294, macro f1=0.905]
train [epoch 9 batch 60]: : 1440it [00:26, 53.59it/s, mean train loss=0.0488]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.37it/s, mean loss=0.212, macro f1=0.916]
train [epoch 10 batch 60]: : 1440it [00:26, 53.63it/s, mean train loss=0.0274]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.34it/s, mean loss=0.266, macro f1=0.917]
train [epoch 11 batch 60]: : 1440it [00:27, 53.04it/s, mean train loss=0.0496]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.75it/s, mean loss=0.193, macro f1=0.909]
train [epoch 12 batch 60]: : 1440it [00:26, 53.56it/s, mean train loss=0.029]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.12it/s, mean loss=0.233, macro f1=0.911]
train [epoch 13 batch 60]: : 1440it [00:27, 53.30it/s, mean train loss=0.0213]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.68it/s, mean loss=0.266, macro f1=0.919]
train [epoch 14 batch 60]: : 1440it [00:26, 53.61it/s, mean train loss=0.0234]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.91it/s, mean loss=0.258, macro f1=0.916]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Early stopping at epoch 14</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>test [batch 38]: : 912it [00:07, 121.08it/s, mean loss=0.254, macro f1=0.898]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 12 fold 3</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 60]: : 1440it [00:26, 53.49it/s, mean train loss=1.5]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 109.48it/s, mean loss=0.97, macro f1=0.418]
train [epoch 2 batch 60]: : 1440it [00:26, 53.83it/s, mean train loss=0.639]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.73it/s, mean loss=0.467, macro f1=0.762]
train [epoch 3 batch 60]: : 1440it [00:26, 53.53it/s, mean train loss=0.256]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 114.00it/s, mean loss=0.309, macro f1=0.852]
train [epoch 4 batch 60]: : 1440it [00:26, 53.65it/s, mean train loss=0.207]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 112.56it/s, mean loss=0.337, macro f1=0.839]
train [epoch 5 batch 60]: : 1440it [00:26, 53.64it/s, mean train loss=0.169]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 104.44it/s, mean loss=0.32, macro f1=0.883]
train [epoch 6 batch 60]: : 1440it [00:26, 53.47it/s, mean train loss=0.13]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.20it/s, mean loss=0.291, macro f1=0.869]
train [epoch 7 batch 60]: : 1440it [00:26, 53.62it/s, mean train loss=0.0973]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.73it/s, mean loss=0.273, macro f1=0.887]
train [epoch 8 batch 60]: : 1440it [00:27, 53.15it/s, mean train loss=0.126]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.83it/s, mean loss=0.322, macro f1=0.868]
train [epoch 9 batch 60]: : 1440it [00:26, 53.60it/s, mean train loss=0.0528]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.61it/s, mean loss=0.357, macro f1=0.888]
train [epoch 10 batch 60]: : 1440it [00:27, 53.07it/s, mean train loss=0.0528]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.99it/s, mean loss=0.449, macro f1=0.835]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Early stopping at epoch 10</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>test [batch 38]: : 912it [00:07, 121.53it/s, mean loss=0.333, macro f1=0.868]
[I 2024-10-19 22:03:05,590] Trial 12 finished with value: 0.8935955507453679 and parameters: {'batch_size': 24, 'lr': 0.0007418366656250753, 'weight_decay': 0.0023652436805426938, 'reduce_lr_factor': 0.09810883725002961, 'momentum': 0.9482279084641699, 'early_stop_patience': 3, 'jitter': False, 'horizontal_flip': False, 'rotation': False}. Best is trial 10 with value: 0.8959995960548577.</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 13 params: {'batch_size': 24, 'lr': 0.00025703928328789605, 'weight_decay': 0.002615828504402075, 'reduce_lr_factor': 0.08147142907323117, 'momentum': 0.9585454689948866, 'early_stop_patience': 2, 'jitter': False, 'horizontal_flip': False, 'rotation': False}
Trial 13 fold 1</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 60]: : 1440it [00:26, 53.69it/s, mean train loss=1.51]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.65it/s, mean loss=0.763, macro f1=0.62]
train [epoch 2 batch 60]: : 1440it [00:26, 54.14it/s, mean train loss=0.587]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 114.16it/s, mean loss=0.523, macro f1=0.751]
train [epoch 3 batch 60]: : 1440it [00:26, 54.38it/s, mean train loss=0.341]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 114.06it/s, mean loss=0.297, macro f1=0.843]
train [epoch 4 batch 60]: : 1440it [00:27, 53.32it/s, mean train loss=0.175]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 108.96it/s, mean loss=0.284, macro f1=0.878]
train [epoch 5 batch 60]: : 1440it [00:26, 53.49it/s, mean train loss=0.112]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.54it/s, mean loss=0.263, macro f1=0.888]
train [epoch 6 batch 60]: : 1440it [00:26, 53.59it/s, mean train loss=0.113]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 112.17it/s, mean loss=0.221, macro f1=0.897]
train [epoch 7 batch 60]: : 1440it [00:26, 53.45it/s, mean train loss=0.0782]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.59it/s, mean loss=0.217, macro f1=0.901]
train [epoch 8 batch 60]: : 1440it [00:26, 53.57it/s, mean train loss=0.0506]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.19it/s, mean loss=0.229, macro f1=0.917]
train [epoch 9 batch 60]: : 1440it [00:27, 53.13it/s, mean train loss=0.0375]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.96it/s, mean loss=0.247, macro f1=0.909]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Early stopping at epoch 9</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>test [batch 38]: : 912it [00:07, 121.69it/s, mean loss=0.215, macro f1=0.919]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 13 fold 2</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 60]: : 1440it [00:26, 54.50it/s, mean train loss=1.57]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 114.01it/s, mean loss=0.841, macro f1=0.598]
train [epoch 2 batch 60]: : 1440it [00:27, 53.12it/s, mean train loss=0.528]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 114.46it/s, mean loss=0.4, macro f1=0.786]
train [epoch 3 batch 60]: : 1440it [00:26, 53.73it/s, mean train loss=0.292]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 114.31it/s, mean loss=0.315, macro f1=0.863]
train [epoch 4 batch 60]: : 1440it [00:27, 53.22it/s, mean train loss=0.202]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 114.71it/s, mean loss=0.231, macro f1=0.884]
train [epoch 5 batch 60]: : 1440it [00:26, 53.64it/s, mean train loss=0.157]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 114.29it/s, mean loss=0.262, macro f1=0.887]
train [epoch 6 batch 60]: : 1440it [00:26, 53.50it/s, mean train loss=0.11]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 106.45it/s, mean loss=0.255, macro f1=0.883]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Early stopping at epoch 6</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>test [batch 38]: : 912it [00:07, 120.55it/s, mean loss=0.255, macro f1=0.887]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 13 fold 3</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 60]: : 1440it [00:26, 53.83it/s, mean train loss=1.67]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.99it/s, mean loss=1.11, macro f1=0.298]
train [epoch 2 batch 60]: : 1440it [00:26, 53.74it/s, mean train loss=0.715]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 111.79it/s, mean loss=0.588, macro f1=0.719]
train [epoch 3 batch 60]: : 1440it [00:26, 53.70it/s, mean train loss=0.396]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 114.60it/s, mean loss=0.407, macro f1=0.816]
train [epoch 4 batch 60]: : 1440it [00:26, 53.80it/s, mean train loss=0.264]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 114.36it/s, mean loss=0.326, macro f1=0.838]
train [epoch 5 batch 60]: : 1440it [00:26, 53.85it/s, mean train loss=0.17]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 114.08it/s, mean loss=0.367, macro f1=0.805]
train [epoch 6 batch 60]: : 1440it [00:26, 53.74it/s, mean train loss=0.174]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 114.22it/s, mean loss=0.246, macro f1=0.876]
train [epoch 7 batch 60]: : 1440it [00:27, 53.13it/s, mean train loss=0.162]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 114.25it/s, mean loss=0.314, macro f1=0.854]
train [epoch 8 batch 60]: : 1440it [00:26, 53.66it/s, mean train loss=0.0956]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.12it/s, mean loss=0.251, macro f1=0.895]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Early stopping at epoch 8</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>test [batch 38]: : 912it [00:07, 121.24it/s, mean loss=0.252, macro f1=0.884]
[I 2024-10-19 22:15:12,265] Trial 13 finished with value: 0.8967164091113543 and parameters: {'batch_size': 24, 'lr': 0.00025703928328789605, 'weight_decay': 0.002615828504402075, 'reduce_lr_factor': 0.08147142907323117, 'momentum': 0.9585454689948866, 'early_stop_patience': 2, 'jitter': False, 'horizontal_flip': False, 'rotation': False}. Best is trial 13 with value: 0.8967164091113543.</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 14 params: {'batch_size': 24, 'lr': 0.00024174775803100938, 'weight_decay': 0.0035305699906768183, 'reduce_lr_factor': 0.08054162794179773, 'momentum': 0.9597674671303823, 'early_stop_patience': 2, 'jitter': False, 'horizontal_flip': False, 'rotation': False}
Trial 14 fold 1</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 60]: : 1440it [00:26, 53.52it/s, mean train loss=1.58]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 114.20it/s, mean loss=0.932, macro f1=0.486]
train [epoch 2 batch 60]: : 1440it [00:26, 53.67it/s, mean train loss=0.652]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.34it/s, mean loss=0.531, macro f1=0.713]
train [epoch 3 batch 60]: : 1440it [00:26, 53.58it/s, mean train loss=0.339]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 107.34it/s, mean loss=0.43, macro f1=0.77]
train [epoch 4 batch 60]: : 1440it [00:26, 53.71it/s, mean train loss=0.253]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.48it/s, mean loss=0.423, macro f1=0.828]
train [epoch 5 batch 60]: : 1440it [00:26, 53.57it/s, mean train loss=0.25]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 108.88it/s, mean loss=0.302, macro f1=0.873]
train [epoch 6 batch 60]: : 1440it [00:27, 53.17it/s, mean train loss=0.153]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 114.28it/s, mean loss=0.319, macro f1=0.84]
train [epoch 7 batch 60]: : 1440it [00:26, 53.63it/s, mean train loss=0.146]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 114.08it/s, mean loss=0.314, macro f1=0.868]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Early stopping at epoch 7</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>test [batch 38]: : 912it [00:07, 122.50it/s, mean loss=0.254, macro f1=0.885]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 14 fold 2</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 60]: : 1440it [00:27, 53.27it/s, mean train loss=1.5]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.79it/s, mean loss=0.843, macro f1=0.444]
train [epoch 2 batch 60]: : 1440it [00:26, 53.67it/s, mean train loss=0.637]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.80it/s, mean loss=0.443, macro f1=0.787]
train [epoch 3 batch 60]: : 1440it [00:26, 53.35it/s, mean train loss=0.349]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 114.15it/s, mean loss=0.317, macro f1=0.864]
train [epoch 4 batch 60]: : 1440it [00:26, 53.80it/s, mean train loss=0.244]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 114.10it/s, mean loss=0.251, macro f1=0.877]
train [epoch 5 batch 60]: : 1440it [00:27, 52.93it/s, mean train loss=0.173]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 114.22it/s, mean loss=0.232, macro f1=0.881]
[I 2024-10-19 22:21:29,022] Trial 14 pruned. </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 15 params: {'batch_size': 24, 'lr': 0.00030194290448022913, 'weight_decay': 0.0028312494041207183, 'reduce_lr_factor': 0.06953388696278257, 'momentum': 0.9615861031772759, 'early_stop_patience': 2, 'jitter': False, 'horizontal_flip': False, 'rotation': False}
Trial 15 fold 1</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 60]: : 1440it [00:26, 54.52it/s, mean train loss=1.57]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 116.28it/s, mean loss=1.05, macro f1=0.356]
train [epoch 2 batch 60]: : 1440it [00:26, 54.24it/s, mean train loss=0.713]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 111.04it/s, mean loss=0.554, macro f1=0.701]
train [epoch 3 batch 60]: : 1440it [00:26, 53.50it/s, mean train loss=0.382]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.71it/s, mean loss=0.388, macro f1=0.826]
train [epoch 4 batch 60]: : 1440it [00:26, 53.61it/s, mean train loss=0.254]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.33it/s, mean loss=0.313, macro f1=0.863]
train [epoch 5 batch 60]: : 1440it [00:27, 53.02it/s, mean train loss=0.13]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 112.40it/s, mean loss=0.292, macro f1=0.873]
train [epoch 6 batch 60]: : 1440it [00:26, 53.63it/s, mean train loss=0.0776]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 112.51it/s, mean loss=0.401, macro f1=0.844]
train [epoch 7 batch 60]: : 1440it [00:27, 53.10it/s, mean train loss=0.0907]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 112.79it/s, mean loss=0.306, macro f1=0.876]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Early stopping at epoch 7</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>test [batch 38]: : 912it [00:07, 122.54it/s, mean loss=0.284, macro f1=0.878]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 15 fold 2</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 60]: : 1440it [00:26, 53.82it/s, mean train loss=1.45]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.51it/s, mean loss=0.708, macro f1=0.638]
train [epoch 2 batch 60]: : 1440it [00:27, 53.09it/s, mean train loss=0.588]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.81it/s, mean loss=0.395, macro f1=0.798]
train [epoch 3 batch 60]: : 1440it [00:26, 53.73it/s, mean train loss=0.301]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.80it/s, mean loss=0.287, macro f1=0.867]
train [epoch 4 batch 60]: : 1440it [00:26, 53.60it/s, mean train loss=0.157]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 106.94it/s, mean loss=0.249, macro f1=0.888]
train [epoch 5 batch 60]: : 1440it [00:26, 53.53it/s, mean train loss=0.124]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.87it/s, mean loss=0.232, macro f1=0.9]
train [epoch 6 batch 60]: : 1440it [00:26, 53.60it/s, mean train loss=0.103]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.82it/s, mean loss=0.262, macro f1=0.902]
train [epoch 7 batch 60]: : 1440it [00:27, 53.16it/s, mean train loss=0.0531]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.96it/s, mean loss=0.296, macro f1=0.893]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Early stopping at epoch 7</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>test [batch 38]: : 912it [00:07, 121.91it/s, mean loss=0.294, macro f1=0.894]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 15 fold 3</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 60]: : 1440it [00:26, 53.57it/s, mean train loss=1.51]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.51it/s, mean loss=0.897, macro f1=0.538]
train [epoch 2 batch 60]: : 1440it [00:27, 53.16it/s, mean train loss=0.554]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 114.02it/s, mean loss=0.559, macro f1=0.77]
train [epoch 3 batch 60]: : 1440it [00:26, 53.72it/s, mean train loss=0.363]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.89it/s, mean loss=0.392, macro f1=0.789]
train [epoch 4 batch 60]: : 1440it [00:27, 53.09it/s, mean train loss=0.271]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.97it/s, mean loss=0.374, macro f1=0.844]
train [epoch 5 batch 60]: : 1440it [00:26, 53.66it/s, mean train loss=0.198]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.92it/s, mean loss=0.28, macro f1=0.833]
train [epoch 6 batch 60]: : 1440it [00:27, 53.28it/s, mean train loss=0.123]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 106.14it/s, mean loss=0.237, macro f1=0.886]
train [epoch 7 batch 60]: : 1440it [00:26, 53.52it/s, mean train loss=0.116]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.48it/s, mean loss=0.328, macro f1=0.868]
train [epoch 8 batch 60]: : 1440it [00:26, 53.50it/s, mean train loss=0.0568]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 110.28it/s, mean loss=0.242, macro f1=0.893]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Early stopping at epoch 8</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>test [batch 38]: : 912it [00:08, 110.28it/s, mean loss=0.246, macro f1=0.9]
[I 2024-10-19 22:33:07,675] Trial 15 finished with value: 0.8909237380561086 and parameters: {'batch_size': 24, 'lr': 0.00030194290448022913, 'weight_decay': 0.0028312494041207183, 'reduce_lr_factor': 0.06953388696278257, 'momentum': 0.9615861031772759, 'early_stop_patience': 2, 'jitter': False, 'horizontal_flip': False, 'rotation': False}. Best is trial 13 with value: 0.8967164091113543.</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 16 params: {'batch_size': 24, 'lr': 0.0004819093288876784, 'weight_decay': 0.0016937287816857739, 'reduce_lr_factor': 0.04514422541734965, 'momentum': 0.9669468784630878, 'early_stop_patience': 3, 'jitter': False, 'horizontal_flip': False, 'rotation': False}
Trial 16 fold 1</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 60]: : 1440it [00:26, 53.66it/s, mean train loss=1.26]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.84it/s, mean loss=0.724, macro f1=0.715]
train [epoch 2 batch 60]: : 1440it [00:26, 53.70it/s, mean train loss=0.458]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.53it/s, mean loss=0.421, macro f1=0.805]
train [epoch 3 batch 60]: : 1440it [00:27, 53.13it/s, mean train loss=0.247]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.76it/s, mean loss=0.377, macro f1=0.829]
train [epoch 4 batch 60]: : 1440it [00:26, 53.79it/s, mean train loss=0.218]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.15it/s, mean loss=0.296, macro f1=0.826]
train [epoch 5 batch 60]: : 1440it [00:27, 53.10it/s, mean train loss=0.116]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.74it/s, mean loss=0.334, macro f1=0.871]
train [epoch 6 batch 60]: : 1440it [00:26, 53.60it/s, mean train loss=0.0912]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.65it/s, mean loss=0.241, macro f1=0.907]
train [epoch 7 batch 60]: : 1440it [00:27, 53.11it/s, mean train loss=0.0676]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 110.20it/s, mean loss=0.215, macro f1=0.925]
train [epoch 8 batch 60]: : 1440it [00:26, 53.63it/s, mean train loss=0.0709]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 112.95it/s, mean loss=0.273, macro f1=0.911]
train [epoch 9 batch 60]: : 1440it [00:26, 53.71it/s, mean train loss=0.0472]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 108.82it/s, mean loss=0.264, macro f1=0.892]
train [epoch 10 batch 60]: : 1440it [00:27, 53.25it/s, mean train loss=0.0275]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 112.75it/s, mean loss=0.235, macro f1=0.914]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Early stopping at epoch 10</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>test [batch 38]: : 912it [00:07, 122.33it/s, mean loss=0.2, macro f1=0.929]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 16 fold 2</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 60]: : 1440it [00:26, 53.77it/s, mean train loss=1.35]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 108.04it/s, mean loss=0.636, macro f1=0.719]
train [epoch 2 batch 60]: : 1440it [00:27, 53.06it/s, mean train loss=0.466]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.40it/s, mean loss=0.333, macro f1=0.85]
train [epoch 3 batch 60]: : 1440it [00:26, 53.64it/s, mean train loss=0.224]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.48it/s, mean loss=0.238, macro f1=0.894]
train [epoch 4 batch 60]: : 1440it [00:27, 53.02it/s, mean train loss=0.151]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 114.07it/s, mean loss=0.278, macro f1=0.877]
train [epoch 5 batch 60]: : 1440it [00:26, 53.68it/s, mean train loss=0.0919]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.54it/s, mean loss=0.224, macro f1=0.923]
train [epoch 6 batch 60]: : 1440it [00:27, 52.97it/s, mean train loss=0.0647]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.93it/s, mean loss=0.247, macro f1=0.914]
train [epoch 7 batch 60]: : 1440it [00:26, 53.63it/s, mean train loss=0.0508]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.67it/s, mean loss=0.257, macro f1=0.924]
train [epoch 8 batch 60]: : 1440it [00:26, 53.51it/s, mean train loss=0.0339]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 110.29it/s, mean loss=0.391, macro f1=0.882]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Early stopping at epoch 8</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>test [batch 38]: : 912it [00:07, 117.56it/s, mean loss=0.331, macro f1=0.903]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 16 fold 3</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 60]: : 1440it [00:26, 53.52it/s, mean train loss=1.41]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.92it/s, mean loss=0.778, macro f1=0.657]
train [epoch 2 batch 60]: : 1440it [00:26, 53.57it/s, mean train loss=0.554]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 110.46it/s, mean loss=0.426, macro f1=0.818]
train [epoch 3 batch 60]: : 1440it [00:26, 53.36it/s, mean train loss=0.302]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.53it/s, mean loss=0.368, macro f1=0.809]
train [epoch 4 batch 60]: : 1440it [00:26, 53.58it/s, mean train loss=0.231]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.89it/s, mean loss=0.383, macro f1=0.819]
train [epoch 5 batch 60]: : 1440it [00:27, 53.10it/s, mean train loss=0.141]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.45it/s, mean loss=0.253, macro f1=0.87]
train [epoch 6 batch 60]: : 1440it [00:26, 53.55it/s, mean train loss=0.124]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.39it/s, mean loss=0.241, macro f1=0.871]
train [epoch 7 batch 60]: : 1440it [00:27, 53.19it/s, mean train loss=0.0649]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 111.22it/s, mean loss=0.258, macro f1=0.901]
train [epoch 8 batch 60]: : 1440it [00:26, 53.54it/s, mean train loss=0.0685]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.85it/s, mean loss=0.211, macro f1=0.893]
train [epoch 9 batch 60]: : 1440it [00:26, 53.61it/s, mean train loss=0.0727]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 111.26it/s, mean loss=0.241, macro f1=0.906]
train [epoch 10 batch 60]: : 1440it [00:27, 53.15it/s, mean train loss=0.0456]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.72it/s, mean loss=0.207, macro f1=0.924]
train [epoch 11 batch 60]: : 1440it [00:26, 53.55it/s, mean train loss=0.0389]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.69it/s, mean loss=0.295, macro f1=0.909]
train [epoch 12 batch 60]: : 1440it [00:27, 53.26it/s, mean train loss=0.0496]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.39it/s, mean loss=0.302, macro f1=0.862]
train [epoch 13 batch 60]: : 1440it [00:26, 53.60it/s, mean train loss=0.0379]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 112.54it/s, mean loss=0.244, macro f1=0.913]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Early stopping at epoch 13</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>test [batch 38]: : 912it [00:07, 122.34it/s, mean loss=0.239, macro f1=0.917]
[I 2024-10-19 22:49:20,884] Trial 16 finished with value: 0.9163844390183554 and parameters: {'batch_size': 24, 'lr': 0.0004819093288876784, 'weight_decay': 0.0016937287816857739, 'reduce_lr_factor': 0.04514422541734965, 'momentum': 0.9669468784630878, 'early_stop_patience': 3, 'jitter': False, 'horizontal_flip': False, 'rotation': False}. Best is trial 16 with value: 0.9163844390183554.</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 17 params: {'batch_size': 24, 'lr': 0.0001668437153729059, 'weight_decay': 0.0020084306972898095, 'reduce_lr_factor': 0.040683683734386994, 'momentum': 0.9236127660399246, 'early_stop_patience': 3, 'jitter': False, 'horizontal_flip': False, 'rotation': False}
Trial 17 fold 1</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 60]: : 1440it [00:27, 53.13it/s, mean train loss=1.82]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.60it/s, mean loss=1.34, macro f1=0.177]
train [epoch 2 batch 60]: : 1440it [00:26, 54.48it/s, mean train loss=1.06]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 112.23it/s, mean loss=0.815, macro f1=0.598]
train [epoch 3 batch 60]: : 1440it [00:26, 54.27it/s, mean train loss=0.664]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 105.24it/s, mean loss=0.593, macro f1=0.752]
train [epoch 4 batch 60]: : 1440it [00:26, 54.36it/s, mean train loss=0.503]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.35it/s, mean loss=0.555, macro f1=0.769]
train [epoch 5 batch 60]: : 1440it [00:26, 54.34it/s, mean train loss=0.435]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.95it/s, mean loss=0.45, macro f1=0.803]
train [epoch 6 batch 60]: : 1440it [00:27, 53.15it/s, mean train loss=0.335]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.46it/s, mean loss=0.467, macro f1=0.741]
[I 2024-10-19 22:52:24,414] Trial 17 pruned. </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 18 params: {'batch_size': 24, 'lr': 0.0005649250961480911, 'weight_decay': 0.0038202728999250303, 'reduce_lr_factor': 0.031075387954819105, 'momentum': 0.9451414712739943, 'early_stop_patience': 3, 'jitter': False, 'horizontal_flip': False, 'rotation': False}
Trial 18 fold 1</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 60]: : 1440it [00:26, 54.32it/s, mean train loss=1.33]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 112.72it/s, mean loss=0.736, macro f1=0.618]
train [epoch 2 batch 60]: : 1440it [00:26, 53.78it/s, mean train loss=0.463]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 114.01it/s, mean loss=0.456, macro f1=0.804]
train [epoch 3 batch 60]: : 1440it [00:26, 53.66it/s, mean train loss=0.221]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.70it/s, mean loss=0.428, macro f1=0.79]
train [epoch 4 batch 60]: : 1440it [00:26, 54.17it/s, mean train loss=0.24]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 104.79it/s, mean loss=0.373, macro f1=0.845]
train [epoch 5 batch 60]: : 1440it [00:26, 54.19it/s, mean train loss=0.133]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 114.35it/s, mean loss=0.285, macro f1=0.833]
train [epoch 6 batch 60]: : 1440it [00:26, 53.72it/s, mean train loss=0.108]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 110.13it/s, mean loss=0.3, macro f1=0.831]
[I 2024-10-19 22:55:27,942] Trial 18 pruned. </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 19 params: {'batch_size': 24, 'lr': 0.0004961347267509213, 'weight_decay': 0.0014024498900143132, 'reduce_lr_factor': 0.059636330343116425, 'momentum': 0.9676079987515462, 'early_stop_patience': 3, 'jitter': False, 'horizontal_flip': False, 'rotation': False}
Trial 19 fold 1</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 60]: : 1440it [00:27, 52.77it/s, mean train loss=1.26]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.24it/s, mean loss=0.589, macro f1=0.717]
train [epoch 2 batch 60]: : 1440it [00:27, 53.30it/s, mean train loss=0.368]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 111.14it/s, mean loss=0.381, macro f1=0.795]
train [epoch 3 batch 60]: : 1440it [00:27, 52.99it/s, mean train loss=0.256]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 111.01it/s, mean loss=0.441, macro f1=0.828]
train [epoch 4 batch 60]: : 1440it [00:26, 53.54it/s, mean train loss=0.133]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 106.53it/s, mean loss=0.291, macro f1=0.881]
train [epoch 5 batch 60]: : 1440it [00:27, 53.24it/s, mean train loss=0.0917]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 104.57it/s, mean loss=0.246, macro f1=0.902]
train [epoch 6 batch 60]: : 1440it [00:26, 53.48it/s, mean train loss=0.0766]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 112.71it/s, mean loss=0.249, macro f1=0.898]
train [epoch 7 batch 60]: : 1440it [00:26, 53.58it/s, mean train loss=0.0468]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 112.10it/s, mean loss=0.331, macro f1=0.901]
train [epoch 8 batch 60]: : 1440it [00:27, 53.05it/s, mean train loss=0.0608]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.42it/s, mean loss=0.267, macro f1=0.879]
[I 2024-10-19 22:59:35,095] Trial 19 pruned. </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 20 params: {'batch_size': 24, 'lr': 0.00035411316315068196, 'weight_decay': 0.0033040227384895787, 'reduce_lr_factor': 0.04748691266855614, 'momentum': 0.9131494575049925, 'early_stop_patience': 3, 'jitter': False, 'horizontal_flip': False, 'rotation': False}
Trial 20 fold 1</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 60]: : 1440it [00:26, 53.66it/s, mean train loss=1.67]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.19it/s, mean loss=1.09, macro f1=0.343]
train [epoch 2 batch 60]: : 1440it [00:27, 52.98it/s, mean train loss=0.858]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.60it/s, mean loss=0.65, macro f1=0.641]
train [epoch 3 batch 60]: : 1440it [00:26, 53.60it/s, mean train loss=0.505]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.26it/s, mean loss=0.593, macro f1=0.607]
train [epoch 4 batch 60]: : 1440it [00:27, 53.22it/s, mean train loss=0.338]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 105.05it/s, mean loss=0.336, macro f1=0.825]
train [epoch 5 batch 60]: : 1440it [00:26, 53.62it/s, mean train loss=0.223]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.45it/s, mean loss=0.337, macro f1=0.838]
train [epoch 6 batch 60]: : 1440it [00:26, 53.70it/s, mean train loss=0.154]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.92it/s, mean loss=0.266, macro f1=0.857]
train [epoch 7 batch 60]: : 1440it [00:27, 52.86it/s, mean train loss=0.159]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.02it/s, mean loss=0.31, macro f1=0.849]
[I 2024-10-19 23:03:10,819] Trial 20 pruned. </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 21 params: {'batch_size': 24, 'lr': 0.0004336370725904691, 'weight_decay': 0.0004066475847682252, 'reduce_lr_factor': 0.08463519964480161, 'momentum': 0.9762195658502444, 'early_stop_patience': 2, 'jitter': False, 'horizontal_flip': False, 'rotation': False}
Trial 21 fold 1</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 60]: : 1440it [00:26, 53.71it/s, mean train loss=1.47]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.58it/s, mean loss=0.778, macro f1=0.548]
train [epoch 2 batch 60]: : 1440it [00:26, 53.95it/s, mean train loss=0.527]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.50it/s, mean loss=0.379, macro f1=0.817]
train [epoch 3 batch 60]: : 1440it [00:26, 53.77it/s, mean train loss=0.318]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.52it/s, mean loss=0.335, macro f1=0.815]
train [epoch 4 batch 60]: : 1440it [00:26, 53.98it/s, mean train loss=0.181]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 107.95it/s, mean loss=0.262, macro f1=0.883]
train [epoch 5 batch 60]: : 1440it [00:26, 54.37it/s, mean train loss=0.183]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 112.85it/s, mean loss=0.267, macro f1=0.863]
train [epoch 6 batch 60]: : 1440it [00:26, 54.35it/s, mean train loss=0.108]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 112.51it/s, mean loss=0.28, macro f1=0.879]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Early stopping at epoch 6</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>test [batch 38]: : 912it [00:08, 113.54it/s, mean loss=0.226, macro f1=0.9]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 21 fold 2</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 60]: : 1440it [00:26, 54.06it/s, mean train loss=1.48]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.00it/s, mean loss=0.77, macro f1=0.606]
train [epoch 2 batch 60]: : 1440it [00:26, 53.78it/s, mean train loss=0.552]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 115.15it/s, mean loss=0.414, macro f1=0.805]
train [epoch 3 batch 60]: : 1440it [00:26, 53.38it/s, mean train loss=0.281]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 114.14it/s, mean loss=0.404, macro f1=0.847]
train [epoch 4 batch 60]: : 1440it [00:26, 54.42it/s, mean train loss=0.264]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 112.69it/s, mean loss=0.366, macro f1=0.862]
train [epoch 5 batch 60]: : 1440it [00:26, 53.76it/s, mean train loss=0.118]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 111.75it/s, mean loss=0.332, macro f1=0.88]
[I 2024-10-19 23:08:55,367] Trial 21 pruned. </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 22 params: {'batch_size': 24, 'lr': 0.00020235892239526168, 'weight_decay': 0.0014620396847455305, 'reduce_lr_factor': 0.07059156074150905, 'momentum': 0.9479754424548245, 'early_stop_patience': 2, 'jitter': False, 'horizontal_flip': False, 'rotation': False}
Trial 22 fold 1</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 60]: : 1440it [00:26, 53.67it/s, mean train loss=1.65]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 115.24it/s, mean loss=1.08, macro f1=0.426]
train [epoch 2 batch 60]: : 1440it [00:26, 53.68it/s, mean train loss=0.784]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 109.39it/s, mean loss=0.585, macro f1=0.714]
train [epoch 3 batch 60]: : 1440it [00:26, 53.90it/s, mean train loss=0.438]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.56it/s, mean loss=0.44, macro f1=0.795]
train [epoch 4 batch 60]: : 1440it [00:26, 53.42it/s, mean train loss=0.282]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 114.37it/s, mean loss=0.357, macro f1=0.826]
train [epoch 5 batch 60]: : 1440it [00:27, 52.72it/s, mean train loss=0.187]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 114.84it/s, mean loss=0.329, macro f1=0.854]
train [epoch 6 batch 60]: : 1440it [00:26, 54.11it/s, mean train loss=0.169]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 112.83it/s, mean loss=0.339, macro f1=0.821]
train [epoch 7 batch 60]: : 1440it [00:27, 53.19it/s, mean train loss=0.108]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 108.61it/s, mean loss=0.33, macro f1=0.856]
[I 2024-10-19 23:12:30,487] Trial 22 pruned. </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 23 params: {'batch_size': 24, 'lr': 0.0003998000959719629, 'weight_decay': 0.002486203660251997, 'reduce_lr_factor': 0.09072339594884746, 'momentum': 0.9750198623344721, 'early_stop_patience': 2, 'jitter': False, 'horizontal_flip': False, 'rotation': False}
Trial 23 fold 1</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 60]: : 1440it [00:26, 53.64it/s, mean train loss=1.52]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 112.83it/s, mean loss=0.857, macro f1=0.588]
train [epoch 2 batch 60]: : 1440it [00:26, 54.20it/s, mean train loss=0.574]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 107.73it/s, mean loss=0.463, macro f1=0.78]
train [epoch 3 batch 60]: : 1440it [00:26, 53.88it/s, mean train loss=0.313]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 112.56it/s, mean loss=0.415, macro f1=0.81]
train [epoch 4 batch 60]: : 1440it [00:26, 53.68it/s, mean train loss=0.208]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.20it/s, mean loss=0.258, macro f1=0.865]
train [epoch 5 batch 60]: : 1440it [00:27, 52.81it/s, mean train loss=0.16]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 114.79it/s, mean loss=0.252, macro f1=0.858]
train [epoch 6 batch 60]: : 1440it [00:26, 53.62it/s, mean train loss=0.134]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.26it/s, mean loss=0.243, macro f1=0.886]
train [epoch 7 batch 60]: : 1440it [00:26, 54.31it/s, mean train loss=0.0773]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 105.07it/s, mean loss=0.255, macro f1=0.902]
train [epoch 8 batch 60]: : 1440it [00:26, 53.65it/s, mean train loss=0.0419]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 111.02it/s, mean loss=0.228, macro f1=0.905]
[I 2024-10-19 23:16:35,547] Trial 23 pruned. </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 24 params: {'batch_size': 24, 'lr': 0.0004981905694438408, 'weight_decay': 0.0001860081403390097, 'reduce_lr_factor': 0.07923197031501877, 'momentum': 0.9554222873825267, 'early_stop_patience': 2, 'jitter': False, 'horizontal_flip': False, 'rotation': False}
Trial 24 fold 1</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 60]: : 1440it [00:26, 53.89it/s, mean train loss=1.35]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 112.16it/s, mean loss=0.905, macro f1=0.632]
train [epoch 2 batch 60]: : 1440it [00:26, 53.68it/s, mean train loss=0.572]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 111.72it/s, mean loss=0.599, macro f1=0.69]
train [epoch 3 batch 60]: : 1440it [00:26, 54.52it/s, mean train loss=0.382]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 109.82it/s, mean loss=0.321, macro f1=0.841]
train [epoch 4 batch 60]: : 1440it [00:26, 54.21it/s, mean train loss=0.229]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.89it/s, mean loss=0.315, macro f1=0.842]
train [epoch 5 batch 60]: : 1440it [00:26, 54.47it/s, mean train loss=0.178]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.49it/s, mean loss=0.362, macro f1=0.85]
train [epoch 6 batch 60]: : 1440it [00:26, 54.32it/s, mean train loss=0.0838]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 109.01it/s, mean loss=0.37, macro f1=0.849]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Early stopping at epoch 6</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>test [batch 38]: : 912it [00:08, 111.73it/s, mean loss=0.238, macro f1=0.9]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trial 24 fold 2</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 60]: : 1440it [00:26, 54.15it/s, mean train loss=1.39]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 112.44it/s, mean loss=0.633, macro f1=0.707]
train [epoch 2 batch 60]: : 1440it [00:26, 54.41it/s, mean train loss=0.448]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 105.40it/s, mean loss=0.375, macro f1=0.808]
train [epoch 3 batch 60]: : 1440it [00:26, 53.97it/s, mean train loss=0.26]
validate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.65it/s, mean loss=0.327, macro f1=0.826]
[I 2024-10-19 23:21:19,223] Trial 24 pruned. </code></pre>
</div>
</div>
</div>
<div id="723ca96f" class="cell">
<div class="sourceCode cell-code" id="cb578"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb578-1"><a href="#cb578-1" aria-hidden="true" tabindex="-1"></a>vgg16_pretrained_study.best_params</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-container">
<div class="cell-output cell-output-display" data-execution_count="41">
<pre><code>{'batch_size': 24,
 'lr': 0.0004819093288876784,
 'weight_decay': 0.0016937287816857739,
 'reduce_lr_factor': 0.04514422541734965,
 'momentum': 0.9669468784630878,
 'early_stop_patience': 3,
 'jitter': False,
 'horizontal_flip': False,
 'rotation': False}</code></pre>
</div>
</div>
</div>
<p>Interestingly, the pretrained VGG-16 model performed best without any data augmentation, like the MLP.</p>
</section>
</section>
<section id="training-and-evaluation" class="level2">
<h2 class="anchored" data-anchor-id="training-and-evaluation">Training and Evaluation</h2>
<p>Finally, with the best set of hyperparameters determined for each model, we can perform the actual training and evaluation. I first define a helper function to display the evaluation results, including a classification report containing various metrics, a confusion matrix and a visualisation of some of the predictions.</p>
<div id="0d432eea" class="cell">
<div class="sourceCode cell-code" id="cb580"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb580-1"><a href="#cb580-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> display_results(</span>
<span id="cb580-2"><a href="#cb580-2" aria-hidden="true" tabindex="-1"></a>    results: EvaluationResults,</span>
<span id="cb580-3"><a href="#cb580-3" aria-hidden="true" tabindex="-1"></a>    model: torch.nn.Module,</span>
<span id="cb580-4"><a href="#cb580-4" aria-hidden="true" tabindex="-1"></a>    ds: torch.utils.data.Dataset,</span>
<span id="cb580-5"><a href="#cb580-5" aria-hidden="true" tabindex="-1"></a>    transform: torchvision.transforms.v2.Transform,</span>
<span id="cb580-6"><a href="#cb580-6" aria-hidden="true" tabindex="-1"></a>    device: <span class="bu">str</span> <span class="op">=</span> DEVICE,</span>
<span id="cb580-7"><a href="#cb580-7" aria-hidden="true" tabindex="-1"></a>):</span>
<span id="cb580-8"><a href="#cb580-8" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(results.classification_report_str)</span>
<span id="cb580-9"><a href="#cb580-9" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"mean test loss: </span><span class="sc">{</span>results<span class="sc">.</span>mean_loss<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb580-10"><a href="#cb580-10" aria-hidden="true" tabindex="-1"></a>    disp <span class="op">=</span> sklearn.metrics.ConfusionMatrixDisplay(results.confusion_matrix, display_labels<span class="op">=</span>target_le.classes_)</span>
<span id="cb580-11"><a href="#cb580-11" aria-hidden="true" tabindex="-1"></a>    fig <span class="op">=</span> plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">10</span>))</span>
<span id="cb580-12"><a href="#cb580-12" aria-hidden="true" tabindex="-1"></a>    ax <span class="op">=</span> fig.add_subplot()</span>
<span id="cb580-13"><a href="#cb580-13" aria-hidden="true" tabindex="-1"></a>    disp.plot(ax<span class="op">=</span>ax)</span>
<span id="cb580-14"><a href="#cb580-14" aria-hidden="true" tabindex="-1"></a>    plt.show()</span>
<span id="cb580-15"><a href="#cb580-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb580-16"><a href="#cb580-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Show predictions for some samples.</span></span>
<span id="cb580-17"><a href="#cb580-17" aria-hidden="true" tabindex="-1"></a>    rows <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb580-18"><a href="#cb580-18" aria-hidden="true" tabindex="-1"></a>    cols <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb580-19"><a href="#cb580-19" aria-hidden="true" tabindex="-1"></a>    fig, axes <span class="op">=</span> plt.subplots(rows, cols, figsize<span class="op">=</span>(<span class="dv">2</span> <span class="op">*</span> cols, <span class="dv">2</span> <span class="op">*</span> rows))</span>
<span id="cb580-20"><a href="#cb580-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> idx <span class="kw">in</span> <span class="bu">range</span>(rows <span class="op">*</span> cols):</span>
<span id="cb580-21"><a href="#cb580-21" aria-hidden="true" tabindex="-1"></a>        img, label <span class="op">=</span> ds[idx]</span>
<span id="cb580-22"><a href="#cb580-22" aria-hidden="true" tabindex="-1"></a>        img_transformed <span class="op">=</span> transform(img.detach().clone().to(device))</span>
<span id="cb580-23"><a href="#cb580-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb580-24"><a href="#cb580-24" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Add an additional first dimension to simulate batching.</span></span>
<span id="cb580-25"><a href="#cb580-25" aria-hidden="true" tabindex="-1"></a>        img_transformed <span class="op">=</span> img_transformed[<span class="va">None</span>, :, :, :]</span>
<span id="cb580-26"><a href="#cb580-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb580-27"><a href="#cb580-27" aria-hidden="true" tabindex="-1"></a>        label <span class="op">=</span> target_le.inverse_transform([label])[<span class="dv">0</span>]</span>
<span id="cb580-28"><a href="#cb580-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb580-29"><a href="#cb580-29" aria-hidden="true" tabindex="-1"></a>        predicted_idx <span class="op">=</span> model(img_transformed).argmax(dim<span class="op">=</span><span class="dv">1</span>).cpu()</span>
<span id="cb580-30"><a href="#cb580-30" aria-hidden="true" tabindex="-1"></a>        predicted <span class="op">=</span> target_le.inverse_transform(predicted_idx)[<span class="dv">0</span>]</span>
<span id="cb580-31"><a href="#cb580-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb580-32"><a href="#cb580-32" aria-hidden="true" tabindex="-1"></a>        ax <span class="op">=</span> axes[idx <span class="op">//</span> cols, idx <span class="op">%</span> cols]</span>
<span id="cb580-33"><a href="#cb580-33" aria-hidden="true" tabindex="-1"></a>        ax.imshow(img.permute(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">0</span>))</span>
<span id="cb580-34"><a href="#cb580-34" aria-hidden="true" tabindex="-1"></a>        ax.set_title(<span class="ss">f"Truth: </span><span class="sc">{</span>label<span class="sc">}</span><span class="ch">\n</span><span class="ss">Predicted: </span><span class="sc">{</span>predicted<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb580-35"><a href="#cb580-35" aria-hidden="true" tabindex="-1"></a>    plt.tight_layout()</span>
<span id="cb580-36"><a href="#cb580-36" aria-hidden="true" tabindex="-1"></a>    plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>For keeping track of the training and validation losses using Tensorboard, we need to use PyTorch’s <code>SummaryWriter</code>. Training and validation losses are logged for each model using the train and validation loss hooks in the <code>Trainer</code>.</p>
<div id="6dd2f56c" class="cell">
<div class="sourceCode cell-code" id="cb581"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb581-1"><a href="#cb581-1" aria-hidden="true" tabindex="-1"></a>writer <span class="op">=</span> SummaryWriter()</span>
<span id="cb581-2"><a href="#cb581-2" aria-hidden="true" tabindex="-1"></a>writer.add_custom_scalars({</span>
<span id="cb581-3"><a href="#cb581-3" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Multilayer Perceptron"</span>: {</span>
<span id="cb581-4"><a href="#cb581-4" aria-hidden="true" tabindex="-1"></a>        <span class="st">"Loss"</span>: [<span class="st">"Multiline"</span>, [<span class="st">"mlp/loss/train"</span>, <span class="st">"mlp/loss/validation"</span>]],</span>
<span id="cb581-5"><a href="#cb581-5" aria-hidden="true" tabindex="-1"></a>    },</span>
<span id="cb581-6"><a href="#cb581-6" aria-hidden="true" tabindex="-1"></a>    <span class="st">"VGG16"</span>: {</span>
<span id="cb581-7"><a href="#cb581-7" aria-hidden="true" tabindex="-1"></a>        <span class="st">"Loss"</span>: [<span class="st">"Multiline"</span>, [<span class="st">"vgg16/loss/train"</span>, <span class="st">"vgg16/loss/validation"</span>]],</span>
<span id="cb581-8"><a href="#cb581-8" aria-hidden="true" tabindex="-1"></a>    },</span>
<span id="cb581-9"><a href="#cb581-9" aria-hidden="true" tabindex="-1"></a>    <span class="st">"VGG16 Pretrained"</span>: {</span>
<span id="cb581-10"><a href="#cb581-10" aria-hidden="true" tabindex="-1"></a>        <span class="st">"Loss"</span>: [<span class="st">"Multiline"</span>, [<span class="st">"vgg16-pretrained/loss/train"</span>, <span class="st">"vgg16-pretrained/loss/validation"</span>]],</span>
<span id="cb581-11"><a href="#cb581-11" aria-hidden="true" tabindex="-1"></a>    },</span>
<span id="cb581-12"><a href="#cb581-12" aria-hidden="true" tabindex="-1"></a>})</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>As a quick recap, we use an undersampler to undersample the shoes and tees classes to mitigate the class imbalance problem. Training will also use early stopping based on the validation loss — if the validation loss stops improving after a certain number of epochs, the training will be stopped.</p>
<p>Evaluation will use the following metrics / tools:</p>
<ul>
<li><p><strong>Accuracy:</strong> The number of correct predictions out of the total number of predictions. Although accuracy provides a general indication of how well the model is performing, it can be misleading when there is a class imbalance (which is the case for our dataset).</p></li>
<li><p><strong>Precision:</strong> The number of true positive predictions out of the total number of positive predictions. A high precision means the model is generally correct when it predicts a positive.</p></li>
<li><p><strong>Recall:</strong> The number of true positive predictions out of the total number of actual positive samples. A high recall means the model is able to identify most positive samples.</p></li>
<li><p><strong>F1 score:</strong> The harmonic mean of precision and recall. Can be interpreted as an average between precision and recall. F1 is especially useful when dealing with class imbalances since it combines both precision and recall.</p></li>
<li><p><strong>Macro average F1 score:</strong> The mean of the F1 scores for each class, useful for assessing overall model performance in multi-class classification tasks.</p></li>
<li><p><strong>Confusion matrix:</strong> A table that compares the actual target labels with the labels predicted by the model. From the confusion matrix, we can see what the model tends to misclassify samples as.</p></li>
</ul>
<p><strong>Note:</strong> For each model, its architecture will be shown before the training and evaluation begin.</p>
<section id="multilayer-perceptron-2" class="level3">
<h3 class="anchored" data-anchor-id="multilayer-perceptron-2">Multilayer Perceptron</h3>
<div id="8ccdb9f5" class="cell">
<div class="sourceCode cell-code" id="cb582"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb582-1"><a href="#cb582-1" aria-hidden="true" tabindex="-1"></a>mlp <span class="op">=</span> mlp_from_params(mlp_study.best_params)</span>
<span id="cb582-2"><a href="#cb582-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(mlp)</span>
<span id="cb582-3"><a href="#cb582-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb582-4"><a href="#cb582-4" aria-hidden="true" tabindex="-1"></a>mlp_results <span class="op">=</span> train_eval_generic_params(</span>
<span id="cb582-5"><a href="#cb582-5" aria-hidden="true" tabindex="-1"></a>    model<span class="op">=</span>mlp,</span>
<span id="cb582-6"><a href="#cb582-6" aria-hidden="true" tabindex="-1"></a>    ds_train<span class="op">=</span>ds_train,</span>
<span id="cb582-7"><a href="#cb582-7" aria-hidden="true" tabindex="-1"></a>    ds_val<span class="op">=</span>ds_val,</span>
<span id="cb582-8"><a href="#cb582-8" aria-hidden="true" tabindex="-1"></a>    ds_test<span class="op">=</span>ds_test,</span>
<span id="cb582-9"><a href="#cb582-9" aria-hidden="true" tabindex="-1"></a>    sampler<span class="op">=</span>undersampler,</span>
<span id="cb582-10"><a href="#cb582-10" aria-hidden="true" tabindex="-1"></a>    params<span class="op">=</span>mlp_study.best_params,</span>
<span id="cb582-11"><a href="#cb582-11" aria-hidden="true" tabindex="-1"></a>    train_loss_hook<span class="op">=</span><span class="kw">lambda</span> loss, epoch: writer.add_scalar(</span>
<span id="cb582-12"><a href="#cb582-12" aria-hidden="true" tabindex="-1"></a>        <span class="st">"mlp/loss/train"</span>, loss, epoch</span>
<span id="cb582-13"><a href="#cb582-13" aria-hidden="true" tabindex="-1"></a>    ),</span>
<span id="cb582-14"><a href="#cb582-14" aria-hidden="true" tabindex="-1"></a>    val_results_hook<span class="op">=</span><span class="kw">lambda</span> results, epoch: writer.add_scalar(</span>
<span id="cb582-15"><a href="#cb582-15" aria-hidden="true" tabindex="-1"></a>        <span class="st">"mlp/loss/validation"</span>, results.mean_loss, epoch</span>
<span id="cb582-16"><a href="#cb582-16" aria-hidden="true" tabindex="-1"></a>    ),</span>
<span id="cb582-17"><a href="#cb582-17" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-container">
<div class="cell-output cell-output-stdout">
<pre><code>Sequential(
  (0): Flatten(start_dim=1, end_dim=-1)
  (1): MultilayerPerceptron(
    (layers): ModuleList(
      (0): LazyLinear(in_features=0, out_features=256, bias=True)
      (1): LazyBatchNorm1d(0, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU()
      (3): LazyLinear(in_features=0, out_features=512, bias=True)
      (4): LazyBatchNorm1d(0, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU()
      (6): LazyLinear(in_features=0, out_features=256, bias=True)
      (7): LazyBatchNorm1d(0, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (8): ReLU()
      (9): LazyLinear(in_features=0, out_features=10, bias=True)
    )
  )
)</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 85]: : 2720it [00:08, 337.37it/s, mean train loss=1.14]
validate [batch 19]: : 608it [00:01, 305.52it/s, mean loss=0.953, macro f1=0.593]
train [epoch 2 batch 85]: : 2720it [00:08, 329.19it/s, mean train loss=0.669]
validate [batch 19]: : 608it [00:01, 307.83it/s, mean loss=0.743, macro f1=0.716]
train [epoch 3 batch 85]: : 2720it [00:08, 339.25it/s, mean train loss=0.551]
validate [batch 19]: : 608it [00:01, 307.82it/s, mean loss=0.652, macro f1=0.716]
train [epoch 4 batch 85]: : 2720it [00:08, 337.62it/s, mean train loss=0.477]
validate [batch 19]: : 608it [00:01, 305.81it/s, mean loss=0.672, macro f1=0.733]
train [epoch 5 batch 85]: : 2720it [00:08, 328.63it/s, mean train loss=0.387]
validate [batch 19]: : 608it [00:01, 310.40it/s, mean loss=0.662, macro f1=0.733]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Early stopping at epoch 5</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>test [batch 19]: : 608it [00:01, 308.16it/s, mean loss=0.573, macro f1=0.738]</code></pre>
</div>
</div>
</div>
<div id="d6926bfb" class="cell">
<div class="sourceCode cell-code" id="cb587"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb587-1"><a href="#cb587-1" aria-hidden="true" tabindex="-1"></a>torch.save(mlp.state_dict(), <span class="st">"mlp_state.pt"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="2b482e23" class="cell">
<div class="sourceCode cell-code" id="cb588"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb588-1"><a href="#cb588-1" aria-hidden="true" tabindex="-1"></a>display_results(</span>
<span id="cb588-2"><a href="#cb588-2" aria-hidden="true" tabindex="-1"></a>    mlp_results,</span>
<span id="cb588-3"><a href="#cb588-3" aria-hidden="true" tabindex="-1"></a>    mlp,</span>
<span id="cb588-4"><a href="#cb588-4" aria-hidden="true" tabindex="-1"></a>    ds_test,</span>
<span id="cb588-5"><a href="#cb588-5" aria-hidden="true" tabindex="-1"></a>    transforms_from_params(mlp_study.best_params)</span>
<span id="cb588-6"><a href="#cb588-6" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-container">
<div class="cell-output cell-output-stdout">
<pre><code>              precision    recall  f1-score   support

 accessories       0.93      0.98      0.96        44
     jackets       0.46      0.72      0.56        47
       jeans       0.97      0.97      0.97        39
    knitwear       0.24      0.27      0.25        41
      shirts       0.52      0.45      0.48        49
       shoes       0.99      0.98      0.99       148
      shorts       0.92      0.97      0.94        34
        tees       0.81      0.69      0.74       176

    accuracy                           0.77       578
   macro avg       0.73      0.75      0.74       578
weighted avg       0.79      0.77      0.78       578

mean test loss: 0.5728871100827267</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="mlp-vgg16-fashion_files/figure-html/cell-49-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="mlp-vgg16-fashion_files/figure-html/cell-49-output-3.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</div>
<p><strong>Train loss vs.&nbsp;validation loss curve from Tensorboard</strong> (squared points are train loss; diamond points are validation loss):</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="mlp-vgg16-fashion_files/figure-html/830226f9-6446-427d-bfa6-c54e56e070f2-1-4accfd39-e15f-4ffd-997f-6e8518d41c0d.png" class="img-fluid figure-img"></p>
<figcaption>curve-mlp.png</figcaption>
</figure>
</div>
<p>The train loss starts higher than the validation loss, though they both decrease. However, the validation loss seems to have a lower rate of decrease than the train loss — eventually, the train loss becomes lower than the validation loss. The MLP model seems to struggle to generalise to the validation set despite learning from the training data, which could either be a sign of overfitting or perhaps the model is too simple to adequately capture the underlying patterns. It may also be that the resizing of images to 64 by 64 removed too much information, so maybe more trials for hyperparameter tuning is needed.</p>
<p>Nonetheless, the performance metrics for the MLP model show strong results for some classes. For instance, categories like “shoes,” “accessories,” and “jeans” perform exceptionally well, with F1-scores of 99%, 96%, and 97% respectively. However, for classes like “knitwear” and “shirts”, the MLP model struggles, with F1-scores of 25% and 48% respectively. These results indicate both low precision and recall; i.e., the model often misclassifies these items and fails to identify them correctly. If we look at the confusion matrix, we see that the model appears to struggle with differentiating between knitwear, shirts and tees. For example, it misclassified 17 tees as jackets, 22 tees as knitwear and 14 tees as shirts. However, considering how similar these items can be, this is not unexpected, especially since MLPs do not account for the order of the pixels in the input images.</p>
<p>The accuracy of 77% indicates that the model’s overall performance is not bad. The macro average F1-score of 74% shows that the model’s performance across all classes is somewhat balanced. However, there is clearly still some room for improvement considering the misclassifications.</p>
<p>Future improvements should likely focus on improving the model’s ability to distinguish between knitwear, shirts and tees. Perhaps we could explore patch-based learning to allow the model to focus on the specific areas of the images (such as the sleeves).</p>
</section>
<section id="vgg-16-2" class="level3">
<h3 class="anchored" data-anchor-id="vgg-16-2">VGG-16</h3>
<div id="993bd0d8" class="cell">
<div class="sourceCode cell-code" id="cb590"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb590-1"><a href="#cb590-1" aria-hidden="true" tabindex="-1"></a>vgg16 <span class="op">=</span> VGG16(<span class="dv">10</span>)</span>
<span id="cb590-2"><a href="#cb590-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(vgg16)</span>
<span id="cb590-3"><a href="#cb590-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb590-4"><a href="#cb590-4" aria-hidden="true" tabindex="-1"></a>vgg16_results <span class="op">=</span> train_eval_generic_params(</span>
<span id="cb590-5"><a href="#cb590-5" aria-hidden="true" tabindex="-1"></a>    model<span class="op">=</span>vgg16,</span>
<span id="cb590-6"><a href="#cb590-6" aria-hidden="true" tabindex="-1"></a>    ds_train<span class="op">=</span>ds_train,</span>
<span id="cb590-7"><a href="#cb590-7" aria-hidden="true" tabindex="-1"></a>    ds_val<span class="op">=</span>ds_val,</span>
<span id="cb590-8"><a href="#cb590-8" aria-hidden="true" tabindex="-1"></a>    ds_test<span class="op">=</span>ds_test,</span>
<span id="cb590-9"><a href="#cb590-9" aria-hidden="true" tabindex="-1"></a>    sampler<span class="op">=</span>undersampler,</span>
<span id="cb590-10"><a href="#cb590-10" aria-hidden="true" tabindex="-1"></a>    params<span class="op">=</span>vgg16_study.best_params,</span>
<span id="cb590-11"><a href="#cb590-11" aria-hidden="true" tabindex="-1"></a>    train_loss_hook<span class="op">=</span><span class="kw">lambda</span> loss, epoch: writer.add_scalar(</span>
<span id="cb590-12"><a href="#cb590-12" aria-hidden="true" tabindex="-1"></a>        <span class="st">"vgg16/loss/train"</span>, loss, epoch</span>
<span id="cb590-13"><a href="#cb590-13" aria-hidden="true" tabindex="-1"></a>    ),</span>
<span id="cb590-14"><a href="#cb590-14" aria-hidden="true" tabindex="-1"></a>    val_results_hook<span class="op">=</span><span class="kw">lambda</span> results, epoch: writer.add_scalar(</span>
<span id="cb590-15"><a href="#cb590-15" aria-hidden="true" tabindex="-1"></a>        <span class="st">"vgg16/loss/validation"</span>, results.mean_loss, epoch</span>
<span id="cb590-16"><a href="#cb590-16" aria-hidden="true" tabindex="-1"></a>    ),</span>
<span id="cb590-17"><a href="#cb590-17" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-container">
<div class="cell-output cell-output-stdout">
<pre><code>VGG16(
  (convs): Sequential(
    (0): Sequential(
      (0): LazyConv2d(0, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): LazyBatchNorm2d(0, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): LazyConv2d(0, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): LazyBatchNorm2d(0, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU(inplace=True)
      (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    )
    (1): Sequential(
      (0): LazyConv2d(0, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): LazyBatchNorm2d(0, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): LazyConv2d(0, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): LazyBatchNorm2d(0, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU(inplace=True)
      (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    )
    (2): Sequential(
      (0): LazyConv2d(0, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): LazyBatchNorm2d(0, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): LazyConv2d(0, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): LazyBatchNorm2d(0, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU(inplace=True)
      (6): LazyConv2d(0, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (7): LazyBatchNorm2d(0, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (8): ReLU(inplace=True)
      (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    )
    (3): Sequential(
      (0): LazyConv2d(0, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): LazyBatchNorm2d(0, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): LazyConv2d(0, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): LazyBatchNorm2d(0, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU(inplace=True)
      (6): LazyConv2d(0, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (7): LazyBatchNorm2d(0, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (8): ReLU(inplace=True)
      (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    )
    (4): Sequential(
      (0): LazyConv2d(0, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): LazyBatchNorm2d(0, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): LazyConv2d(0, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): LazyBatchNorm2d(0, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU(inplace=True)
      (6): LazyConv2d(0, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (7): LazyBatchNorm2d(0, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (8): ReLU(inplace=True)
      (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    )
  )
  (avg_pool): Sequential(
    (0): AdaptiveAvgPool2d(output_size=(7, 7))
    (1): Flatten(start_dim=1, end_dim=-1)
  )
  (fcs): Sequential(
    (0): LazyLinear(in_features=0, out_features=4096, bias=True)
    (1): ReLU(inplace=True)
    (2): Dropout(p=0.5, inplace=False)
    (3): LazyLinear(in_features=0, out_features=4096, bias=True)
    (4): ReLU(inplace=True)
    (5): Dropout(p=0.5, inplace=False)
    (6): LazyLinear(in_features=0, out_features=10, bias=True)
  )
)</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 113]: : 2712it [00:16, 159.73it/s, mean train loss=1.41]
validate [batch 25]: : 600it [00:02, 255.52it/s, mean loss=0.925, macro f1=0.445]
train [epoch 2 batch 113]: : 2712it [00:16, 161.91it/s, mean train loss=0.83]
validate [batch 25]: : 600it [00:02, 236.30it/s, mean loss=0.595, macro f1=0.693]
train [epoch 3 batch 113]: : 2712it [00:16, 162.11it/s, mean train loss=0.569]
validate [batch 25]: : 600it [00:02, 257.69it/s, mean loss=0.574, macro f1=0.692]
train [epoch 4 batch 113]: : 2712it [00:16, 160.68it/s, mean train loss=0.442]
validate [batch 25]: : 600it [00:02, 250.62it/s, mean loss=0.326, macro f1=0.834]
train [epoch 5 batch 113]: : 2712it [00:16, 161.85it/s, mean train loss=0.368]
validate [batch 25]: : 600it [00:02, 255.54it/s, mean loss=0.405, macro f1=0.824]
train [epoch 6 batch 113]: : 2712it [00:16, 161.04it/s, mean train loss=0.331]
validate [batch 25]: : 600it [00:02, 258.74it/s, mean loss=0.41, macro f1=0.848]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Early stopping at epoch 6</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>test [batch 25]: : 600it [00:02, 240.97it/s, mean loss=0.529, macro f1=0.814]</code></pre>
</div>
</div>
</div>
<div id="c2268d0c" class="cell">
<div class="sourceCode cell-code" id="cb595"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb595-1"><a href="#cb595-1" aria-hidden="true" tabindex="-1"></a>torch.save(vgg16.state_dict(), <span class="st">"vgg16_state.pt"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="1cac2a5b" class="cell">
<div class="sourceCode cell-code" id="cb596"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb596-1"><a href="#cb596-1" aria-hidden="true" tabindex="-1"></a>display_results(</span>
<span id="cb596-2"><a href="#cb596-2" aria-hidden="true" tabindex="-1"></a>    vgg16_results,</span>
<span id="cb596-3"><a href="#cb596-3" aria-hidden="true" tabindex="-1"></a>    vgg16,</span>
<span id="cb596-4"><a href="#cb596-4" aria-hidden="true" tabindex="-1"></a>    ds_test,</span>
<span id="cb596-5"><a href="#cb596-5" aria-hidden="true" tabindex="-1"></a>    transforms_from_params(vgg16_study.best_params)</span>
<span id="cb596-6"><a href="#cb596-6" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-container">
<div class="cell-output cell-output-stdout">
<pre><code>              precision    recall  f1-score   support

 accessories       0.91      0.98      0.95        44
     jackets       0.68      0.96      0.80        47
       jeans       0.97      1.00      0.99        39
    knitwear       0.42      0.76      0.54        41
      shirts       0.53      0.41      0.46        49
       shoes       0.99      0.97      0.98       148
      shorts       0.97      1.00      0.99        34
        tees       0.94      0.72      0.82       176

    accuracy                           0.83       578
   macro avg       0.80      0.85      0.81       578
weighted avg       0.86      0.83      0.84       578

mean test loss: 0.5294404172897339</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="mlp-vgg16-fashion_files/figure-html/cell-52-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="mlp-vgg16-fashion_files/figure-html/cell-52-output-3.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</div>
<p><strong>Train loss vs.&nbsp;validation loss curve from Tensorboard</strong> (squared points are train loss; diamond points are validation loss):</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="mlp-vgg16-fashion_files/figure-html/6c19c9fb-e85c-46dc-9306-ef6df84e5cef-1-b324e84a-9bee-47b4-a1bd-04bd329ff750.png" class="img-fluid figure-img"></p>
<figcaption>curve-vgg16.png</figcaption>
</figure>
</div>
<p>The curve for the VGG-16 model looks good — both train and validation losses decrease and eventually converge. There does not seem to be any sign of overfitting as the validation loss does not increase to become higher than the train loss.</p>
<p>The custom VGG-16 model achieved an overall accuracy of 83% and a macro average F1-score of 81%. This indicates generally balanced performance across classes. However, this is worse than I expected since the VGG-16 architecture is deep and should be able to handle fairly complex patterns. High-performing classes include “accessories”, “jeans”, “shoes”, and “shorts”, with F1-scores above 95%.</p>
<p>The model struggles with “knitwear” and “shirts,” showing F1-scores of 54% and 46% respectively. Looking at the confusion matrix, the model seems to have issues distinguishing between knitwear, shirts and tees — similar to the MLP model, but to a lesser extent. “Shirts”, in particular, has both low precision (53%) and recall (41%). Like the MLP, if we want to improve the model’s performance, we should focus on distinguishing between these classes.</p>
</section>
<section id="vgg-16-pretrained-2" class="level3">
<h3 class="anchored" data-anchor-id="vgg-16-pretrained-2">VGG-16 Pretrained</h3>
<div id="4d68fbc7" class="cell">
<div class="sourceCode cell-code" id="cb598"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb598-1"><a href="#cb598-1" aria-hidden="true" tabindex="-1"></a>vgg16_pretrained <span class="op">=</span> make_vgg16_pretrained()</span>
<span id="cb598-2"><a href="#cb598-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(vgg16_pretrained)</span>
<span id="cb598-3"><a href="#cb598-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb598-4"><a href="#cb598-4" aria-hidden="true" tabindex="-1"></a>vgg16_pretrained_results <span class="op">=</span> train_eval_generic_params(</span>
<span id="cb598-5"><a href="#cb598-5" aria-hidden="true" tabindex="-1"></a>    model<span class="op">=</span>vgg16_pretrained,</span>
<span id="cb598-6"><a href="#cb598-6" aria-hidden="true" tabindex="-1"></a>    ds_train<span class="op">=</span>ds_train,</span>
<span id="cb598-7"><a href="#cb598-7" aria-hidden="true" tabindex="-1"></a>    ds_val<span class="op">=</span>ds_val,</span>
<span id="cb598-8"><a href="#cb598-8" aria-hidden="true" tabindex="-1"></a>    ds_test<span class="op">=</span>ds_test,</span>
<span id="cb598-9"><a href="#cb598-9" aria-hidden="true" tabindex="-1"></a>    sampler<span class="op">=</span>undersampler,</span>
<span id="cb598-10"><a href="#cb598-10" aria-hidden="true" tabindex="-1"></a>    params<span class="op">=</span>vgg16_pretrained_study.best_params,</span>
<span id="cb598-11"><a href="#cb598-11" aria-hidden="true" tabindex="-1"></a>    train_loss_hook<span class="op">=</span><span class="kw">lambda</span> loss, epoch: writer.add_scalar(</span>
<span id="cb598-12"><a href="#cb598-12" aria-hidden="true" tabindex="-1"></a>        <span class="st">"vgg16-pretrained/loss/train"</span>, loss, epoch</span>
<span id="cb598-13"><a href="#cb598-13" aria-hidden="true" tabindex="-1"></a>    ),</span>
<span id="cb598-14"><a href="#cb598-14" aria-hidden="true" tabindex="-1"></a>    val_results_hook<span class="op">=</span><span class="kw">lambda</span> results, epoch: writer.add_scalar(</span>
<span id="cb598-15"><a href="#cb598-15" aria-hidden="true" tabindex="-1"></a>        <span class="st">"vgg16-pretrained/loss/validation"</span>, results.mean_loss, epoch</span>
<span id="cb598-16"><a href="#cb598-16" aria-hidden="true" tabindex="-1"></a>    ),</span>
<span id="cb598-17"><a href="#cb598-17" aria-hidden="true" tabindex="-1"></a>    transforms_override<span class="op">=</span>torchvision.models.VGG16_Weights.IMAGENET1K_V1.transforms(),</span>
<span id="cb598-18"><a href="#cb598-18" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-container">
<div class="cell-output cell-output-stdout">
<pre><code>VGG(
  (features): Sequential(
    (0): LazyConv2d(0, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU(inplace=True)
    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (3): ReLU(inplace=True)
    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (6): ReLU(inplace=True)
    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (8): ReLU(inplace=True)
    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (11): ReLU(inplace=True)
    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (13): ReLU(inplace=True)
    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (15): ReLU(inplace=True)
    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (18): ReLU(inplace=True)
    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (20): ReLU(inplace=True)
    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (22): ReLU(inplace=True)
    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (25): ReLU(inplace=True)
    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (27): ReLU(inplace=True)
    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (29): ReLU(inplace=True)
    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))
  (classifier): Sequential(
    (0): Linear(in_features=25088, out_features=4096, bias=True)
    (1): ReLU(inplace=True)
    (2): Dropout(p=0.5, inplace=False)
    (3): Linear(in_features=4096, out_features=4096, bias=True)
    (4): ReLU(inplace=True)
    (5): Dropout(p=0.5, inplace=False)
    (6): LazyLinear(in_features=0, out_features=10, bias=True)
  )
)</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train [epoch 1 batch 113]: : 2712it [00:50, 54.02it/s, mean train loss=1.1]
validate [batch 25]: : 600it [00:04, 120.19it/s, mean loss=0.418, macro f1=0.84]
train [epoch 2 batch 113]: : 2712it [00:50, 54.01it/s, mean train loss=0.299]
validate [batch 25]: : 600it [00:04, 122.14it/s, mean loss=0.307, macro f1=0.898]
train [epoch 3 batch 113]: : 2712it [00:51, 53.10it/s, mean train loss=0.193]
validate [batch 25]: : 600it [00:04, 120.86it/s, mean loss=0.172, macro f1=0.911]
train [epoch 4 batch 113]: : 2712it [00:50, 53.79it/s, mean train loss=0.116]
validate [batch 25]: : 600it [00:04, 121.77it/s, mean loss=0.176, macro f1=0.92]
train [epoch 5 batch 113]: : 2712it [00:51, 53.13it/s, mean train loss=0.0693]
validate [batch 25]: : 600it [00:05, 118.23it/s, mean loss=0.167, macro f1=0.937]
train [epoch 6 batch 113]: : 2712it [00:50, 53.73it/s, mean train loss=0.0641]
validate [batch 25]: : 600it [00:04, 120.54it/s, mean loss=0.138, macro f1=0.927]
train [epoch 7 batch 113]: : 2712it [00:50, 53.66it/s, mean train loss=0.0411]
validate [batch 25]: : 600it [00:04, 121.00it/s, mean loss=0.153, macro f1=0.93]
train [epoch 8 batch 113]: : 2712it [00:51, 53.03it/s, mean train loss=0.026]
validate [batch 25]: : 600it [00:04, 120.74it/s, mean loss=0.244, macro f1=0.926]
train [epoch 9 batch 113]: : 2712it [00:50, 53.63it/s, mean train loss=0.029]
validate [batch 25]: : 600it [00:04, 120.19it/s, mean loss=0.19, macro f1=0.933]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Early stopping at epoch 9</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>test [batch 25]: : 600it [00:04, 120.08it/s, mean loss=0.203, macro f1=0.931]</code></pre>
</div>
</div>
</div>
<div id="fbd77948" class="cell">
<div class="sourceCode cell-code" id="cb603"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb603-1"><a href="#cb603-1" aria-hidden="true" tabindex="-1"></a>torch.save(vgg16_pretrained.state_dict(), <span class="st">"vgg16_pretrained_state.pt"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="a888697e" class="cell">
<div class="sourceCode cell-code" id="cb604"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb604-1"><a href="#cb604-1" aria-hidden="true" tabindex="-1"></a>display_results(</span>
<span id="cb604-2"><a href="#cb604-2" aria-hidden="true" tabindex="-1"></a>    vgg16_pretrained_results,</span>
<span id="cb604-3"><a href="#cb604-3" aria-hidden="true" tabindex="-1"></a>    vgg16_pretrained,</span>
<span id="cb604-4"><a href="#cb604-4" aria-hidden="true" tabindex="-1"></a>    ds_test,</span>
<span id="cb604-5"><a href="#cb604-5" aria-hidden="true" tabindex="-1"></a>    torchvision.models.VGG16_Weights.IMAGENET1K_V1.transforms(),</span>
<span id="cb604-6"><a href="#cb604-6" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-container">
<div class="cell-output cell-output-stdout">
<pre><code>              precision    recall  f1-score   support

 accessories       1.00      0.98      0.99        44
     jackets       0.90      0.91      0.91        47
       jeans       1.00      1.00      1.00        39
    knitwear       0.76      0.68      0.72        41
      shirts       0.88      0.94      0.91        49
       shoes       0.99      1.00      1.00       148
      shorts       0.97      1.00      0.99        34
        tees       0.95      0.94      0.95       176

    accuracy                           0.95       578
   macro avg       0.93      0.93      0.93       578
weighted avg       0.95      0.95      0.95       578

mean test loss: 0.20280604993458837</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="mlp-vgg16-fashion_files/figure-html/cell-55-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="mlp-vgg16-fashion_files/figure-html/cell-55-output-3.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</div>
<p><strong>Train loss vs.&nbsp;validation loss curve from Tensorboard</strong> (squared points are train loss; diamond points are validation loss):</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="mlp-vgg16-fashion_files/figure-html/a3195158-4564-45b2-b26a-289d898466eb-1-43c837df-e3ec-4860-ba14-2271c80c6de5.png" class="img-fluid figure-img"></p>
<figcaption>curve-vgg16-pretrained.png</figcaption>
</figure>
</div>
<p>Unfortunately, the pretrained VGG-16 model seems to have slightly overfitted to the training data as the validation loss starts to increase slightly. However, the training was stopped before it overfitted further. Interestingly, the validation loss decreases at a much slower rate than that of the custom VGG-16 model. This is likely because the pretrained model has already learned common patterns from its previous training — indeed, the validation loss was already nearly 0.4 after the first epoch, unlike the custom VGG-16 model, whose validation loss was around 0.9 after the first epoch.</p>
<p>The fine-tuned pretrained VGG-16 model achieved an impressive overall accuracy of 95% and a macro average F1-score of 93%. Classes such as “accessories,” “jeans,” “shoes,” and “shorts” had near-perfect performance, with F1-scores around 99% or 100%. “Tees” and “jackets” also performed well, both achieving F1-scores above 90%, indicating strong precision and recall.</p>
<p>However, “knitwear” was the lowest-performing category, with an F1-score of 72%. Looking at the confusion matrix, the model seems to sometimes misclassify knitwear as either jackets or tees. Similarly, the model sometimes misclassifies tees as knitwear. Despite this, the model shows excellent results overall with little variance in precision and recall between classes.</p>
</section>
<section id="tensorboard" class="level3">
<h3 class="anchored" data-anchor-id="tensorboard">Tensorboard</h3>
<p>The following cell runs Tensorboard with the train and validation loss data within the notebook. Note that this will not work on Kaggle (but it does work on Google Colab) — if you are using Kaggle, you will have to download the logs and run Tensorboard locally. For convenience, the plots have already been included in this notebook as images in the previous sections.</p>
<div id="90802c92" class="cell">
<div class="sourceCode cell-code" id="cb606"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb606-1"><a href="#cb606-1" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>load_ext tensorboard</span>
<span id="cb606-2"><a href="#cb606-2" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>tensorboard <span class="op">--</span>logdir runs</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>The pretrained VGG-16 model achieved the best performance out of the three models, with an overall accuracy of 95% and a macro average F1-score of 93%. The custom VGG-16 model had decent performance but falls behind, with an overall accuracy of 83% and a macro average F1-score of 81%. This result is expected, however, since training a model from scratch generally requires more extensive tuning and data to achieve optimal results than tuning a pre-trained model, which would already have learned common patterns from its previous training.</p>
<p>Lastly, the MLP model had an overall accuracy of 77% and a macro average F1-score of 74%. Since MLPs are unable to understand the spacial structure of images (they essentially ignore the order of pixels), the fact that the MLP performed the worst is not surprising.</p>
<p>All models seem to struggle with differentiating certain classes (to different extents), especially knitwear, shirts and tees. Considering how similar images in these classes can be, though, the confusion is not unexpected.</p>
<p>For future work, there is clearly room for improvement for the underperforming classes. Although hyperparameter tuning was done, there were still many hyperparameters left unexplored (e.g., use of dropout and regularisation, additional data augmentation transformations). Furthermore, only 25 trials were used for hyperparameter tuning — increasing the number of trials may lead to better hyperparameters that improve model performance. It may also be worth exploring cost-sentitive learning by assigning higher penalties to misclassifications, especially for the underperforming classes.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/dixslyf\.github\.io\/mlp-vgg16-fashion");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




<script src="../site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>