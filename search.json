[
  {
    "objectID": "notebooks/mlp-vgg16-fashion.html",
    "href": "notebooks/mlp-vgg16-fashion.html",
    "title": "A Comparison of Multilayer Perceptron and VGG-16 Models for Fashion Image Classification",
    "section": "",
    "text": "This notebook develops three models for image classification on a dataset containing images of clothes and accessories. It was written as part of a university assignment on machine learning.\nThe three models are as follows:\n\nA multilayer perceptron (MLP)\nA VGG-16 implemented from scratch\nA fine-tuned VGG-16 from PyTorch pre-trained on the ImageNet dataset\n\nAll models were implemented in PyTorch.\nThe development of the models followed a typical machine learning pipeline:\n\nData analysis: Quick exploration of the dataset to understand its structure, class distribution and potential issues such as class imbalance.\nData preprocessing: Preparation of the data by dividing the dataset into train, validation and test splits, normalising pixel values, label encoding the target feature and defining an undersampling procedure.\nHyperparameter tuning: Tuning of hyperparameters like learning rate, batch size and number of layers to optimise each model’s performance. Hyperparameter tuning was performed using Optuna. Each model has different hyperparameters to tune — these will be discussed more extensively later.\nTraining: Training of the models on the train split using the best set of hyperparameters determined by the hyperparameter tuning procedure.\nEvaluation: Evaluation of the models on the test split using confusion matrices and metrics like accuracy and F1-score. Additionally, we will analyse each model’s loss curves using Tensorboard and then compare the models’ performance.\n\nNote that this notebook takes several hours to run even with a GPU."
  },
  {
    "objectID": "notebooks/mlp-vgg16-fashion.html#introduction",
    "href": "notebooks/mlp-vgg16-fashion.html#introduction",
    "title": "A Comparison of Multilayer Perceptron and VGG-16 Models for Fashion Image Classification",
    "section": "",
    "text": "This notebook develops three models for image classification on a dataset containing images of clothes and accessories. It was written as part of a university assignment on machine learning.\nThe three models are as follows:\n\nA multilayer perceptron (MLP)\nA VGG-16 implemented from scratch\nA fine-tuned VGG-16 from PyTorch pre-trained on the ImageNet dataset\n\nAll models were implemented in PyTorch.\nThe development of the models followed a typical machine learning pipeline:\n\nData analysis: Quick exploration of the dataset to understand its structure, class distribution and potential issues such as class imbalance.\nData preprocessing: Preparation of the data by dividing the dataset into train, validation and test splits, normalising pixel values, label encoding the target feature and defining an undersampling procedure.\nHyperparameter tuning: Tuning of hyperparameters like learning rate, batch size and number of layers to optimise each model’s performance. Hyperparameter tuning was performed using Optuna. Each model has different hyperparameters to tune — these will be discussed more extensively later.\nTraining: Training of the models on the train split using the best set of hyperparameters determined by the hyperparameter tuning procedure.\nEvaluation: Evaluation of the models on the test split using confusion matrices and metrics like accuracy and F1-score. Additionally, we will analyse each model’s loss curves using Tensorboard and then compare the models’ performance.\n\nNote that this notebook takes several hours to run even with a GPU."
  },
  {
    "objectID": "notebooks/mlp-vgg16-fashion.html#preamble",
    "href": "notebooks/mlp-vgg16-fashion.html#preamble",
    "title": "A Comparison of Multilayer Perceptron and VGG-16 Models for Fashion Image Classification",
    "section": "Preamble",
    "text": "Preamble\nWe’ll start by installing additional libraries we need:\n\n# For downloading the dataset from Google Drive.\n!pip install gdown\n\n\nCollecting gdown\n  Downloading gdown-5.2.0-py3-none-any.whl.metadata (5.8 kB)\nRequirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.10/site-packages (from gdown) (4.12.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from gdown) (3.15.1)\nRequirement already satisfied: requests[socks] in /opt/conda/lib/python3.10/site-packages (from gdown) (2.32.3)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from gdown) (4.66.4)\nRequirement already satisfied: soupsieve&gt;1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4-&gt;gdown) (2.5)\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /opt/conda/lib/python3.10/site-packages (from requests[socks]-&gt;gdown) (3.3.2)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /opt/conda/lib/python3.10/site-packages (from requests[socks]-&gt;gdown) (3.7)\nRequirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests[socks]-&gt;gdown) (1.26.18)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests[socks]-&gt;gdown) (2024.8.30)\nRequirement already satisfied: PySocks!=1.5.7,&gt;=1.5.6 in /opt/conda/lib/python3.10/site-packages (from requests[socks]-&gt;gdown) (1.7.1)\nDownloading gdown-5.2.0-py3-none-any.whl (18 kB)\nInstalling collected packages: gdown\nSuccessfully installed gdown-5.2.0\n\n\n\nKaggle environments install Optuna by default. However, other environments may not, so we check and install Optuna if needed:\n\nimport importlib.util\n\nspec = importlib.util.find_spec(\"optuna\")\nif spec is None:\n    import subprocess\n    subprocess.run([\"pip\", \"install\", \"optuna\"])\n\nNow, we can import all the libraries and modules we need for the rest of the notebook:\n\nimport itertools\nimport gc\nimport math\nimport os\nimport statistics\nfrom collections.abc import Iterable\nfrom dataclasses import dataclass\nfrom typing import Any, Callable, Iterable, Optional, Sequence\n\nimport gdown\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport optuna\nimport seaborn as sns\nimport sklearn\nimport sklearn.model_selection\nimport torch\nimport torchvision\nimport torchvision.transforms.v2\nfrom torch.utils.tensorboard import SummaryWriter\nfrom sklearn.preprocessing import LabelEncoder\nimport tensorboard\nfrom torch import Tensor\nfrom torch.utils.data import DataLoader, Dataset, Subset\nfrom tqdm import tqdm\n\nWe’ll define some constants as configuration for this notebook. Feel free to adjust these constants to reduce the run time of the notebook:\n\n# Variables for downloading and locating the dataset.\nDATASET_GDRIVE_URL = \"https://drive.google.com/file/d/1nWRm-Npq_QE0j_sHyVVxVEx2Rb0Lc1zU/view\"\nDATASET_OUT_PATH = \"data.zip\"\nDATASET_ROOT_PATH = \"data/\"\n\n# Automatically determine the device to use for PyTorch.\n# On Google Colab and Kaggle, make sure to enable an accelerator to use CUDA.\nDEVICE = torch.device(\n    \"cuda\"\n    if torch.cuda.is_available()\n    else \"mps\"\n    if torch.backends.mps.is_available()\n    else \"cpu\"\n)\n\n# Default batch size for training and evaluation.\n# Note that the batch size is still a hyperparameter that will be tuned;\n# this is just the default.\nDEFAULT_BATCH_SIZE: int = 32\n\n# Number of folds to use for hyperparameter tuning.\nN_FOLDS: int = 3\n\n# Number of trials for hyperparameter tuning.\n# Note: Be careful not to reduce this too much as you can accidentally cause\n#       all trials to be pruned! If you want to set this to a low number,\n#       you should disable trial pruning by setting `PRUNE_TRIALS` (below) to `False`.\nN_TRIALS: int = 25\n\n# Max number of epochs for hyperparameter tuning.\nN_EPOCHS_TUNE: int = 30\n\n# Max number of epochs for the real training.\nN_EPOCHS_TRAIN: int = 30\n\n# Whether to prune trials during hyperparameter tuning.\nPRUNE_TRIALS: bool = True"
  },
  {
    "objectID": "notebooks/mlp-vgg16-fashion.html#data",
    "href": "notebooks/mlp-vgg16-fashion.html#data",
    "title": "A Comparison of Multilayer Perceptron and VGG-16 Models for Fashion Image Classification",
    "section": "Data",
    "text": "Data\nThis section includes the downloading of the dataset, preliminary analysis and preprocessing steps.\nThe dataset contains various categories of fashion images. Unfortunately, the origin of the dataset remains unknown — students were only given a Google Drive link to the dataset without any source provided.\n\nDownloading the Dataset\nFirst, we need to download the dataset. We’ll download it directly from Google Drive using the gdown library:\n\ngdown.cached_download(DATASET_GDRIVE_URL, DATASET_OUT_PATH, fuzzy=True, postprocess=gdown.extractall)\n\n\nCached downloading...\nFrom (original): https://drive.google.com/uc?id=1nWRm-Npq_QE0j_sHyVVxVEx2Rb0Lc1zU\nFrom (redirected): https://drive.google.com/uc?id=1nWRm-Npq_QE0j_sHyVVxVEx2Rb0Lc1zU&confirm=t&uuid=92f80be4-e2d6-4b2e-b82d-94faf30a6bf7\nTo: data.zip\n100%|██████████| 44.2M/44.2M [00:00&lt;00:00, 64.7MB/s]\n\n\n'data.zip'\n\n\n\nTo verify that the dataset was downloaded and extracted, we’ll use the shell’s ls command to list the files in the data directory. The following command should show test  train  valid if the dataset was successfully downloaded and extracted:\n\n!ls data\n\n\n/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n\n\ntest  train  valid\n\n\n\nIf the download fails (e.g., because the Google Drive link is down), please manually download and extract the dataset and set the DATASET_ROOT_PATH constant to the root directory of the dataset. You can find a copy of the dataset from this notebook’s GitHub repository here.\n\n\nCustom Dataset\nTo load the images from the dataset, we need to define a custom PyTorch Dataset:\n\nclass ClothesDataset(Dataset):\n    \"\"\"\n    A PyTorch `Dataset` class for loading the clothes and accessories dataset.\n\n    Attributes:\n        input_transform (Optional[Callable[[Tensor], Tensor]]): Transformation applied to input images.\n        target_transform (Optional[Callable[[str], Any]]): Transformation applied to target labels.\n        categories (set[str]): The unique categories (subdirectory names) present in the dataset.\n    \"\"\"\n\n    def __init__(\n        self,\n        img_dirs: str | Iterable[str],\n        input_transform: Optional[Callable[[Tensor], Tensor]] = None,\n        target_transform: Optional[Callable[[str], Any]] = None,\n    ):\n        \"\"\"\n        Initialises the dataset with one or more directories containing the images,\n        and with optional input and target transformations.\n\n        The given directories are assumed to be organised such that images are put into subdirectories\n        that correspond to the categories.\n\n        Args:\n            img_dirs (str | Iterable[str]): Directory path(s) containing the images.\n            input_transform (Optional[Callable[[Tensor], Tensor]]): Transformation applied to input images.\n            target_transform (Optional[Callable[[str], Any]]): Transformation applied to target labels.\n        \"\"\"\n        self._img_dirs: list[str] = (\n            [img_dirs] if type(img_dirs) is str else list(img_dirs)\n        )\n\n        self.input_transform = input_transform\n        self.target_transform = target_transform\n\n        # A list of pairs that maps the path of an image (path includes each `img_dir`)\n        # to its category. We're intentionally not using a dictionary because we need\n        # to access elements by index, which is not possible with a dictionary.\n        #\n        # To be lazy and avoid I/O in `__init__()`, this will only be populated on\n        # the first call to `__getitem__()` or `__len__()`.\n        self._img_label_map: Optional[list[tuple[str, str]]] = None\n\n    @property\n    def categories(self) -&gt; set[str]:\n        \"\"\"\n        Returns the unique categories in the dataset.\n\n        Returns:\n            set[str]: Set of unique categories in the dataset.\n        \"\"\"\n        if self._img_label_map is None:\n            self._init_img_label_map()\n\n        return self._categories\n\n    def targets(self, indices: Optional[Iterable[int]] = None) -&gt; Iterable:\n        \"\"\"\n        Returns the target labels for the specified indices or for all samples if indices are not provided.\n\n        Args:\n            indices (Optional[Iterable[int]]): Indices of the target labels to retrieve.\n\n        Yields:\n            Iterable: Transformed target labels or raw labels if no transformation is applied.\n        \"\"\"\n        if indices is None:\n            it = self._img_label_map\n        else:\n            it = (self._img_label_map[idx] for idx in indices)\n\n        for path, cat in it:\n            if self.target_transform is None:\n                yield cat\n            else:\n                yield self.target_transform(cat)\n\n    def _init_img_label_map(self):\n        \"\"\"\n        Initialises the image-label mapping by listing all images and their corresponding categories\n        from the specified directories. The subdirectory names represent the categories.\n        \"\"\"\n        cats = {\n            cat\n            for img_dir in self._img_dirs\n            for cat in os.listdir(img_dir)\n            if os.path.isdir(os.path.join(img_dir, cat))\n        }\n\n        self._categories = cats\n        self._img_label_map = []\n        for img_dir, cat in itertools.product(self._img_dirs, cats):\n            cat_dir = os.path.join(img_dir, cat)\n            if not os.path.isdir(cat_dir):\n                continue\n\n            for img in os.listdir(cat_dir):\n                img_path = os.path.join(cat_dir, img)\n                self._img_label_map.append((img_path, cat))\n\n    def __getitem__(self, idx: int) -&gt; tuple[Tensor, str]:\n        \"\"\"\n        Returns the image and corresponding label at the specified index.\n\n        Args:\n            idx (int): Index of the image-label pair to retrieve.\n\n        Returns:\n            tuple[Tensor, str]: A pair of the image tensor and its corresponding label, with optional\n            transformations applied.\n        \"\"\"\n        if self._img_label_map is None:\n            self._init_img_label_map()\n\n        img_path, label = self._img_label_map[idx]\n        img = torchvision.io.read_image(img_path)\n\n        if self.input_transform is not None:\n            img = self.input_transform(img)\n\n        if self.target_transform is not None:\n            label = self.target_transform(label)\n\n        return img, label\n\n    def __len__(self) -&gt; int:\n        \"\"\"\n        Returns the total number of samples in the dataset.\n\n        Returns:\n            int: The number of images in the dataset.\n        \"\"\"\n        if self._img_label_map is None:\n            self._init_img_label_map()\n\n        return len(self._img_label_map)\n\nI also define a custom Subset to make it easier to split the dataset and access the target labels of a subset:\n\nclass ClothesSubset(torch.utils.data.Subset):\n    \"\"\"\n    A subset of a `ClothesDataset` that maintains access to the dataset's target labels.\n\n    Args:\n        dataset (ClothesDataset): The `ClothesDataset` from which the subset is drawn.\n        indices (Sequence[int]): The indices of the elements in the original dataset to include in the subset.\n    \"\"\"\n\n    def __init__(\n        self, dataset: torch.utils.data.Dataset, indices: Sequence[int]\n    ) -&gt; None:\n        \"\"\"\n        Initialises the subset with the given dataset and a sequence of indices.\n\n        Args:\n            dataset (Dataset): The `ClothesDataset` to create a subset from.\n            indices (Sequence[int]): Indices specifying which elements of the dataset\n                should be included in this subset.\n        \"\"\"\n        super().__init__(dataset, indices)\n\n    def targets(self, indices: Optional[Iterable[int]] = None):\n        \"\"\"\n        Returns the target labels for the specified indices, or for the entire subset if no indices are provided.\n\n        This method is designed to work specifically with `ClothesDataset` and also handles\n        the case where the subset is a subset of another subset.\n\n        Args:\n            indices (Optional[Iterable[int]]): Indices of the target labels to retrieve from the subset.\n                If no indices are provided, returns the labels for all items in the subset.\n\n        Yields:\n            The target labels corresponding to the specified indices in the subset.\n        \"\"\"\n        if indices is None:\n            for target in self.dataset.targets(self.indices):\n                yield target\n        else:\n            # In case we have a subset of a subset, use a set to handle index lookups efficiently.\n            indices_set = set(indices)\n            for idx, target in enumerate(self.dataset.targets(self.indices)):\n                if idx in indices_set:\n                    yield target\n\n\nWe can now load and explore the dataset.\nNote: I intentionally ignore the test split because it is unlabelled and only has 8 samples, which makes it unuseful for evaluation. In the code cell below, I combine the train and validation splits together — later in the notebook, I perform my own train-validation-test split on this combined dataset.\n\nds = ClothesDataset((os.path.join(DATASET_ROOT_PATH, split) for split in (\"train\", \"valid\")))\n\n\nprint(f\"Total number of samples: {len(ds)}\")\nprint(f\"Categories: {ds.categories}\")\n\n\nTotal number of samples: 3849\nCategories: {'shirts', 'shoes', 'shorts', 'accessories', 'knitwear', 'jackets', 'jeans', 'tees'}\n\n\n\nThe dataset has 3849 samples, which is relatively small compared to other image datasets.\n\n\nPreliminary Analysis\nTo start, I’ll plot some of the images to have a feel of what the images look like:\n\nrows = 4\ncols = 4\nfig, axes = plt.subplots(rows, cols, figsize=(2 * cols, 2 * rows))\nfor idx in range(rows * cols):\n    img, label = ds[idx * 200] # Multiply by 200 so that we don't just see shorts.\n    ax = axes[idx // cols, idx % cols]\n    ax.imshow(img.permute(1, 2, 0))\n    ax.set_title(f\"Label: {label}\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nSome observations:\n\nThe images are coloured, not grayscale.\nThe images appear to all be the same size (of course, this is just a small sample size of 16, so there’s no guarantee).\n\nThe dimensions of each image are likely 432 by 300:\n\n# Dimensions of the first image.\nprint(f\"Dimensions: {ds[0][0].shape}\")\n\n\nDimensions: torch.Size([3, 432, 300])\n\n\n\nIn case there are any images that have different dimensions (which would cause problems when feeding them to the models), we’ll resize all images to be the same size later. 432 by 300 is quite large and could potentially exhaust GPU memory during hyperparameter tuning and training, so the images will be resized to smaller dimensions (the dimensions will be tuned during hyperparameter tuning).\nNext, are the pixel values normalised? They do not seem to be. Instead, they seem to be in the typical range of 0 to 255. We can gain some assurance from the documentation for PyTorch’s read_image(): “The values of the output tensor are in uint8 in [0, 255] for most cases”. We will need to normalise the pixel values later.\n\nprint(f\"Pixel values of the first image:\\n{ds[0][0]}\")\n\n\nPixel values of the first image:\ntensor([[[242, 242, 242,  ..., 242, 242, 242],\n         [242, 242, 242,  ..., 242, 242, 242],\n         [242, 242, 242,  ..., 242, 242, 242],\n         ...,\n         [242, 242, 242,  ..., 242, 242, 242],\n         [242, 242, 242,  ..., 242, 242, 242],\n         [242, 242, 242,  ..., 242, 242, 242]],\n\n        [[242, 242, 242,  ..., 242, 242, 242],\n         [242, 242, 242,  ..., 242, 242, 242],\n         [242, 242, 242,  ..., 242, 242, 242],\n         ...,\n         [242, 242, 242,  ..., 242, 242, 242],\n         [242, 242, 242,  ..., 242, 242, 242],\n         [242, 242, 242,  ..., 242, 242, 242]],\n\n        [[242, 242, 242,  ..., 242, 242, 242],\n         [242, 242, 242,  ..., 242, 242, 242],\n         [242, 242, 242,  ..., 242, 242, 242],\n         ...,\n         [242, 242, 242,  ..., 242, 242, 242],\n         [242, 242, 242,  ..., 242, 242, 242],\n         [242, 242, 242,  ..., 242, 242, 242]]], dtype=torch.uint8)\n\n\n\nUnfortunately, looking at the distribution of the classes, there is an imbalanced class problem — the “shoes” and “tees” classes have a much higher number of samples than the others. This could affect the performance of the models and make them biased towards these dominant classes. Thankfully, the other classes are fairly balanced, so we can resolve the issue by undersampling “shoes” and “tees”. This will lead to less training data, however, so we will try out various data augmentations (e.g., horizontal flips, colour jitter) later during hyperparameter tuning to introduce more varied data.\n\nsns.displot(list(ds.targets()), height=8, aspect=10/8)\n\n\n/opt/conda/lib/python3.10/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n\n\n\n\n\n\n\n\n\n\n\n\nPreprocessing\n\nNormalisation and Label Encoding\nNow, to prepare the data for use, we’ll first normalise the pixel values. The raw images have pixel values that range from 0 to 255, so we can normalise them by simply dividing each value by 255:\n\n# Normalise.\ndef input_transform(X):\n  return X / 255\n\nds.input_transform = input_transform\n\nTo confirm that the pixel values were normalised correctly, we’ll inspect the pixel values for one of the images:\n\nprint(f\"Pixel values:\\n{ds[0][0]}\")\n\n\nPixel values:\ntensor([[[0.9490, 0.9490, 0.9490,  ..., 0.9490, 0.9490, 0.9490],\n         [0.9490, 0.9490, 0.9490,  ..., 0.9490, 0.9490, 0.9490],\n         [0.9490, 0.9490, 0.9490,  ..., 0.9490, 0.9490, 0.9490],\n         ...,\n         [0.9490, 0.9490, 0.9490,  ..., 0.9490, 0.9490, 0.9490],\n         [0.9490, 0.9490, 0.9490,  ..., 0.9490, 0.9490, 0.9490],\n         [0.9490, 0.9490, 0.9490,  ..., 0.9490, 0.9490, 0.9490]],\n\n        [[0.9490, 0.9490, 0.9490,  ..., 0.9490, 0.9490, 0.9490],\n         [0.9490, 0.9490, 0.9490,  ..., 0.9490, 0.9490, 0.9490],\n         [0.9490, 0.9490, 0.9490,  ..., 0.9490, 0.9490, 0.9490],\n         ...,\n         [0.9490, 0.9490, 0.9490,  ..., 0.9490, 0.9490, 0.9490],\n         [0.9490, 0.9490, 0.9490,  ..., 0.9490, 0.9490, 0.9490],\n         [0.9490, 0.9490, 0.9490,  ..., 0.9490, 0.9490, 0.9490]],\n\n        [[0.9490, 0.9490, 0.9490,  ..., 0.9490, 0.9490, 0.9490],\n         [0.9490, 0.9490, 0.9490,  ..., 0.9490, 0.9490, 0.9490],\n         [0.9490, 0.9490, 0.9490,  ..., 0.9490, 0.9490, 0.9490],\n         ...,\n         [0.9490, 0.9490, 0.9490,  ..., 0.9490, 0.9490, 0.9490],\n         [0.9490, 0.9490, 0.9490,  ..., 0.9490, 0.9490, 0.9490],\n         [0.9490, 0.9490, 0.9490,  ..., 0.9490, 0.9490, 0.9490]]])\n\n\n\nNext, because our models can only understand numbers, we need to encode the labels (i.e., the categories) into numbers. I use scikit-learn’s LabelEncoder for this:\n\ntarget_le = LabelEncoder()\ntarget_le.fit(list(ds.categories))\n\ndef target_transform(y):\n  encoded = target_le.transform([y])\n  return encoded[0]\n\nds.target_transform = target_transform\n\nWe’ll check the label encoder’s classes and ensure that we have 8 unique integers for the labels:\n\ntarget_le.classes_\n\n\narray(['accessories', 'jackets', 'jeans', 'knitwear', 'shirts', 'shoes',\n       'shorts', 'tees'], dtype='&lt;U11')\n\n\n\n\nset(ds.targets())\n\n\n{0, 1, 2, 3, 4, 5, 6, 7}\n\n\n\n\n\nSplitting the Dataset\nAs briefly mentioned earlier, I do not use the test split provided by the dataset as it only contains 8 samples and does not contain labels. I instead perform my own train-validation-test split with a ratio of 70:15:15.\nThe test set will be used for the final evaluation of the models — we will not be touch this split at all during hyperparameter tuning and training to avoid data leakage, which would lead to overly optimistic results.\nThe validation set will be used to estimate the validation loss during training so that we can compare it to the training loss. This validation loss will also be used to stop training early when the generalisation loss stops improving to prevent the model from overfitting.\nThe training set will, of course, be used for training. However, it will also be used for hyperparameter tuning. Note that the validation set will not be used for hyperparameter tuning. I prefer to use K-fold cross-validation to further split the training set into train and validation folds during hyperparameter tuning to get a more unbiased estimate of model performance — if I re-use the validation set for hyperparameter tuning, there is a risk of overfitting to it, especially since the dataset is rather small.\n\nindices_train, indices_other = sklearn.model_selection.train_test_split(\n    np.arange(len(ds)),\n    train_size=0.7,\n    random_state=0,\n    shuffle=True,\n    stratify=np.fromiter(ds.targets(), dtype=int),\n)\n\nindices_val, indices_test = sklearn.model_selection.train_test_split(\n    indices_other,\n    train_size=0.5,\n    random_state=0,\n    shuffle=True,\n    stratify=np.fromiter(ds.targets(indices_other), dtype=int),\n)\n\nds_train = ClothesSubset(ds, indices_train)\nds_val = ClothesSubset(ds, indices_val)\nds_test = ClothesSubset(ds, indices_test)\n\n\nprint(f\"Size of train split: {len(ds_train)}\")\n\n\nSize of train split: 2694\n\n\n\n\nprint(f\"Size of validation split: {len(ds_val)}\")\n\n\nSize of validation split: 577\n\n\n\n\nprint(f\"Size of test split: {len(ds_test)}\")\n\n\nSize of test split: 578\n\n\n\n\n\nUndersampling\nAs mentioned earlier, the dataset has a class imbalance problem, where “shoes” and “tees” have significantly more samples than the other classes. To combat this issue, we will undersample the “shoes” and “tees” classes by using a WeightedRandomSampler to give smaller weights (i.e., lower probability of being sampled) to their instances. This sampler will be passed to a DataLoader later during training and hyperparameter tuning.\nNote: The weights are determined only from the train split. Otherwise, we would be leaking data from the test and validation splits. We also need to be careful during hyperparameter tuning to not use the weights determined by the whole train split since each iteration of K-fold cross-validation will have its own train set. Hence, I define a function to create an undersampler from specified training labels so that each iteration of K-fold cross-validation can pass its own train split and create its own undersampler:\n\ndef make_undersampler(labels_train: torch.Tensor):\n    # Count the number of samples in each class\n    # of the train split.\n    class_counts = torch.bincount(labels_train)\n\n    # Use the inverses of the class counts as the weights\n    # so that \"shoes\" and \"tees\" have lower weights.\n    class_weights = 1. / class_counts.float()\n\n    # Assign weights to each sample.\n    sample_weights = class_weights[labels_train]\n\n    return torch.utils.data.WeightedRandomSampler(\n        weights=sample_weights,\n        num_samples=len(sample_weights),\n        replacement=True\n    )\n\nundersampler = make_undersampler(torch.tensor(np.fromiter(ds_train.targets(), dtype=int)))\n\nThe undersampler created here (undersampler) is only for the real training process that will use the entire train split. Each iteration of K-fold cross-validation during hyperparameter tuning will call the make_undersampler() function with its own train set to create its own undersampler."
  },
  {
    "objectID": "notebooks/mlp-vgg16-fashion.html#model-definitions",
    "href": "notebooks/mlp-vgg16-fashion.html#model-definitions",
    "title": "A Comparison of Multilayer Perceptron and VGG-16 Models for Fashion Image Classification",
    "section": "Model Definitions",
    "text": "Model Definitions\nThis section contains definitions for each model (MLP, VGG-16 and pretrained VGG-16).\nNote that the MLP and VGG-16 that I implemented from scratch use the lazy variants of PyTorch’s neural network modules, which defer the initialisation of layer parameters. The rationale is so that (a) I do not need to manually calculate the dimensions of the inputs and outputs for each layer, which is error-prone, and (b) the model will automatically detect the shape of the inputs. These make the model definitions much more readable and easier to prototype with. The trade-offs are that there is a slight overhead for initialising the layers during the first forward pass, and shape mismatch errors are detected later instead of during model initialisation.\nNote: For visualisations of the architectures of the final models, please see the Training and Evaluation section. The model architectures defined here are “semi-fixed” — some architectural decisions are treated as hyperparameters that need to be tuned, so the models are not instantiated until then.\n\nMultilayer Perceptron\nSince it is not obvious what MLP architecture would work best for the dataset, the MultilayerPercecptron class below provides a semi-fixed architecture and allows specifying the activation function to use and the number of layers and their corresponding sizes. These are treated as hyperparameters to be tuned.\nThis MLP model adds a batch normalisation layer after each linear layer to stabilise training, reduce dependency on input scaling and add a regularisation effect. Ideally, whether to use batch normalisation would have been a hyperparameter to tune, but I decided not to make it configurable to reduce the hyperparameter search space.\nOther architectural options I could have explored include: use of dropout layers, use of L2 regularisation and which weight initialisation technique to use. However, these were left unexplored to keep the hyperparameter search space small. MLPs are not well-suited for image tasks anyway (they do not consider the spacial structure of the pixels — you could randomly rearrange the pixels and the MLP would still yield the same performance results), so trying to explore this architectural space will likely be unfruitful.\nNote: Flattening of the input images is performed outside of the MultilayerPerceptron class so that the class can be reused for other inputs that do not require flattening.\n\nclass MultilayerPerceptron(torch.nn.Module):\n    def __init__(\n        self,\n        layer_sizes: Iterable[int],\n        activation_constructor: Callable[[], torch.nn.Module],\n    ):\n        super().__init__()\n        self.layers = torch.nn.ModuleList(\n            [\n                module\n                for out_features in layer_sizes[:-1]\n                for module in (\n                    torch.nn.LazyLinear(out_features),\n                    torch.nn.LazyBatchNorm1d(),\n                    activation_constructor(),\n                )\n            ]\n        )\n        self.layers.append(torch.nn.LazyLinear(layer_sizes[-1]))\n\n    def forward(self, x):\n        for layer in self.layers:\n            x = layer(x)\n        return x\n\nNote: For visualisation of the architecture of the final model, please see the Training and Evaluation section.\n\n\nVGG-16\nThe VGG-16 model follows the original VGG-16 architecture (configuration D) proposed in Very Deep Convultional Networks for Large-Scale Image Recognition:\n\n\n\nvgg16-architecture.png\n\n\nAlthough not depicted in the image above, the authors mention in the paper that dropout is applied to the first two linear layers with a probability of 0.5.\nWhile my implementation follows the original architecture closely, some minor adjustments / additions were made:\n\nWhen the model is instantiated, you can specify whether to use batch normalisation after each convolutional layer. This is a hyperparameter that will be tuned. During my initial tests, I found that the model struggled to learn the features of the input images without batch normalisation (the loss very quickly plateaued). This could be because VGG-16 is a considerably deep model and thus experiences the vanishing / exploding gradient problem — batch normalisation may help stabilise the gradients.\nThe VGG-16 model provided by PyTorch adds an additional adaptive average pooling layer before flattening the last feature map for the linear layers, presumably to allow the network to handle variable input image sizes by ensuring that the dimensions of the feature map before the linear layers is always the same. I replicate that addition in my implementation for the same reason.\n\nTo reduce code repetition, I define a function to create a single VGG block. This function is loosely based on a similar function from d2l.ai.\n\ndef vgg_block(n_conv_layers: int, out_channels: int, batch_norm: bool = True):\n    conv_layers = (\n        module\n        for _ in range(n_conv_layers)\n        for module in (\n            torch.nn.LazyConv2d(out_channels, kernel_size=3, padding=1),\n            torch.nn.LazyBatchNorm2d() if batch_norm else torch.nn.Identity(),\n            torch.nn.ReLU(inplace=True),\n        )\n    )\n\n    return torch.nn.Sequential(\n        *conv_layers, torch.nn.MaxPool2d(kernel_size=2, stride=2)\n    )\n\nThe implementation of VGG-16 is then as follows:\n\nclass VGG16(torch.nn.Module):\n    def __init__(self, out_classes: int, batch_norm: bool = True):\n        super().__init__()\n        self.convs = torch.nn.Sequential(\n            vgg_block(n_conv_layers=2, out_channels=64, batch_norm=batch_norm),\n            vgg_block(n_conv_layers=2, out_channels=128, batch_norm=batch_norm),\n            vgg_block(n_conv_layers=3, out_channels=256, batch_norm=batch_norm),\n            vgg_block(n_conv_layers=3, out_channels=512, batch_norm=batch_norm),\n            vgg_block(n_conv_layers=3, out_channels=512, batch_norm=batch_norm),\n        )\n        self.avg_pool = torch.nn.Sequential(\n            torch.nn.AdaptiveAvgPool2d((7, 7)),\n            torch.nn.Flatten(),\n        )\n        self.fcs = torch.nn.Sequential(\n            *(\n                module\n                for _ in range(2)\n                for module in (\n                    torch.nn.LazyLinear(out_features=4096),\n                    torch.nn.ReLU(inplace=True),\n                    torch.nn.Dropout(p=0.5),\n                )\n            ),\n            torch.nn.LazyLinear(out_features=out_classes)\n        )\n\n    def forward(self, X):\n        X = self.convs(X)\n        X = self.avg_pool(X)\n        return self.fcs(X)\n\n\n\nVGG-16 Pretrained\nFor the pretrained VGG-16 model, we still need to perform a bit of surgery:\n\nThe pretrained VGG-16 was trained to perform classification on ImageNet, which has 1000 different classes. We need to replace the last linear layer with one whose number of outputs matches the number of classes in our dataset.\nThe pretrained VGG-16 expects inputs with 3 channels, so the first convolutional layer expects that. Since I want to experiment with different numbers of input channels (e.g., grayscale images), I replace the first layer with a lazy convolutional layer (lazy means that the layer will automatically determine the number of input channels based on the first input).\n\nPerforming these adjustments does mean that we lose some of the pretrained weights. However, most of them are still kept, so there should not be any issues.\nThe following cell shows the untouched architecture of the pretrained VGG-16 model:\n\ntorchvision.models.vgg16(weights=\"DEFAULT\", progress=True)\n\n\nDownloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n100%|██████████| 528M/528M [00:02&lt;00:00, 205MB/s]\n\n\nVGG(\n  (features): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU(inplace=True)\n    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (3): ReLU(inplace=True)\n    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (6): ReLU(inplace=True)\n    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (8): ReLU(inplace=True)\n    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (11): ReLU(inplace=True)\n    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (13): ReLU(inplace=True)\n    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (15): ReLU(inplace=True)\n    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (18): ReLU(inplace=True)\n    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (20): ReLU(inplace=True)\n    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (22): ReLU(inplace=True)\n    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (25): ReLU(inplace=True)\n    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (27): ReLU(inplace=True)\n    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (29): ReLU(inplace=True)\n    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n  (classifier): Sequential(\n    (0): Linear(in_features=25088, out_features=4096, bias=True)\n    (1): ReLU(inplace=True)\n    (2): Dropout(p=0.5, inplace=False)\n    (3): Linear(in_features=4096, out_features=4096, bias=True)\n    (4): ReLU(inplace=True)\n    (5): Dropout(p=0.5, inplace=False)\n    (6): Linear(in_features=4096, out_features=1000, bias=True)\n  )\n)\n\n\n\nThe function below performs the surgery operations described above:\n\ndef make_vgg16_pretrained(weights: Optional[str] = \"DEFAULT\", freeze_conv: bool = False) -&gt; torchvision.models.vgg.VGG:\n    vgg16_pretrained = torchvision.models.vgg16(weights=weights, progress=True)\n\n    if freeze_conv:\n        # Freeze the convolutional layers.\n        for param in vgg16_pretrained.features.parameters():\n            param.requires_grad = False\n\n    vgg16_pretrained.features[0] = torch.nn.LazyConv2d(64, kernel_size=3, stride=1, padding=1)\n    vgg16_pretrained.classifier[6] = torch.nn.LazyLinear(10)\n    return vgg16_pretrained\n\nWe can verify that the function works as intended by manually inspecting the architecture of the output model:\n\nmake_vgg16_pretrained()\n\n\nVGG(\n  (features): Sequential(\n    (0): LazyConv2d(0, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU(inplace=True)\n    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (3): ReLU(inplace=True)\n    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (6): ReLU(inplace=True)\n    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (8): ReLU(inplace=True)\n    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (11): ReLU(inplace=True)\n    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (13): ReLU(inplace=True)\n    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (15): ReLU(inplace=True)\n    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (18): ReLU(inplace=True)\n    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (20): ReLU(inplace=True)\n    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (22): ReLU(inplace=True)\n    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (25): ReLU(inplace=True)\n    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (27): ReLU(inplace=True)\n    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (29): ReLU(inplace=True)\n    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n  (classifier): Sequential(\n    (0): Linear(in_features=25088, out_features=4096, bias=True)\n    (1): ReLU(inplace=True)\n    (2): Dropout(p=0.5, inplace=False)\n    (3): Linear(in_features=4096, out_features=4096, bias=True)\n    (4): ReLU(inplace=True)\n    (5): Dropout(p=0.5, inplace=False)\n    (6): LazyLinear(in_features=0, out_features=10, bias=True)\n  )\n)"
  },
  {
    "objectID": "notebooks/mlp-vgg16-fashion.html#definitions-for-training-and-evaluation",
    "href": "notebooks/mlp-vgg16-fashion.html#definitions-for-training-and-evaluation",
    "title": "A Comparison of Multilayer Perceptron and VGG-16 Models for Fashion Image Classification",
    "section": "Definitions for Training and Evaluation",
    "text": "Definitions for Training and Evaluation\nBefore proceeding further, we’ll define the necessary functions and classes needed ahead of time for hyperparameter tuning, training and evaluation so that the notebook is more readable. I suggest reading the documentation for each class to get a better understanding of their capabilities.\nFirst, for evaluation:\n\n@dataclass\nclass EvaluationResults:\n    \"\"\"\n    Stores the evaluation results of a model.\n\n    Attributes:\n        mean_loss (float): The mean loss across the evaluation dataset.\n        classification_report (dict): A dictionary containing precision, recall,\n            f1-score, and support for each class. See scikit-learn's `classification_report()`.\n        classification_report_str (str): A string representation of the classification report.\n            See scikit-learn's `classification_report()`.\n        confusion_matrix (numpy.ndarray): A confusion matrix indicating the performance\n            of the model on the evaluation dataset. See scikit-learn's `confusion_matrix()`.\n        predictions (numpy.ndarray): An array of predicted labels for the evaluation dataset.\n    \"\"\"\n\n    mean_loss: float\n    classification_report: dict\n    classification_report_str: str\n    confusion_matrix: np.ndarray\n    predictions: np.ndarray\n\n\nclass Evaluator:\n    \"\"\"\n    Evaluator for evaluating machine learning models on a dataset.\n\n    Attributes:\n        batch_size (int): The size of the batches used during evaluation. Default is\n            set to `DEFAULT_BATCH_SIZE`.\n        desc (str): A short description label for the evaluation process used for\n            progress tracking with tqdm. Defaults to \"test\".\n        zero_division (str or int): Controls the behavior of metrics when there is\n            a zero division. Can be \"warn\", 0 or 1. Defaults to \"warn\".\n        device (str): The device on which to perform evaluation.\n    \"\"\"\n\n    def __init__(\n        self,\n        batch_size: int = DEFAULT_BATCH_SIZE,\n        desc: str = \"test\",\n        zero_division=\"warn\",\n        device: str = DEVICE,\n    ):\n        \"\"\"\n        Initialises the Evaluator with the specified parameters.\n\n        Args:\n            batch_size (int): The size of the batches to use during evaluation.\n            desc (str): A description for the evaluation process.\n            zero_division (str or int): Specifies the behavior when there is a zero division.\n            device (str): The device for evaluation.\n        \"\"\"\n        self.batch_size: int = batch_size\n        self.desc = desc\n        self.zero_division = zero_division\n        self.device: str = device\n\n    def evaluate(\n        self,\n        model: torch.nn.Module,\n        ds_test: Dataset,\n        loss_fn: torch.nn.Module,\n        transform: Optional[torchvision.transforms.v2.Transform] = None,\n        target_labels: Optional[Iterable[str]] = None,\n    ) -&gt; EvaluationResults:\n        \"\"\"\n        Evaluates the specified model on the given test dataset.\n\n        Args:\n            model (torch.nn.Module): The model to be evaluated.\n            ds_test (Dataset): The test dataset to evaluate the model on.\n            loss_fn (torch.nn.Module): The loss function to compute the loss.\n            transform (Optional[torchvision.transforms.v2.Transform]): Optional\n                transformations to apply to the input data before passing to the model.\n            target_labels (Optional[Iterable[str]]): Optional list of target labels\n                for classification report.\n\n        Returns:\n            EvaluationResults: Contains the evaluation metrics and predictions.\n        \"\"\"\n        model.eval()\n\n        dl_test = DataLoader(ds_test, batch_size=self.batch_size, shuffle=True)\n\n        # Sum of the losses from each batch for the current epoch.\n        # Used to calculate the mean loss for the current epoch.\n        total_loss: float = 0\n        with torch.no_grad(), tqdm(total=len(dl_test.dataset)) as pbar:\n            # Tensor of all predictions.\n            truth_all: torch.Tensor = torch.tensor([], dtype=int)\n            pred_all: torch.Tensor = torch.tensor([], dtype=int)\n\n            for batch, (X, y) in enumerate(dl_test):\n                pbar.set_description(f\"{self.desc} [batch {batch + 1}]\")\n\n                truth_all = torch.cat((truth_all, y))\n\n                X = X.to(self.device)\n                y = y.to(self.device)\n\n                if transform is not None:\n                    X = transform(X)\n\n                unnormalised_logits = model(X)\n                pred_indices = unnormalised_logits.argmax(dim=1)\n                pred_all = torch.cat((pred_all, pred_indices.cpu()))\n\n                loss = loss_fn(unnormalised_logits, y)\n\n                del X\n                del y\n\n                total_loss += loss.item()\n\n                pbar.update(dl_test.batch_size)\n                pbar.set_postfix({\"loss\": loss.item()})\n\n            gc.collect()\n            torch.cuda.empty_cache()\n\n            mean_loss: float = total_loss / len(dl_test)\n\n            report_kwargs = {\"output_dict\": True}\n            if target_labels is not None:\n                report_kwargs[\"target_names\"] = target_labels\n            report = sklearn.metrics.classification_report(\n                truth_all.cpu(),  # sklearn can only work with CPU tensors.\n                pred_all.cpu(),\n                zero_division=self.zero_division,\n                **report_kwargs,\n            )\n\n            pbar.set_postfix(\n                {\"mean loss\": mean_loss, \"macro f1\": report[\"macro avg\"][\"f1-score\"]}\n            )\n\n        report_str_kwargs = {}\n        if target_labels is not None:\n            report_str_kwargs[\"target_names\"] = target_labels\n\n        report_str = sklearn.metrics.classification_report(\n            truth_all.cpu(),  # sklearn can only work with CPU tensors.\n            pred_all.cpu(),\n            zero_division=self.zero_division,\n            **report_str_kwargs,\n        )\n\n        confusion_matrix = sklearn.metrics.confusion_matrix(\n            truth_all.cpu(),  # sklearn can only work with CPU tensors.\n            pred_all.cpu(),\n        )\n\n        return EvaluationResults(\n            mean_loss,\n            report,\n            report_str,\n            confusion_matrix,\n            pred_all.detach().cpu().numpy(),\n        )\n\nFor training, I use early stopping based on the validation loss to prevent overfitting and save time if the model’s performance starts to plateau. If the observed validation loss does not improve after a specified number of consecutive epochs (patience), the training process is stopped. The delta parameter specifies the minimum increase in validation loss for it to be counted as a lack of improvement.\n\nclass ValidationLossEarlyStopper:\n    \"\"\"\n    Implements early stopping based on validation loss to prevent overfitting\n    during model training.\n\n    The training process is halted when no improvement in validation loss is observed\n    for a specified number of consecutive epochs (patience).\n    \"\"\"\n\n    def __init__(self, patience: int = 1, delta: float = 0.0):\n        \"\"\"\n        Initialises the early stopper with the specified patience and delta values.\n\n        Args:\n            patience (int): Number of epochs to wait for an improvement before stopping.\n                Default is 1.\n            delta (float): The minimum increase in validation loss required to count as a\n                deterioration. Default is 0.0.\n        \"\"\"\n        self._patience: int = patience\n        self._delta: float = delta\n        self._lapse_count: int = 0\n        self._min_val_loss: float = math.inf\n\n    def should_stop(self, val_loss: float) -&gt; bool:\n        \"\"\"\n        Checks whether training should stop based on the given validation loss.\n\n        Args:\n            val_loss (float): The current validation loss for the current epoch.\n\n        Returns:\n            bool: True if the training process should stop due to lack of improvement in\n            validation loss; otherwise, False.\n        \"\"\"\n        if val_loss &lt; self._min_val_loss:\n            self._min_val_loss = val_loss\n            self._lapse_count = 0\n        elif val_loss &gt; (self._min_val_loss + self._delta):\n            # Validation loss went up by more than delta from the min.\n            self._lapse_count += 1\n\n            # If the number of times the validation loss got worse\n            # exceeds the patience level, then we should stop.\n            if self._lapse_count &gt;= self._patience:\n                return True\n        return False\n\nFinally, for the actual training of the models, I define a Trainer class. Notably, the Trainer class allows specifying transforms for data augmentation — this will be used to introduce more varied data.\n\nParamsT = Iterable[torch.Tensor] | Iterable[dict[str, Any]]\n\n\nclass Trainer:\n    \"\"\"\n    Trains a PyTorch model over multiple epochs.\n\n    Attributes:\n        epochs (int): The total number of epochs to train the model.\n        batch_size (int): The number of samples per gradient update.\n        augment_transform (Optional[torchvision.transforms.v2.Transform]): Transformations to apply\n            for data augmentation during training.\n        train_loss_hook (Optional[Callable[[float, int], None]]): A callback function that is called\n            after each training epoch to log the training loss.\n        val_results_hook (Optional[Callable[[EvaluationResults, int], None]]): A callback function that\n            is called after each validation epoch to log validation results.\n        device (str): The device to run the model on.\n    \"\"\"\n\n    def __init__(\n        self,\n        epochs: int,\n        batch_size: int = DEFAULT_BATCH_SIZE,\n        augment_transform: Optional[torchvision.transforms.v2.Transform] = None,\n        train_loss_hook: Optional[Callable[[float, int], None]] = None,\n        val_results_hook: Optional[Callable[[EvaluationResults, int], None]] = None,\n        device: str = DEVICE,\n    ):\n        \"\"\"\n        Initialises the Trainer with the specified parameters.\n\n        Args:\n            epochs (int): The total number of epochs to train the model.\n            batch_size (int): The number of samples per gradient update. Default is `DEFAULT_BATCH_SIZE`.\n            augment_transform (Optional[torchvision.transforms.v2.Transform]): Transformations to apply\n                for data augmentation during training. Default is None.\n            train_loss_hook (Optional[Callable[[float, int], None]]): A callback function to log\n                training loss after each epoch. Default is None.\n            val_results_hook (Optional[Callable[[EvaluationResults, int], None]]): A callback function\n                to log validation results after each epoch. Default is None.\n            device (str): The device to run the model on. Default is `DEVICE`.\n        \"\"\"\n        self.epochs: int = epochs\n        self.batch_size: int = batch_size\n\n        self.augment_transform: Optional[torchvision.transforms.v2.Transform] = (\n            augment_transform\n        )\n        self.train_loss_hook: Optional[Callable[[float, int], None]] = train_loss_hook\n        self.val_results_hook: Optional[Callable[[EvaluationResults, int], None]] = (\n            val_results_hook\n        )\n        self.device: str = device\n\n    def train(\n        self,\n        model: torch.nn.Module,\n        ds_train: Dataset,\n        loss_fn: torch.nn.Module,\n        optimiser: torch.optim.Optimizer,\n        train_sampler: Optional[torch.utils.data.Sampler] = None,\n        transform: Optional[torchvision.transforms.v2.Transform] = None,\n        ds_val: Optional[Dataset] = None,\n        scheduler: Optional[torch.optim.lr_scheduler.ReduceLROnPlateau] = None,\n        early_stop: Optional[ValidationLossEarlyStopper] = None,\n    ):\n        \"\"\"\n        Trains the given model for a specified number of epochs.\n\n        Args:\n            model (torch.nn.Module): The PyTorch model to train.\n            ds_train (Dataset): The training dataset.\n            loss_fn (torch.nn.Module): The loss function to use for training.\n            optimiser (torch.optim.Optimizer): The optimiser for updating model weights.\n            train_sampler (Optional[torch.utils.data.Sampler]): An optional sampler for the training dataset.\n            transform (Optional[torchvision.transforms.v2.Transform]): Optional transformations to apply\n                to the training data before feeding them to the model.\n            ds_val (Optional[Dataset]): An optional validation dataset for validating the model.\n            scheduler (Optional[torch.optim.lr_scheduler.ReduceLROnPlateau]): Optional learning rate scheduler\n                to adjust the learning rate based on validation loss.\n            early_stop (Optional[ValidationLossEarlyStopper]): An optional early stopper to terminate training\n                if validation loss does not improve.\n        \"\"\"\n        model.to(self.device)\n\n        # Create the data loader for the training set.\n        # If a sampler was specified, use it.\n        dl_train = DataLoader(\n            ds_train,\n            batch_size=self.batch_size,\n            sampler=train_sampler,\n        )\n\n        # Training loop.\n        for epoch in range(self.epochs):\n            train_loss = self._train_one_epoch(\n                model,\n                dl_train,\n                loss_fn,\n                optimiser,\n                epoch,\n                transform=transform,\n            )\n\n            if self.train_loss_hook is not None:\n                self.train_loss_hook(train_loss, epoch)\n\n            if ds_val is not None:\n                val_results = self._validate_one_epoch(\n                    model,\n                    ds_val,\n                    loss_fn,\n                    epoch,\n                    transform=transform,\n                )\n\n                if self.val_results_hook is not None:\n                    self.val_results_hook(val_results, epoch)\n\n                if early_stop is not None and early_stop.should_stop(\n                    val_results.mean_loss\n                ):\n                    print(f\"Early stopping at epoch {epoch + 1}\")\n                    break\n\n                if scheduler is not None:\n                    scheduler.step(val_results.mean_loss)\n\n    def _train_one_epoch(\n        self,\n        model: torch.nn.Module,\n        dl_train: DataLoader,\n        loss_fn: torch.nn.Module,\n        optimiser: torch.optim.Optimizer,\n        epoch: int,\n        transform: Optional[torchvision.transforms.v2.Transform] = None,\n    ):\n        \"\"\"\n        Trains the given model for one epoch on the provided dataloader.\n\n        Args:\n            model (torch.nn.Module): The model to train.\n            dl_train (DataLoader): The dataloader for the training dataset.\n            loss_fn (torch.nn.Module): The loss function to calculate the loss.\n            optimiser (torch.optim.Optimizer): The optimiser for updating weights.\n            epoch (int): The current epoch number.\n            transform (Optional[torchvision.transforms.v2.Transform]): Optional transformations to apply\n                to the training data before feeding them to the model.\n\n        Returns:\n            float: The mean loss for the training epoch.\n        \"\"\"\n        model.train()\n\n        # Sum of the losses from each batch for the current epoch.\n        # Used to calculate the mean loss for the current epoch.\n        total_loss: float = 0\n\n        with tqdm(total=len(dl_train.dataset)) as pbar:\n            for batch, (X, y) in enumerate(dl_train):\n                pbar.set_description(f\"train [epoch {epoch + 1} batch {batch + 1}]\")\n\n                X = X.to(self.device)\n                y = y.to(self.device)\n\n                if self.augment_transform is not None:\n                    X = self.augment_transform(X)\n\n                if transform is not None:\n                    X = transform(X)\n\n                pred = model(X)\n                loss = loss_fn(pred, y)\n\n                del X\n                del y\n\n                # Backpropagation.\n                loss.backward()\n                optimiser.step()\n                optimiser.zero_grad()\n\n                total_loss += loss.item()\n\n                pbar.update(dl_train.batch_size)\n                pbar.set_postfix({\"train loss\": loss.item()})\n\n            mean_loss = total_loss / len(dl_train)\n            pbar.set_postfix({\"mean train loss\": mean_loss})\n\n        gc.collect()\n        torch.cuda.empty_cache()\n\n        return mean_loss\n\n    def _validate_one_epoch(\n        self,\n        model: torch.nn.Module,\n        ds_val: Dataset,\n        loss_fn: torch.nn.Module,\n        epoch: int,\n        transform: Optional[torchvision.transforms.v2.Transform] = None,\n    ) -&gt; EvaluationResults:\n        \"\"\"\n        Validates the model for one epoch on the given validation dataset.\n\n        Args:\n            model (torch.nn.Module): The model to validate.\n            ds_val (Dataset): The validation dataset.\n            loss_fn (torch.nn.Module): The loss function to calculate the loss.\n            epoch (int): The current epoch number.\n            transform (Optional[torchvision.transforms.v2.Transform]): Optional transformations to apply\n                to the validation data before feeding them to the model.\n\n        Returns:\n            EvaluationResults: The results of the validation.\n        \"\"\"\n        evaluator = Evaluator(\n            batch_size=self.batch_size,\n            desc=\"validate\",\n            zero_division=0,\n            device=self.device,\n        )\n\n        return evaluator.evaluate(\n            model,\n            ds_val,\n            loss_fn,\n            transform=transform,\n        )\n\nNow, to prepare for hyperparameter tuning, I define some convenience functions that each accepts a dictionary of hyperparameters determined by Optuna during hyperparameter tuning:\n\ndef augmentation_transforms_from_params(params: dict) -&gt; Optional[torchvision.transforms.v2.Transform]:\n    \"\"\"\n    Returns the augmentation transforms according to the given hyperparameter dictionary.\n    \"\"\"\n    transforms = []\n\n    if params[\"jitter\"]:\n        transforms.append(\n            torchvision.transforms.v2.ColorJitter(\n                brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1\n            )\n        )\n\n    if params[\"horizontal_flip\"]:\n        transforms.append(torchvision.transforms.v2.RandomHorizontalFlip())\n\n    if params[\"rotation\"]:\n        transforms.append(torchvision.transforms.v2.RandomRotation(30))\n\n    # Compose does not allow an empty list of transformations.\n    if not transforms:\n        return None\n\n    return torchvision.transforms.v2.Compose(transforms)\n\n\ndef transforms_from_params(params: dict) -&gt; torchvision.transforms.v2.Transform:\n    \"\"\"\n    Returns the preprocessing transformations according to the given hyperparameter dictionary.\n    \"\"\"\n    transforms = []\n    length = params[\"resize_length\"]\n    transforms.append(torchvision.transforms.v2.Resize((length, length)))\n\n    if params[\"grayscale\"]:\n        transforms.append(torchvision.transforms.v2.Grayscale())\n\n    return torchvision.transforms.v2.Compose(transforms)\n\nThe following function simply ties everything together into one function to reduce code duplication:\n\ndef train_eval_generic_params(\n    model: torch.nn.Module,\n    ds_train: torch.utils.data.Dataset,\n    ds_val: torch.utils.data.Dataset,\n    ds_test: torch.utils.data.Dataset,\n    sampler: torch.utils.data.Sampler,\n    params: dict,\n    train_loss_hook: Optional[Callable[[float, int], None]] = None,\n    val_results_hook: Optional[Callable[[EvaluationResults, int], None]] = None,\n    transforms_override: Optional[torchvision.transforms.v2.Transform] = None,\n    max_epochs: int = N_EPOCHS_TRAIN,\n    device: str = DEVICE,\n):\n    # Train.\n    trainer = Trainer(\n        epochs=max_epochs,\n        batch_size=params[\"batch_size\"],\n        augment_transform=augmentation_transforms_from_params(params),\n        train_loss_hook=train_loss_hook,\n        val_results_hook=val_results_hook,\n        device=device,\n    )\n\n    optimiser = torch.optim.SGD(\n        model.parameters(),\n        lr=params[\"lr\"],\n        weight_decay=params[\"weight_decay\"],\n        momentum=params[\"momentum\"],\n    )\n\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n        optimiser, factor=params[\"reduce_lr_factor\"]\n    )\n\n    transforms = transforms_override if transforms_override is not None else transforms_from_params(params)\n    trainer.train(\n        model,\n        ds_train,\n        loss_fn=torch.nn.CrossEntropyLoss(),\n        optimiser=optimiser,\n        train_sampler=sampler,\n        transform=transforms,\n        ds_val=ds_val,\n        scheduler=scheduler,\n        early_stop=ValidationLossEarlyStopper(\n            patience=params[\"early_stop_patience\"],\n            delta=0.0,\n        ),\n    )\n\n    # Evaluate.\n    evaluator = Evaluator(\n        batch_size=params[\"batch_size\"],\n        zero_division=0,\n        device=device,\n    )\n\n    return evaluator.evaluate(\n        model,\n        ds_test,\n        loss_fn=torch.nn.CrossEntropyLoss(),\n        transform=transforms,\n        target_labels=target_le.classes_,\n    )"
  },
  {
    "objectID": "notebooks/mlp-vgg16-fashion.html#hyperparameter-tuning",
    "href": "notebooks/mlp-vgg16-fashion.html#hyperparameter-tuning",
    "title": "A Comparison of Multilayer Perceptron and VGG-16 Models for Fashion Image Classification",
    "section": "Hyperparameter Tuning",
    "text": "Hyperparameter Tuning\nI use the Optuna library for hyperparameter tuning. Optuna uses Bayesian optimisation (at least by default — it also provides other optimisation methods, but I used Bayesian optimisation) to find the best set of hyperparameters. Based on the performance from past hyperparameter sets, it tries to select promising hyperparameters. Unlike the traditional grid search and random search, Bayesian optimisation can reduce the number of trials needed to find an optimal set of hyperparameters.\nWe first need to define an objective to optimise. I chose to maximise the macro average F1-score. A quick explanation of what macro average F1 is:\n\nPrecision measures the accuracy of positive predictions (i.e., high precision means that the model tends to be correct when it predicts a positive instance).\nRecall measures how well the model is able to identify positive instances (i.e., high recall means that model is able to identify most positive instances).\nThe F1 score is the harmonic mean of precision and recall, so it balances these two metrics into a single measure.\nThe macro average F1 score calculates the F1 score for each class independently and then takes the average, treating all classes equally. This is important in multi-class problems where class distribution may be imbalanced (which is the case for our dataset) as it ensures that the performance on each class contributes equally to the overall score.\n\nWhy macro average F1?\n\nIt is one of the metrics that will be used for final evaluation and has real-world implications.\nUnlike accuracy, the macro average F1-score takes into account both precision and recall, so maximising it will likely lead to the model being less biased towards specific classes.\nWe could minimise the loss, but that does not always mean that the model meets real-world objectives.\n\nAlthough the models have the same objective to maximise, they have different hyperparameters. Some of these hyperparameters are general and applicable to all of the models (e.g., learning rate and batch size). These general hyperparameters are:\n\nBatch size: The number of training samples to process in a single batch before updating the model’s parameters. Smaller batch sizes means more frequent updates and potentially better generalisation at the cost of longer training time. Larger batch sizes can stabilise the training process but may require more memory and may lead to poorer generalisation. See On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima for a more detailed explanation. Unfortunately, while I wanted to try larger batch sizes (64 and above), I frequently encountered scenarios in which the GPU ran out of memory; hence, the search space for the batch size is kept on the lower side (24 and 32).\nLearning rate: Controls the size of the steps for gradient updates. A low learning rate means updates are more precise but requires more epochs to get comparable performance. A high learning rate speeds up training at the cost of risking overshooting, where the optimal model parameters are constantly missed, potentially causing failure to converge.\nWeight decay: A regularisation parameter that prevents weights from getting too large, which helps to prevent overfitting. A higher weight decay can improve generalisation but may make learning too slow.\nLearning rate reduction factor: Factor by which to reduce the learning rate when validation performance starts to stagnate. A smaller factor allows for finer adjustments to the learning rate, which can help the model converge better towards the end of training.\nMomentum: A parameter for stochastic gradient descent that accelerates the gradient descent process using past gradients to smooth out updates, which can lead to faster convergence and prevent the model from getting stuck in local minima.\nEarly stop patience: The number of epochs to wait without improvement in validation performance before stopping training. A higher patience value allows more epochs for the model to improve but risks overfitting to the training set while a lower value may stop training prematurely if the model is slow to converge.\nColor jitter?: Whether to randomly alter the colours in the input images (e.g., perturb brightness, contrast, saturation and hue). Color jittering is useful as a data augmentation technique and can improve the model’s robustness to variations in lighting and color conditions.\nRandom horizontal flip?: Whether to randomly flip images horizontally during training. Serves as a data augmentation technique that may help the model become more generalisable with respect to left-right orientation. I kept the probability of randomly flipping an image to 0.5.\nRandom rotation?: Whether to randomly rotate input images. Serves as a data augmentation technique that may help the model generalise better with respect to orientation. In this case, I kept this is a simple Boolean toggle to reduce the search space and time — random rotations are always done within 30 degrees.\n\nFor the MLP and custom VGG-16 models, but not the pretrained VGG-16 model, the following additional hyperparameters were tuned:\n\nResize length: Specifies the target size for input images — images are resized to a square. Resizing makes input dimensions consistent and reduces computational load. However, if images are resized to dimensions that are too small, we may lose too much information. Unfortunately, I had to keep the maximum resize length rather small (224) because the GPU kept running out of memory.\nGrayscale?: Whether to convert images to grayscale. Using grayscale reduces input dimensionality and computational requirements, which is especially useful for the MLP model (since there are many fully-connected units). However, it removes colour information that could have been important.\n\nThe reason these hyperparameters are not tuned for the pretrained VGG-16 model is because we need to use the same transformations that were used when it was trained on ImageNet. These transformations can be accessed from the pretrained weights, as documented here.\nThe function below creates and returns an objective function that includes the logic for suggesting the general hyperparameters. The model_from_params argument should be a callable that creates a model with the hyperparameter suggestions. Since the dataset is quite small, the function uses K-fold cross-validation to get a more unbiased estimate of model performance — using a single validation set for hyperparameter tuning would risk overfitting.\nFinally, the hyperparameter tuning process uses median pruning, which compares the intermediate results of a trial (e.g., validation loss) against the median value of all previously completed trials at the same step. If a trial’s results is worse than the median, it is unlikely to improve on the previous trials, so the trial is pruned to save computational resources and time.\n\ndef make_objective(\n    suggest_model_params: Callable[[optuna.trial.Trial], None],\n    model_from_params: Callable[[dict], torch.nn.Module],\n    ds_train: torch.utils.data.Dataset,\n    prune: bool = PRUNE_TRIALS,\n    n_folds: int = N_FOLDS,\n    max_epochs: int = N_EPOCHS_TUNE,\n    transforms_override: Optional[torchvision.transforms.v2.Transform] = None,\n    device: str = DEVICE,\n) -&gt; Callable[[optuna.trial.Trial], tuple[float, float]]:\n    def objective(trial: optuna.trial.Trial) -&gt; tuple[float, float]:\n        # General hyperparameters.\n        trial.suggest_int(\"batch_size\", 24, 32, step=8)\n        trial.suggest_float(\"lr\", 1e-5, 1e-3)\n        trial.suggest_float(\"weight_decay\", 0, 1e-2)\n        trial.suggest_float(\"reduce_lr_factor\", 0.01, 0.1)\n        trial.suggest_float(\"momentum\", 0.8, 0.99)\n        trial.suggest_int(\"early_stop_patience\", 1, 3)\n\n        # Preprocessing input transforms.\n        if transforms_override is None:\n            trial.suggest_int(\"resize_length\", 32, 224, step=32)\n            trial.suggest_categorical(\"grayscale\", [True, False])\n\n        # Augmentation transforms.\n        trial.suggest_categorical(\"jitter\", [True, False])\n        trial.suggest_categorical(\"horizontal_flip\", [True, False])\n        trial.suggest_categorical(\"rotation\", [True, False])\n\n        suggest_model_params(trial)\n\n        print(f\"Trial {trial.number} params: {trial.params}\")\n\n        f1_scores: list[float] = []\n\n        # Use stratified K-fold cross-validation with 4 folds.\n        kfold_cv = sklearn.model_selection.StratifiedKFold(\n            n_splits=n_folds, shuffle=True, random_state=0\n        )\n        for fold_idx, (cv_indices_other, cv_indices_test) in enumerate(\n            kfold_cv.split(\n                X=np.arange(len(ds_train)),\n                y=np.fromiter(ds_train.targets(), dtype=int),\n            )\n        ):\n            print(f\"Trial {trial.number} fold {fold_idx + 1}\")\n\n            cv_ds_other = ClothesSubset(ds_train, cv_indices_other)\n            cv_ds_test = ClothesSubset(ds_train, cv_indices_test)\n\n            # The `other` set is 3 out of 4 folds.\n            # Use 80% of the `other` set as the training set.\n            # The remaining 20% are used as the validation set for early stopping.\n            # The 4th fold is used as the test set for evaluation.\n            # Note: These indices are indices into `cv_ds_other`.\n            cv_indices_train, cv_indices_val = sklearn.model_selection.train_test_split(\n                np.arange(len(cv_ds_other)),\n                train_size=0.8,\n                random_state=0,\n                shuffle=True,\n                stratify=np.fromiter(cv_ds_other.targets(), dtype=int),\n            )\n\n            cv_ds_train = ClothesSubset(cv_ds_other, cv_indices_train)\n            cv_ds_val = ClothesSubset(cv_ds_other, cv_indices_val)\n\n            # Create the undersampler using `cv_ds_train`'s labels.\n            # To avoid leakage, we must not use `ds_train`.\n            # The shape won't match anyway.\n            undersampler_cv_train = make_undersampler(\n                torch.tensor(np.fromiter(cv_ds_train.targets(), dtype=int))\n            )\n\n            # Hook onto the validation results for pruning trials.\n            def val_results_hook(results: EvaluationResults, epoch: int):\n                if not prune:\n                    return\n\n                if epoch == 4 and results.mean_loss &gt;= 1.5:\n                    raise optuna.TrialPruned(\"Mean validation loss is still above or equal to 1.5 at the 5th epoch.\")\n\n                step = fold_idx * max_epochs + epoch\n                f1 = results.classification_report[\"macro avg\"][\"f1-score\"]\n                trial.report(f1, step)\n                if trial.should_prune():\n                    raise optuna.TrialPruned()\n\n            model = model_from_params(trial.params)\n            results = train_eval_generic_params(\n                model=model,\n                ds_train=cv_ds_train,\n                ds_val=cv_ds_val,\n                ds_test=cv_ds_test,\n                sampler=undersampler_cv_train,\n                params=trial.params,\n                train_loss_hook=None,\n                val_results_hook=val_results_hook,\n                max_epochs=max_epochs,\n                transforms_override=transforms_override,\n            )\n\n            f1 = results.classification_report[\"macro avg\"][\"f1-score\"]\n            f1_scores.append(f1)\n\n            del model\n            gc.collect()\n            torch.cuda.empty_cache()\n\n        mean_f1_score = statistics.fmean(f1_scores)\n        return mean_f1_score\n\n    return objective\n\n\nMultilayer Perceptron\nThe hyperparameters for the MLP are as follows:\n\nNumber of layers: The number of layers the MLP has, including the input and output layers. A higher number of layers allows the model to capture more complex patterns. However, setting it too high can lead to overfitting, vanishing/exploding gradients and infeasible computational requirements.\nNumber of units in each layer: The number of neurons in a specific layer. Each layer has its number of units tuned (except the first and last layers since those are fixed), so they may not always have the same number of units. Having more units increases the model’s ability to learn complex patterns, but too many units can lead to overfitting or high computational requirements.\nActivation function: The activation function to use after each linear layer. Affects how well the network captures nonlinear relationships.\n\nI would have liked to further explore other hyperparameters, such as whether to use dropout and regularisation, but the search space was starting to get too large to reasonably explore in time.\n\ndef suggest_mlp_params(trial: optuna.trial.Trial):\n    n_layers: int = trial.suggest_int(\"mlp_n_layers\", 3, 5)\n\n    # Suggest size of the second layer.\n    # This layer is given a smaller size than the rest because\n    # it will be connected to every single pixel of the input image,\n    # which can lead to the model having too many parameters\n    # and hence exhaust GPU memory.\n    trial.suggest_categorical(\"mlp_layer_1_size\", [256, 512])\n\n    # Exclude the first and last layers since their sizes are predetermined.\n    # Also exclude the second layer since we already suggested a (smaller)\n    # size for it.\n    for idx in range(n_layers - 3):\n        trial.suggest_categorical(f\"mlp_layer_{idx + 2}_size\", [256, 512, 1024, 2048])\n\n    activation_f_str = trial.suggest_categorical(\n        \"mlp_activation_f\",\n        [\"relu\", \"sigmoid\", \"softplus\"],\n    )\n\n\ndef mlp_from_params(params: dict) -&gt; MultilayerPerceptron:\n    n_hidden_layers = params[\"mlp_n_layers\"] - 2\n    layer_sizes = []\n    for idx in range(n_hidden_layers):\n        layer_sizes.append(params[f\"mlp_layer_{idx + 1}_size\"])\n    layer_sizes.append(10)  # Final prediction layer.\n\n    activation_f = {\n        \"relu\": torch.nn.ReLU,\n        \"sigmoid\": torch.nn.Sigmoid,\n        \"softplus\": torch.nn.Softplus,\n    }[params[\"mlp_activation_f\"]]\n\n    return torch.nn.Sequential(\n        torch.nn.Flatten(),\n        MultilayerPerceptron(layer_sizes, activation_f)\n    )\n\n\nmlp_study = optuna.create_study(\n    study_name=\"MLP study\",\n    sampler=optuna.samplers.TPESampler(seed=0),\n    pruner=optuna.pruners.MedianPruner(n_startup_trials=3, n_warmup_steps=5),\n    directions=[\"maximize\"],\n    storage=optuna.storages.JournalStorage(\n        optuna.storages.journal.JournalFileBackend(\"mlp_journal.log\"),\n    ),\n    load_if_exists=True,\n)\n\nmlp_study.optimize(\n    make_objective(\n        suggest_mlp_params,\n        mlp_from_params,\n        ds_train,\n    ),\n    n_trials=N_TRIALS,\n    gc_after_trial=True\n)\n\n\n[I 2024-10-19 23:22:49,624] A new study created in Journal with name: MLP study\n\n\nTrial 0 params: {'batch_size': 32, 'lr': 0.0007180374727086954, 'weight_decay': 0.006027633760716439, 'reduce_lr_factor': 0.05903948646972072, 'momentum': 0.8804944118743919, 'early_stop_patience': 2, 'resize_length': 128, 'grayscale': False, 'jitter': False, 'horizontal_flip': False, 'rotation': True, 'mlp_n_layers': 3, 'mlp_layer_1_size': 512, 'mlp_activation_f': 'softplus'}\nTrial 0 fold 1\n\n\ntrain [epoch 1 batch 45]: : 1440it [00:04, 303.70it/s, mean train loss=1.02]\nvalidate [batch 12]: : 384it [00:01, 288.69it/s, mean loss=1.16, macro f1=0.455]\ntrain [epoch 2 batch 45]: : 1440it [00:04, 317.43it/s, mean train loss=0.718]\nvalidate [batch 12]: : 384it [00:01, 280.00it/s, mean loss=0.833, macro f1=0.551]\ntrain [epoch 3 batch 45]: : 1440it [00:04, 316.49it/s, mean train loss=0.639]\nvalidate [batch 12]: : 384it [00:01, 288.45it/s, mean loss=0.776, macro f1=0.637]\ntrain [epoch 4 batch 45]: : 1440it [00:04, 311.88it/s, mean train loss=0.577]\nvalidate [batch 12]: : 384it [00:01, 289.62it/s, mean loss=0.99, macro f1=0.557]\ntrain [epoch 5 batch 45]: : 1440it [00:04, 315.34it/s, mean train loss=0.552]\nvalidate [batch 12]: : 384it [00:01, 287.94it/s, mean loss=1, macro f1=0.565]\n\n\nEarly stopping at epoch 5\n\n\ntest [batch 29]: : 928it [00:02, 310.55it/s, mean loss=1.12, macro f1=0.561]\n\n\nTrial 0 fold 2\n\n\ntrain [epoch 1 batch 45]: : 1440it [00:04, 311.10it/s, mean train loss=1.04]\nvalidate [batch 12]: : 384it [00:01, 290.78it/s, mean loss=0.775, macro f1=0.553]\ntrain [epoch 2 batch 45]: : 1440it [00:04, 315.27it/s, mean train loss=0.759]\nvalidate [batch 12]: : 384it [00:01, 290.99it/s, mean loss=0.756, macro f1=0.597]\ntrain [epoch 3 batch 45]: : 1440it [00:04, 315.25it/s, mean train loss=0.649]\nvalidate [batch 12]: : 384it [00:01, 285.11it/s, mean loss=0.705, macro f1=0.61]\ntrain [epoch 4 batch 45]: : 1440it [00:04, 316.99it/s, mean train loss=0.637]\nvalidate [batch 12]: : 384it [00:01, 290.49it/s, mean loss=0.729, macro f1=0.612]\ntrain [epoch 5 batch 45]: : 1440it [00:05, 264.14it/s, mean train loss=0.649]\nvalidate [batch 12]: : 384it [00:01, 228.21it/s, mean loss=0.658, macro f1=0.645]\ntrain [epoch 6 batch 45]: : 1440it [00:05, 287.25it/s, mean train loss=0.584]\nvalidate [batch 12]: : 384it [00:01, 261.52it/s, mean loss=0.765, macro f1=0.643]\ntrain [epoch 7 batch 45]: : 1440it [00:04, 306.48it/s, mean train loss=0.58]\nvalidate [batch 12]: : 384it [00:01, 290.95it/s, mean loss=0.867, macro f1=0.622]\n\n\nEarly stopping at epoch 7\n\n\ntest [batch 29]: : 928it [00:02, 313.43it/s, mean loss=0.805, macro f1=0.619]\n\n\nTrial 0 fold 3\n\n\ntrain [epoch 1 batch 45]: : 1440it [00:04, 313.21it/s, mean train loss=1.02]\nvalidate [batch 12]: : 384it [00:01, 288.27it/s, mean loss=1.21, macro f1=0.417]\ntrain [epoch 2 batch 45]: : 1440it [00:04, 311.62it/s, mean train loss=0.736]\nvalidate [batch 12]: : 384it [00:01, 287.49it/s, mean loss=0.842, macro f1=0.618]\ntrain [epoch 3 batch 45]: : 1440it [00:04, 299.34it/s, mean train loss=0.713]\nvalidate [batch 12]: : 384it [00:01, 282.64it/s, mean loss=0.913, macro f1=0.595]\ntrain [epoch 4 batch 45]: : 1440it [00:04, 312.24it/s, mean train loss=0.637]\nvalidate [batch 12]: : 384it [00:01, 285.67it/s, mean loss=0.928, macro f1=0.498]\n\n\nEarly stopping at epoch 4\n\n\ntest [batch 29]: : 928it [00:02, 317.66it/s, mean loss=0.946, macro f1=0.498]\n[I 2024-10-19 23:24:42,002] Trial 0 finished with value: 0.5593087362010596 and parameters: {'batch_size': 32, 'lr': 0.0007180374727086954, 'weight_decay': 0.006027633760716439, 'reduce_lr_factor': 0.05903948646972072, 'momentum': 0.8804944118743919, 'early_stop_patience': 2, 'resize_length': 128, 'grayscale': False, 'jitter': False, 'horizontal_flip': False, 'rotation': True, 'mlp_n_layers': 3, 'mlp_layer_1_size': 512, 'mlp_activation_f': 'softplus'}. Best is trial 0 with value: 0.5593087362010596.\n\n\nTrial 1 params: {'batch_size': 32, 'lr': 0.0004668645686304026, 'weight_decay': 0.007805291762864555, 'reduce_lr_factor': 0.02064469832820399, 'momentum': 0.9215849940522295, 'early_stop_patience': 1, 'resize_length': 224, 'grayscale': True, 'jitter': False, 'horizontal_flip': False, 'rotation': False, 'mlp_n_layers': 4, 'mlp_layer_1_size': 512, 'mlp_layer_2_size': 2048, 'mlp_activation_f': 'softplus'}\nTrial 1 fold 1\n\n\ntrain [epoch 1 batch 45]: : 1440it [00:04, 318.07it/s, mean train loss=0.888]\nvalidate [batch 12]: : 384it [00:01, 282.53it/s, mean loss=0.801, macro f1=0.543]\ntrain [epoch 2 batch 45]: : 1440it [00:04, 317.27it/s, mean train loss=0.587]\nvalidate [batch 12]: : 384it [00:01, 287.42it/s, mean loss=0.753, macro f1=0.643]\ntrain [epoch 3 batch 45]: : 1440it [00:04, 317.14it/s, mean train loss=0.465]\nvalidate [batch 12]: : 384it [00:01, 260.07it/s, mean loss=0.68, macro f1=0.615]\ntrain [epoch 4 batch 45]: : 1440it [00:05, 287.72it/s, mean train loss=0.419]\nvalidate [batch 12]: : 384it [00:01, 230.10it/s, mean loss=0.738, macro f1=0.643]\n\n\nEarly stopping at epoch 4\n\n\ntest [batch 29]: : 928it [00:03, 256.15it/s, mean loss=0.876, macro f1=0.594]\n\n\nTrial 1 fold 2\n\n\ntrain [epoch 1 batch 45]: : 1440it [00:05, 268.15it/s, mean train loss=0.998]\nvalidate [batch 12]: : 384it [00:01, 287.02it/s, mean loss=0.824, macro f1=0.465]\ntrain [epoch 2 batch 45]: : 1440it [00:04, 313.98it/s, mean train loss=0.641]\nvalidate [batch 12]: : 384it [00:01, 284.55it/s, mean loss=0.717, macro f1=0.569]\ntrain [epoch 3 batch 45]: : 1440it [00:04, 320.72it/s, mean train loss=0.512]\nvalidate [batch 12]: : 384it [00:01, 259.50it/s, mean loss=0.631, macro f1=0.689]\ntrain [epoch 4 batch 45]: : 1440it [00:04, 315.79it/s, mean train loss=0.426]\nvalidate [batch 12]: : 384it [00:01, 286.77it/s, mean loss=0.66, macro f1=0.683]\n\n\nEarly stopping at epoch 4\n\n\ntest [batch 29]: : 928it [00:02, 315.31it/s, mean loss=0.701, macro f1=0.635]\n\n\nTrial 1 fold 3\n\n\ntrain [epoch 1 batch 45]: : 1440it [00:04, 318.63it/s, mean train loss=0.97]\nvalidate [batch 12]: : 384it [00:01, 288.29it/s, mean loss=0.858, macro f1=0.492]\ntrain [epoch 2 batch 45]: : 1440it [00:04, 318.71it/s, mean train loss=0.613]\nvalidate [batch 12]: : 384it [00:01, 289.92it/s, mean loss=0.738, macro f1=0.626]\ntrain [epoch 3 batch 45]: : 1440it [00:04, 321.49it/s, mean train loss=0.47]\nvalidate [batch 12]: : 384it [00:01, 289.81it/s, mean loss=1.17, macro f1=0.517]\n\n\nEarly stopping at epoch 3\n\n\ntest [batch 29]: : 928it [00:03, 302.28it/s, mean loss=1.05, macro f1=0.549]\n[I 2024-10-19 23:26:03,597] Trial 1 finished with value: 0.5927770480927247 and parameters: {'batch_size': 32, 'lr': 0.0004668645686304026, 'weight_decay': 0.007805291762864555, 'reduce_lr_factor': 0.02064469832820399, 'momentum': 0.9215849940522295, 'early_stop_patience': 1, 'resize_length': 224, 'grayscale': True, 'jitter': False, 'horizontal_flip': False, 'rotation': False, 'mlp_n_layers': 4, 'mlp_layer_1_size': 512, 'mlp_layer_2_size': 2048, 'mlp_activation_f': 'softplus'}. Best is trial 1 with value: 0.5927770480927247.\n\n\nTrial 2 params: {'batch_size': 24, 'lr': 0.00013763703467830477, 'weight_decay': 0.0031542835092418387, 'reduce_lr_factor': 0.04273396938483604, 'momentum': 0.9083373863793971, 'early_stop_patience': 2, 'resize_length': 224, 'grayscale': False, 'jitter': False, 'horizontal_flip': False, 'rotation': True, 'mlp_n_layers': 3, 'mlp_layer_1_size': 256, 'mlp_activation_f': 'softplus'}\nTrial 2 fold 1\n\n\ntrain [epoch 1 batch 60]: : 1440it [00:04, 294.02it/s, mean train loss=1.11]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 271.17it/s, mean loss=0.964, macro f1=0.481]\ntrain [epoch 2 batch 60]: : 1440it [00:04, 315.20it/s, mean train loss=0.832]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 272.37it/s, mean loss=0.863, macro f1=0.604]\ntrain [epoch 3 batch 60]: : 1440it [00:05, 281.28it/s, mean train loss=0.746]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 255.63it/s, mean loss=0.965, macro f1=0.572]\ntrain [epoch 4 batch 60]: : 1440it [00:05, 280.02it/s, mean train loss=0.71]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 269.84it/s, mean loss=0.818, macro f1=0.601]\ntrain [epoch 5 batch 60]: : 1440it [00:04, 295.38it/s, mean train loss=0.696]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 282.03it/s, mean loss=0.811, macro f1=0.607]\ntrain [epoch 6 batch 60]: : 1440it [00:04, 315.13it/s, mean train loss=0.647]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 268.79it/s, mean loss=0.995, macro f1=0.561]\ntrain [epoch 7 batch 60]: : 1440it [00:04, 314.14it/s, mean train loss=0.649]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 274.77it/s, mean loss=1.04, macro f1=0.519]\n\n\nEarly stopping at epoch 7\n\n\ntest [batch 38]: : 912it [00:02, 310.06it/s, mean loss=1.07, macro f1=0.551]\n\n\nTrial 2 fold 2\n\n\ntrain [epoch 1 batch 60]: : 1440it [00:04, 294.70it/s, mean train loss=1.15]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 264.40it/s, mean loss=0.867, macro f1=0.588]\ntrain [epoch 2 batch 60]: : 1440it [00:04, 312.73it/s, mean train loss=0.881]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 282.58it/s, mean loss=0.822, macro f1=0.628]\ntrain [epoch 3 batch 60]: : 1440it [00:05, 270.65it/s, mean train loss=0.762]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 268.46it/s, mean loss=0.819, macro f1=0.566]\ntrain [epoch 4 batch 60]: : 1440it [00:04, 291.69it/s, mean train loss=0.801]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 283.46it/s, mean loss=0.766, macro f1=0.6]\ntrain [epoch 5 batch 60]: : 1440it [00:04, 312.36it/s, mean train loss=0.723]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 276.49it/s, mean loss=0.817, macro f1=0.624]\ntrain [epoch 6 batch 60]: : 1440it [00:04, 309.39it/s, mean train loss=0.668]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 274.67it/s, mean loss=0.954, macro f1=0.53]\n\n\nEarly stopping at epoch 6\n\n\ntest [batch 38]: : 912it [00:02, 315.11it/s, mean loss=0.917, macro f1=0.574]\n\n\nTrial 2 fold 3\n\n\ntrain [epoch 1 batch 60]: : 1440it [00:05, 281.62it/s, mean train loss=1.12]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 277.43it/s, mean loss=0.96, macro f1=0.469]\ntrain [epoch 2 batch 60]: : 1440it [00:04, 292.89it/s, mean train loss=0.876]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 278.66it/s, mean loss=0.841, macro f1=0.628]\ntrain [epoch 3 batch 60]: : 1440it [00:04, 293.35it/s, mean train loss=0.777]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 279.71it/s, mean loss=0.799, macro f1=0.666]\ntrain [epoch 4 batch 60]: : 1440it [00:04, 293.06it/s, mean train loss=0.742]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 277.43it/s, mean loss=0.854, macro f1=0.664]\ntrain [epoch 5 batch 60]: : 1440it [00:04, 308.60it/s, mean train loss=0.672]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 269.23it/s, mean loss=1.23, macro f1=0.474]\n\n\nEarly stopping at epoch 5\n\n\ntest [batch 38]: : 912it [00:02, 308.90it/s, mean loss=1.23, macro f1=0.498]\n[I 2024-10-19 23:28:10,721] Trial 2 finished with value: 0.541029338155279 and parameters: {'batch_size': 24, 'lr': 0.00013763703467830477, 'weight_decay': 0.0031542835092418387, 'reduce_lr_factor': 0.04273396938483604, 'momentum': 0.9083373863793971, 'early_stop_patience': 2, 'resize_length': 224, 'grayscale': False, 'jitter': False, 'horizontal_flip': False, 'rotation': True, 'mlp_n_layers': 3, 'mlp_layer_1_size': 256, 'mlp_activation_f': 'softplus'}. Best is trial 1 with value: 0.5927770480927247.\n\n\nTrial 3 params: {'batch_size': 24, 'lr': 0.0008395654584238159, 'weight_decay': 0.0009609840789396307, 'reduce_lr_factor': 0.09788135185120563, 'momentum': 0.8890437283130633, 'early_stop_patience': 3, 'resize_length': 160, 'grayscale': True, 'jitter': True, 'horizontal_flip': True, 'rotation': False, 'mlp_n_layers': 3, 'mlp_layer_1_size': 256, 'mlp_activation_f': 'sigmoid'}\nTrial 3 fold 1\n\n\ntrain [epoch 1 batch 60]: : 1440it [00:05, 281.30it/s, mean train loss=1.39]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 272.19it/s, mean loss=1.15, macro f1=0.412]\ntrain [epoch 2 batch 60]: : 1440it [00:05, 282.88it/s, mean train loss=1.1]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 270.48it/s, mean loss=1.03, macro f1=0.522]\ntrain [epoch 3 batch 60]: : 1440it [00:05, 283.68it/s, mean train loss=0.973]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 267.16it/s, mean loss=0.966, macro f1=0.511]\ntrain [epoch 4 batch 60]: : 1440it [00:06, 239.50it/s, mean train loss=0.884]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 256.53it/s, mean loss=0.896, macro f1=0.559]\ntrain [epoch 5 batch 60]: : 1440it [00:06, 238.63it/s, mean train loss=0.893]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 214.66it/s, mean loss=0.859, macro f1=0.585]\ntrain [epoch 6 batch 60]: : 1440it [00:05, 260.83it/s, mean train loss=0.841]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 268.32it/s, mean loss=0.91, macro f1=0.594]\ntrain [epoch 7 batch 60]: : 1440it [00:05, 286.12it/s, mean train loss=0.802]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 270.27it/s, mean loss=0.854, macro f1=0.537]\ntrain [epoch 8 batch 60]: : 1440it [00:05, 283.84it/s, mean train loss=0.757]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 263.41it/s, mean loss=0.823, macro f1=0.573]\ntrain [epoch 9 batch 60]: : 1440it [00:05, 280.62it/s, mean train loss=0.722]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 239.77it/s, mean loss=0.795, macro f1=0.619]\ntrain [epoch 10 batch 60]: : 1440it [00:05, 281.28it/s, mean train loss=0.691]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 265.73it/s, mean loss=0.88, macro f1=0.563]\ntrain [epoch 11 batch 60]: : 1440it [00:05, 278.44it/s, mean train loss=0.701]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 259.96it/s, mean loss=0.786, macro f1=0.592]\ntrain [epoch 12 batch 60]: : 1440it [00:05, 277.27it/s, mean train loss=0.692]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 267.52it/s, mean loss=0.82, macro f1=0.599]\ntrain [epoch 13 batch 60]: : 1440it [00:05, 279.65it/s, mean train loss=0.682]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 268.74it/s, mean loss=0.851, macro f1=0.612]\ntrain [epoch 14 batch 60]: : 1440it [00:05, 272.12it/s, mean train loss=0.626]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 266.22it/s, mean loss=0.748, macro f1=0.628]\ntrain [epoch 15 batch 60]: : 1440it [00:05, 281.01it/s, mean train loss=0.608]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 262.75it/s, mean loss=0.73, macro f1=0.641]\ntrain [epoch 16 batch 60]: : 1440it [00:05, 279.72it/s, mean train loss=0.582]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 262.20it/s, mean loss=0.83, macro f1=0.597]\ntrain [epoch 17 batch 60]: : 1440it [00:05, 281.40it/s, mean train loss=0.569]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 259.31it/s, mean loss=0.731, macro f1=0.614]\ntrain [epoch 18 batch 60]: : 1440it [00:05, 281.23it/s, mean train loss=0.544]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 268.67it/s, mean loss=0.742, macro f1=0.596]\n\n\nEarly stopping at epoch 18\n\n\ntest [batch 38]: : 912it [00:03, 283.01it/s, mean loss=0.797, macro f1=0.576]\n\n\nTrial 3 fold 2\n\n\ntrain [epoch 1 batch 60]: : 1440it [00:05, 281.17it/s, mean train loss=1.43]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 265.28it/s, mean loss=1.14, macro f1=0.392]\ntrain [epoch 2 batch 60]: : 1440it [00:05, 282.57it/s, mean train loss=1.08]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 261.88it/s, mean loss=1.03, macro f1=0.519]\ntrain [epoch 3 batch 60]: : 1440it [00:05, 282.55it/s, mean train loss=0.991]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 266.93it/s, mean loss=0.958, macro f1=0.507]\ntrain [epoch 4 batch 60]: : 1440it [00:05, 280.80it/s, mean train loss=0.965]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 268.29it/s, mean loss=0.9, macro f1=0.524]\ntrain [epoch 5 batch 60]: : 1440it [00:05, 253.26it/s, mean train loss=0.894]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 253.68it/s, mean loss=0.892, macro f1=0.535]\ntrain [epoch 6 batch 60]: : 1440it [00:05, 270.74it/s, mean train loss=0.836]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 253.78it/s, mean loss=0.895, macro f1=0.507]\ntrain [epoch 7 batch 60]: : 1440it [00:05, 247.18it/s, mean train loss=0.84]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 263.34it/s, mean loss=0.839, macro f1=0.555]\ntrain [epoch 8 batch 60]: : 1440it [00:05, 282.31it/s, mean train loss=0.767]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 267.10it/s, mean loss=0.828, macro f1=0.564]\ntrain [epoch 9 batch 60]: : 1440it [00:05, 282.69it/s, mean train loss=0.757]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 232.21it/s, mean loss=0.825, macro f1=0.568]\ntrain [epoch 10 batch 60]: : 1440it [00:05, 278.18it/s, mean train loss=0.704]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 265.27it/s, mean loss=1.05, macro f1=0.521]\ntrain [epoch 11 batch 60]: : 1440it [00:05, 282.80it/s, mean train loss=0.709]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 269.49it/s, mean loss=0.815, macro f1=0.599]\ntrain [epoch 12 batch 60]: : 1440it [00:05, 282.99it/s, mean train loss=0.669]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 261.43it/s, mean loss=0.821, macro f1=0.597]\ntrain [epoch 13 batch 60]: : 1440it [00:05, 282.84it/s, mean train loss=0.651]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 269.17it/s, mean loss=0.817, macro f1=0.598]\ntrain [epoch 14 batch 60]: : 1440it [00:05, 267.71it/s, mean train loss=0.62]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 268.33it/s, mean loss=0.828, macro f1=0.592]\n\n\nEarly stopping at epoch 14\n\n\ntest [batch 38]: : 912it [00:02, 310.44it/s, mean loss=0.842, macro f1=0.556]\n\n\nTrial 3 fold 3\n\n\ntrain [epoch 1 batch 60]: : 1440it [00:05, 283.63it/s, mean train loss=1.42]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 268.80it/s, mean loss=1.19, macro f1=0.39]\ntrain [epoch 2 batch 60]: : 1440it [00:05, 282.65it/s, mean train loss=1.12]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 265.08it/s, mean loss=1.07, macro f1=0.444]\ntrain [epoch 3 batch 60]: : 1440it [00:05, 281.95it/s, mean train loss=1.02]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 271.42it/s, mean loss=1.01, macro f1=0.445]\ntrain [epoch 4 batch 60]: : 1440it [00:05, 278.90it/s, mean train loss=0.953]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 245.01it/s, mean loss=1, macro f1=0.539]\ntrain [epoch 5 batch 60]: : 1440it [00:05, 283.00it/s, mean train loss=0.879]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 262.83it/s, mean loss=0.937, macro f1=0.521]\ntrain [epoch 6 batch 60]: : 1440it [00:05, 282.69it/s, mean train loss=0.86]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 262.82it/s, mean loss=0.956, macro f1=0.572]\ntrain [epoch 7 batch 60]: : 1440it [00:05, 280.95it/s, mean train loss=0.797]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 264.37it/s, mean loss=0.948, macro f1=0.488]\ntrain [epoch 8 batch 60]: : 1440it [00:05, 283.88it/s, mean train loss=0.769]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 268.05it/s, mean loss=0.877, macro f1=0.498]\ntrain [epoch 9 batch 60]: : 1440it [00:05, 270.72it/s, mean train loss=0.761]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 266.44it/s, mean loss=0.87, macro f1=0.566]\ntrain [epoch 10 batch 60]: : 1440it [00:05, 276.30it/s, mean train loss=0.73]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 258.79it/s, mean loss=0.858, macro f1=0.547]\ntrain [epoch 11 batch 60]: : 1440it [00:06, 227.91it/s, mean train loss=0.651]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 210.83it/s, mean loss=0.909, macro f1=0.589]\ntrain [epoch 12 batch 60]: : 1440it [00:06, 239.70it/s, mean train loss=0.665]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 266.98it/s, mean loss=0.862, macro f1=0.564]\ntrain [epoch 13 batch 60]: : 1440it [00:05, 274.57it/s, mean train loss=0.639]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 259.63it/s, mean loss=0.877, macro f1=0.593]\n\n\nEarly stopping at epoch 13\n\n\ntest [batch 38]: : 912it [00:02, 305.43it/s, mean loss=0.818, macro f1=0.672]\n[I 2024-10-19 23:33:34,724] Trial 3 finished with value: 0.6011556440238311 and parameters: {'batch_size': 24, 'lr': 0.0008395654584238159, 'weight_decay': 0.0009609840789396307, 'reduce_lr_factor': 0.09788135185120563, 'momentum': 0.8890437283130633, 'early_stop_patience': 3, 'resize_length': 160, 'grayscale': True, 'jitter': True, 'horizontal_flip': True, 'rotation': False, 'mlp_n_layers': 3, 'mlp_layer_1_size': 256, 'mlp_activation_f': 'sigmoid'}. Best is trial 3 with value: 0.6011556440238311.\n\n\nTrial 4 params: {'batch_size': 32, 'lr': 0.0009300032356004519, 'weight_decay': 0.003185689524513237, 'reduce_lr_factor': 0.07006693419673135, 'momentum': 0.8250415938568345, 'early_stop_patience': 3, 'resize_length': 96, 'grayscale': False, 'jitter': False, 'horizontal_flip': False, 'rotation': False, 'mlp_n_layers': 5, 'mlp_layer_1_size': 512, 'mlp_layer_2_size': 2048, 'mlp_layer_3_size': 512, 'mlp_activation_f': 'softplus'}\nTrial 4 fold 1\n\n\ntrain [epoch 1 batch 45]: : 1440it [00:04, 331.45it/s, mean train loss=0.906]\nvalidate [batch 12]: : 384it [00:01, 285.61it/s, mean loss=0.875, macro f1=0.632]\ntrain [epoch 2 batch 45]: : 1440it [00:04, 329.44it/s, mean train loss=0.564]\nvalidate [batch 12]: : 384it [00:01, 288.01it/s, mean loss=0.664, macro f1=0.693]\ntrain [epoch 3 batch 45]: : 1440it [00:04, 329.94it/s, mean train loss=0.403]\nvalidate [batch 12]: : 384it [00:01, 281.32it/s, mean loss=0.626, macro f1=0.713]\ntrain [epoch 4 batch 45]: : 1440it [00:04, 331.27it/s, mean train loss=0.353]\nvalidate [batch 12]: : 384it [00:01, 286.60it/s, mean loss=0.746, macro f1=0.642]\ntrain [epoch 5 batch 45]: : 1440it [00:04, 305.59it/s, mean train loss=0.266]\nvalidate [batch 12]: : 384it [00:01, 286.65it/s, mean loss=0.738, macro f1=0.706]\ntrain [epoch 6 batch 45]: : 1440it [00:04, 328.57it/s, mean train loss=0.251]\nvalidate [batch 12]: : 384it [00:01, 284.58it/s, mean loss=0.734, macro f1=0.697]\n\n\nEarly stopping at epoch 6\n\n\ntest [batch 29]: : 928it [00:02, 313.78it/s, mean loss=0.773, macro f1=0.688]\n\n\nTrial 4 fold 2\n\n\ntrain [epoch 1 batch 45]: : 1440it [00:04, 330.17it/s, mean train loss=0.941]\nvalidate [batch 12]: : 384it [00:01, 285.00it/s, mean loss=0.877, macro f1=0.571]\ntrain [epoch 2 batch 45]: : 1440it [00:04, 328.60it/s, mean train loss=0.523]\nvalidate [batch 12]: : 384it [00:01, 290.12it/s, mean loss=0.756, macro f1=0.643]\ntrain [epoch 3 batch 45]: : 1440it [00:04, 331.79it/s, mean train loss=0.407]\nvalidate [batch 12]: : 384it [00:01, 263.79it/s, mean loss=0.767, macro f1=0.578]\ntrain [epoch 4 batch 45]: : 1440it [00:04, 319.67it/s, mean train loss=0.332]\nvalidate [batch 12]: : 384it [00:01, 291.67it/s, mean loss=0.8, macro f1=0.661]\ntrain [epoch 5 batch 45]: : 1440it [00:04, 330.17it/s, mean train loss=0.272]\nvalidate [batch 12]: : 384it [00:01, 288.54it/s, mean loss=0.71, macro f1=0.684]\ntrain [epoch 6 batch 45]: : 1440it [00:04, 330.06it/s, mean train loss=0.227]\nvalidate [batch 12]: : 384it [00:01, 287.46it/s, mean loss=0.625, macro f1=0.725]\ntrain [epoch 7 batch 45]: : 1440it [00:04, 325.56it/s, mean train loss=0.186]\nvalidate [batch 12]: : 384it [00:01, 287.85it/s, mean loss=0.787, macro f1=0.711]\ntrain [epoch 8 batch 45]: : 1440it [00:04, 329.45it/s, mean train loss=0.151]\nvalidate [batch 12]: : 384it [00:01, 279.62it/s, mean loss=0.762, macro f1=0.707]\ntrain [epoch 9 batch 45]: : 1440it [00:04, 305.04it/s, mean train loss=0.135]\nvalidate [batch 12]: : 384it [00:01, 286.56it/s, mean loss=1.03, macro f1=0.663]\n\n\nEarly stopping at epoch 9\n\n\ntest [batch 29]: : 928it [00:02, 312.19it/s, mean loss=0.9, macro f1=0.652]\n\n\nTrial 4 fold 3\n\n\ntrain [epoch 1 batch 45]: : 1440it [00:04, 329.18it/s, mean train loss=0.968]\nvalidate [batch 12]: : 384it [00:01, 291.31it/s, mean loss=0.865, macro f1=0.591]\ntrain [epoch 2 batch 45]: : 1440it [00:04, 302.61it/s, mean train loss=0.517]\nvalidate [batch 12]: : 384it [00:01, 236.67it/s, mean loss=0.774, macro f1=0.623]\ntrain [epoch 3 batch 45]: : 1440it [00:05, 261.66it/s, mean train loss=0.39]\nvalidate [batch 12]: : 384it [00:01, 256.67it/s, mean loss=0.669, macro f1=0.713]\ntrain [epoch 4 batch 45]: : 1440it [00:04, 289.79it/s, mean train loss=0.304]\nvalidate [batch 12]: : 384it [00:01, 285.14it/s, mean loss=0.7, macro f1=0.681]\ntrain [epoch 5 batch 45]: : 1440it [00:04, 326.94it/s, mean train loss=0.244]\nvalidate [batch 12]: : 384it [00:01, 286.92it/s, mean loss=0.536, macro f1=0.729]\ntrain [epoch 6 batch 45]: : 1440it [00:04, 330.20it/s, mean train loss=0.209]\nvalidate [batch 12]: : 384it [00:01, 287.60it/s, mean loss=0.745, macro f1=0.645]\ntrain [epoch 7 batch 45]: : 1440it [00:04, 330.34it/s, mean train loss=0.184]\nvalidate [batch 12]: : 384it [00:01, 286.77it/s, mean loss=0.601, macro f1=0.738]\ntrain [epoch 8 batch 45]: : 1440it [00:04, 327.72it/s, mean train loss=0.153]\nvalidate [batch 12]: : 384it [00:01, 288.53it/s, mean loss=0.583, macro f1=0.718]\n\n\nEarly stopping at epoch 8\n\n\ntest [batch 29]: : 928it [00:02, 319.84it/s, mean loss=0.608, macro f1=0.695]\n[I 2024-10-19 23:36:07,453] Trial 4 finished with value: 0.6783023402471665 and parameters: {'batch_size': 32, 'lr': 0.0009300032356004519, 'weight_decay': 0.003185689524513237, 'reduce_lr_factor': 0.07006693419673135, 'momentum': 0.8250415938568345, 'early_stop_patience': 3, 'resize_length': 96, 'grayscale': False, 'jitter': False, 'horizontal_flip': False, 'rotation': False, 'mlp_n_layers': 5, 'mlp_layer_1_size': 512, 'mlp_layer_2_size': 2048, 'mlp_layer_3_size': 512, 'mlp_activation_f': 'softplus'}. Best is trial 4 with value: 0.6783023402471665.\n\n\nTrial 5 params: {'batch_size': 32, 'lr': 0.0008829180082363043, 'weight_decay': 0.006925315900777659, 'reduce_lr_factor': 0.07527288518376765, 'momentum': 0.8952516325660734, 'early_stop_patience': 3, 'resize_length': 160, 'grayscale': False, 'jitter': False, 'horizontal_flip': True, 'rotation': True, 'mlp_n_layers': 3, 'mlp_layer_1_size': 512, 'mlp_activation_f': 'softplus'}\nTrial 5 fold 1\n\n\ntrain [epoch 1 batch 45]: : 1440it [00:05, 279.50it/s, mean train loss=1.05]\nvalidate [batch 12]: : 384it [00:01, 285.92it/s, mean loss=0.97, macro f1=0.541]\ntrain [epoch 2 batch 45]: : 1440it [00:04, 302.73it/s, mean train loss=0.717]\nvalidate [batch 12]: : 384it [00:01, 286.62it/s, mean loss=0.953, macro f1=0.554]\ntrain [epoch 3 batch 45]: : 1440it [00:04, 301.29it/s, mean train loss=0.691]\nvalidate [batch 12]: : 384it [00:01, 285.64it/s, mean loss=0.776, macro f1=0.649]\ntrain [epoch 4 batch 45]: : 1440it [00:04, 299.96it/s, mean train loss=0.638]\nvalidate [batch 12]: : 384it [00:01, 285.84it/s, mean loss=0.774, macro f1=0.658]\ntrain [epoch 5 batch 45]: : 1440it [00:04, 303.19it/s, mean train loss=0.562]\nvalidate [batch 12]: : 384it [00:01, 283.79it/s, mean loss=0.856, macro f1=0.65]\ntrain [epoch 6 batch 45]: : 1440it [00:04, 292.20it/s, mean train loss=0.556]\nvalidate [batch 12]: : 384it [00:01, 286.69it/s, mean loss=0.673, macro f1=0.669]\ntrain [epoch 7 batch 45]: : 1440it [00:04, 301.62it/s, mean train loss=0.526]\nvalidate [batch 12]: : 384it [00:01, 279.35it/s, mean loss=0.855, macro f1=0.638]\ntrain [epoch 8 batch 45]: : 1440it [00:04, 302.41it/s, mean train loss=0.527]\nvalidate [batch 12]: : 384it [00:01, 288.19it/s, mean loss=0.769, macro f1=0.682]\ntrain [epoch 9 batch 45]: : 1440it [00:04, 299.36it/s, mean train loss=0.531]\nvalidate [batch 12]: : 384it [00:01, 284.79it/s, mean loss=1.05, macro f1=0.587]\n\n\nEarly stopping at epoch 9\n\n\ntest [batch 29]: : 928it [00:02, 315.34it/s, mean loss=1.19, macro f1=0.539]\n\n\nTrial 5 fold 2\n\n\ntrain [epoch 1 batch 45]: : 1440it [00:05, 279.60it/s, mean train loss=1.1]\nvalidate [batch 12]: : 384it [00:01, 286.34it/s, mean loss=0.864, macro f1=0.489]\ntrain [epoch 2 batch 45]: : 1440it [00:04, 302.16it/s, mean train loss=0.771]\nvalidate [batch 12]: : 384it [00:01, 280.49it/s, mean loss=0.843, macro f1=0.535]\ntrain [epoch 3 batch 45]: : 1440it [00:04, 302.09it/s, mean train loss=0.703]\nvalidate [batch 12]: : 384it [00:01, 283.69it/s, mean loss=0.68, macro f1=0.647]\ntrain [epoch 4 batch 45]: : 1440it [00:04, 301.40it/s, mean train loss=0.678]\nvalidate [batch 12]: : 384it [00:01, 286.63it/s, mean loss=0.826, macro f1=0.639]\ntrain [epoch 5 batch 45]: : 1440it [00:05, 259.02it/s, mean train loss=0.663]\nvalidate [batch 12]: : 384it [00:01, 239.91it/s, mean loss=0.912, macro f1=0.545]\ntrain [epoch 6 batch 45]: : 1440it [00:05, 246.06it/s, mean train loss=0.604]\nvalidate [batch 12]: : 384it [00:01, 232.12it/s, mean loss=0.75, macro f1=0.645]\n\n\nEarly stopping at epoch 6\n\n\ntest [batch 29]: : 928it [00:03, 254.05it/s, mean loss=0.754, macro f1=0.646]\n\n\nTrial 5 fold 3\n\n\ntrain [epoch 1 batch 45]: : 1440it [00:04, 300.49it/s, mean train loss=0.958]\nvalidate [batch 12]: : 384it [00:01, 285.39it/s, mean loss=0.932, macro f1=0.525]\ntrain [epoch 2 batch 45]: : 1440it [00:04, 300.30it/s, mean train loss=0.778]\nvalidate [batch 12]: : 384it [00:01, 282.72it/s, mean loss=0.821, macro f1=0.613]\ntrain [epoch 3 batch 45]: : 1440it [00:04, 300.01it/s, mean train loss=0.689]\nvalidate [batch 12]: : 384it [00:01, 285.63it/s, mean loss=0.92, macro f1=0.58]\ntrain [epoch 4 batch 45]: : 1440it [00:05, 276.93it/s, mean train loss=0.639]\nvalidate [batch 12]: : 384it [00:01, 285.47it/s, mean loss=0.791, macro f1=0.593]\ntrain [epoch 5 batch 45]: : 1440it [00:04, 299.26it/s, mean train loss=0.594]\nvalidate [batch 12]: : 384it [00:01, 283.38it/s, mean loss=0.824, macro f1=0.62]\ntrain [epoch 6 batch 45]: : 1440it [00:04, 299.92it/s, mean train loss=0.599]\nvalidate [batch 12]: : 384it [00:01, 286.44it/s, mean loss=0.843, macro f1=0.61]\ntrain [epoch 7 batch 45]: : 1440it [00:04, 301.79it/s, mean train loss=0.579]\nvalidate [batch 12]: : 384it [00:01, 286.16it/s, mean loss=1.65, macro f1=0.544]\n\n\nEarly stopping at epoch 7\n\n\ntest [batch 29]: : 928it [00:02, 311.51it/s, mean loss=1.74, macro f1=0.576]\n[I 2024-10-19 23:38:44,377] Trial 5 finished with value: 0.5868615498905451 and parameters: {'batch_size': 32, 'lr': 0.0008829180082363043, 'weight_decay': 0.006925315900777659, 'reduce_lr_factor': 0.07527288518376765, 'momentum': 0.8952516325660734, 'early_stop_patience': 3, 'resize_length': 160, 'grayscale': False, 'jitter': False, 'horizontal_flip': True, 'rotation': True, 'mlp_n_layers': 3, 'mlp_layer_1_size': 512, 'mlp_activation_f': 'softplus'}. Best is trial 4 with value: 0.6783023402471665.\n\n\nTrial 6 params: {'batch_size': 32, 'lr': 0.00043710425107963425, 'weight_decay': 0.00896546595851063, 'reduce_lr_factor': 0.043080568304310694, 'momentum': 0.8828143358004691, 'early_stop_patience': 3, 'resize_length': 192, 'grayscale': True, 'jitter': True, 'horizontal_flip': True, 'rotation': True, 'mlp_n_layers': 4, 'mlp_layer_1_size': 512, 'mlp_layer_2_size': 256, 'mlp_activation_f': 'softplus'}\nTrial 6 fold 1\n\n\ntrain [epoch 1 batch 45]: : 1440it [00:05, 265.34it/s, mean train loss=1.26]\nvalidate [batch 12]: : 384it [00:01, 288.05it/s, mean loss=1.07, macro f1=0.429]\ntrain [epoch 2 batch 45]: : 1440it [00:05, 271.44it/s, mean train loss=0.895]\nvalidate [batch 12]: : 384it [00:01, 284.68it/s, mean loss=0.844, macro f1=0.615]\ntrain [epoch 3 batch 45]: : 1440it [00:05, 269.57it/s, mean train loss=0.824]\nvalidate [batch 12]: : 384it [00:01, 286.61it/s, mean loss=0.79, macro f1=0.565]\ntrain [epoch 4 batch 45]: : 1440it [00:05, 269.74it/s, mean train loss=0.803]\nvalidate [batch 12]: : 384it [00:01, 279.69it/s, mean loss=0.751, macro f1=0.632]\ntrain [epoch 5 batch 45]: : 1440it [00:05, 270.66it/s, mean train loss=0.717]\nvalidate [batch 12]: : 384it [00:01, 286.53it/s, mean loss=0.976, macro f1=0.549]\ntrain [epoch 6 batch 45]: : 1440it [00:05, 253.04it/s, mean train loss=0.741]\nvalidate [batch 12]: : 384it [00:01, 285.41it/s, mean loss=0.799, macro f1=0.57]\ntrain [epoch 7 batch 45]: : 1440it [00:05, 268.64it/s, mean train loss=0.73]\nvalidate [batch 12]: : 384it [00:01, 284.35it/s, mean loss=0.843, macro f1=0.638]\n\n\nEarly stopping at epoch 7\n\n\ntest [batch 29]: : 928it [00:02, 318.07it/s, mean loss=0.876, macro f1=0.612]\n\n\nTrial 6 fold 2\n\n\ntrain [epoch 1 batch 45]: : 1440it [00:05, 269.27it/s, mean train loss=1.24]\nvalidate [batch 12]: : 384it [00:01, 287.78it/s, mean loss=1, macro f1=0.428]\ntrain [epoch 2 batch 45]: : 1440it [00:05, 269.73it/s, mean train loss=0.943]\nvalidate [batch 12]: : 384it [00:01, 285.76it/s, mean loss=0.863, macro f1=0.472]\ntrain [epoch 3 batch 45]: : 1440it [00:05, 244.95it/s, mean train loss=0.851]\nvalidate [batch 12]: : 384it [00:01, 237.99it/s, mean loss=0.787, macro f1=0.553]\ntrain [epoch 4 batch 45]: : 1440it [00:06, 221.34it/s, mean train loss=0.818]\nvalidate [batch 12]: : 384it [00:01, 233.22it/s, mean loss=0.787, macro f1=0.585]\ntrain [epoch 5 batch 45]: : 1440it [00:05, 240.39it/s, mean train loss=0.786]\nvalidate [batch 12]: : 384it [00:01, 286.32it/s, mean loss=0.854, macro f1=0.51]\ntrain [epoch 6 batch 45]: : 1440it [00:05, 269.78it/s, mean train loss=0.757]\nvalidate [batch 12]: : 384it [00:01, 280.42it/s, mean loss=0.762, macro f1=0.565]\n[I 2024-10-19 23:40:22,987] Trial 6 pruned. \n\n\nTrial 7 params: {'batch_size': 32, 'lr': 0.000975766289952857, 'weight_decay': 0.00855803342392611, 'reduce_lr_factor': 0.011054267576650179, 'momentum': 0.8683958322508891, 'early_stop_patience': 3, 'resize_length': 64, 'grayscale': True, 'jitter': True, 'horizontal_flip': True, 'rotation': False, 'mlp_n_layers': 5, 'mlp_layer_1_size': 512, 'mlp_layer_2_size': 2048, 'mlp_layer_3_size': 2048, 'mlp_activation_f': 'sigmoid'}\nTrial 7 fold 1\n\n\ntrain [epoch 1 batch 45]: : 1440it [00:05, 265.89it/s, mean train loss=1.4]\nvalidate [batch 12]: : 384it [00:01, 286.80it/s, mean loss=1.17, macro f1=0.369]\ntrain [epoch 2 batch 45]: : 1440it [00:05, 285.25it/s, mean train loss=0.935]\nvalidate [batch 12]: : 384it [00:01, 290.93it/s, mean loss=0.859, macro f1=0.587]\ntrain [epoch 3 batch 45]: : 1440it [00:05, 284.32it/s, mean train loss=0.843]\nvalidate [batch 12]: : 384it [00:01, 287.50it/s, mean loss=0.792, macro f1=0.562]\ntrain [epoch 4 batch 45]: : 1440it [00:05, 285.79it/s, mean train loss=0.75]\nvalidate [batch 12]: : 384it [00:01, 289.70it/s, mean loss=0.729, macro f1=0.602]\ntrain [epoch 5 batch 45]: : 1440it [00:05, 282.88it/s, mean train loss=0.729]\nvalidate [batch 12]: : 384it [00:01, 274.13it/s, mean loss=0.702, macro f1=0.606]\ntrain [epoch 6 batch 45]: : 1440it [00:05, 278.53it/s, mean train loss=0.665]\nvalidate [batch 12]: : 384it [00:01, 289.70it/s, mean loss=0.706, macro f1=0.609]\n[I 2024-10-19 23:41:04,429] Trial 7 pruned. \n\n\nTrial 8 params: {'batch_size': 24, 'lr': 0.0009449286660840943, 'weight_decay': 0.007395507950492875, 'reduce_lr_factor': 0.054141292775581044, 'momentum': 0.8432087793149314, 'early_stop_patience': 1, 'resize_length': 32, 'grayscale': True, 'jitter': True, 'horizontal_flip': True, 'rotation': False, 'mlp_n_layers': 4, 'mlp_layer_1_size': 512, 'mlp_layer_2_size': 256, 'mlp_activation_f': 'sigmoid'}\nTrial 8 fold 1\n\n\ntrain [epoch 1 batch 60]: : 1440it [00:05, 285.28it/s, mean train loss=1.74]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 263.00it/s, mean loss=1.43, macro f1=0.251]\ntrain [epoch 2 batch 60]: : 1440it [00:05, 282.07it/s, mean train loss=1.33]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 267.37it/s, mean loss=1.23, macro f1=0.302]\ntrain [epoch 3 batch 60]: : 1440it [00:05, 282.87it/s, mean train loss=1.23]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 266.95it/s, mean loss=1.14, macro f1=0.37]\ntrain [epoch 4 batch 60]: : 1440it [00:05, 271.59it/s, mean train loss=1.16]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 265.50it/s, mean loss=1.07, macro f1=0.463]\ntrain [epoch 5 batch 60]: : 1440it [00:05, 283.61it/s, mean train loss=1.07]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 265.84it/s, mean loss=1.01, macro f1=0.43]\ntrain [epoch 6 batch 60]: : 1440it [00:05, 282.05it/s, mean train loss=1.02]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 266.92it/s, mean loss=0.99, macro f1=0.412]\n[I 2024-10-19 23:41:45,854] Trial 8 pruned. \n\n\nTrial 9 params: {'batch_size': 24, 'lr': 0.0005924339424180696, 'weight_decay': 0.008310484552361904, 'reduce_lr_factor': 0.06660836592320339, 'momentum': 0.9658036245350051, 'early_stop_patience': 1, 'resize_length': 192, 'grayscale': False, 'jitter': True, 'horizontal_flip': True, 'rotation': True, 'mlp_n_layers': 4, 'mlp_layer_1_size': 512, 'mlp_layer_2_size': 1024, 'mlp_activation_f': 'sigmoid'}\nTrial 9 fold 1\n\n\ntrain [epoch 1 batch 60]: : 1440it [00:05, 241.72it/s, mean train loss=1.26]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 261.46it/s, mean loss=1.2, macro f1=0.331]\ntrain [epoch 2 batch 60]: : 1440it [00:05, 240.83it/s, mean train loss=0.878]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 228.43it/s, mean loss=0.954, macro f1=0.515]\ntrain [epoch 3 batch 60]: : 1440it [00:06, 239.94it/s, mean train loss=0.79]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 263.81it/s, mean loss=0.909, macro f1=0.598]\ntrain [epoch 4 batch 60]: : 1440it [00:06, 218.50it/s, mean train loss=0.72]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 206.99it/s, mean loss=0.733, macro f1=0.662]\ntrain [epoch 5 batch 60]: : 1440it [00:07, 203.72it/s, mean train loss=0.659]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 209.82it/s, mean loss=0.76, macro f1=0.684]\n\n\nEarly stopping at epoch 5\n\n\ntest [batch 38]: : 912it [00:03, 274.89it/s, mean loss=0.81, macro f1=0.665]\n\n\nTrial 9 fold 2\n\n\ntrain [epoch 1 batch 60]: : 1440it [00:06, 228.81it/s, mean train loss=1.31]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 262.50it/s, mean loss=1.07, macro f1=0.408]\ntrain [epoch 2 batch 60]: : 1440it [00:05, 241.79it/s, mean train loss=0.949]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 261.20it/s, mean loss=0.818, macro f1=0.543]\ntrain [epoch 3 batch 60]: : 1440it [00:06, 239.00it/s, mean train loss=0.819]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 261.75it/s, mean loss=0.766, macro f1=0.565]\ntrain [epoch 4 batch 60]: : 1440it [00:05, 240.70it/s, mean train loss=0.766]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 260.72it/s, mean loss=0.849, macro f1=0.563]\n\n\nEarly stopping at epoch 4\n\n\ntest [batch 38]: : 912it [00:03, 288.87it/s, mean loss=0.884, macro f1=0.549]\n\n\nTrial 9 fold 3\n\n\ntrain [epoch 1 batch 60]: : 1440it [00:05, 242.34it/s, mean train loss=1.18]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 259.74it/s, mean loss=1.18, macro f1=0.31]\ntrain [epoch 2 batch 60]: : 1440it [00:05, 241.82it/s, mean train loss=0.869]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 262.47it/s, mean loss=0.856, macro f1=0.458]\ntrain [epoch 3 batch 60]: : 1440it [00:05, 241.22it/s, mean train loss=0.758]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 261.09it/s, mean loss=1.2, macro f1=0.45]\n\n\nEarly stopping at epoch 3\n\n\ntest [batch 38]: : 912it [00:03, 303.69it/s, mean loss=1.07, macro f1=0.522]\n[I 2024-10-19 23:43:32,294] Trial 9 finished with value: 0.5787833240374795 and parameters: {'batch_size': 24, 'lr': 0.0005924339424180696, 'weight_decay': 0.008310484552361904, 'reduce_lr_factor': 0.06660836592320339, 'momentum': 0.9658036245350051, 'early_stop_patience': 1, 'resize_length': 192, 'grayscale': False, 'jitter': True, 'horizontal_flip': True, 'rotation': True, 'mlp_n_layers': 4, 'mlp_layer_1_size': 512, 'mlp_layer_2_size': 1024, 'mlp_activation_f': 'sigmoid'}. Best is trial 4 with value: 0.6783023402471665.\n\n\nTrial 10 params: {'batch_size': 32, 'lr': 0.0002329857865536498, 'weight_decay': 0.0038215851878374606, 'reduce_lr_factor': 0.09308742500540024, 'momentum': 0.8052673117171141, 'early_stop_patience': 2, 'resize_length': 96, 'grayscale': False, 'jitter': False, 'horizontal_flip': False, 'rotation': False, 'mlp_n_layers': 5, 'mlp_layer_1_size': 256, 'mlp_layer_2_size': 512, 'mlp_layer_3_size': 512, 'mlp_activation_f': 'relu'}\nTrial 10 fold 1\n\n\ntrain [epoch 1 batch 45]: : 1440it [00:04, 319.81it/s, mean train loss=1.38]\nvalidate [batch 12]: : 384it [00:01, 288.29it/s, mean loss=1.1, macro f1=0.571]\ntrain [epoch 2 batch 45]: : 1440it [00:04, 334.05it/s, mean train loss=0.879]\nvalidate [batch 12]: : 384it [00:01, 287.44it/s, mean loss=0.933, macro f1=0.608]\ntrain [epoch 3 batch 45]: : 1440it [00:04, 337.46it/s, mean train loss=0.688]\nvalidate [batch 12]: : 384it [00:01, 287.57it/s, mean loss=0.814, macro f1=0.625]\ntrain [epoch 4 batch 45]: : 1440it [00:04, 334.28it/s, mean train loss=0.619]\nvalidate [batch 12]: : 384it [00:01, 288.72it/s, mean loss=0.705, macro f1=0.674]\ntrain [epoch 5 batch 45]: : 1440it [00:04, 338.11it/s, mean train loss=0.496]\nvalidate [batch 12]: : 384it [00:01, 281.69it/s, mean loss=0.687, macro f1=0.66]\ntrain [epoch 6 batch 45]: : 1440it [00:04, 337.06it/s, mean train loss=0.408]\nvalidate [batch 12]: : 384it [00:01, 248.42it/s, mean loss=0.681, macro f1=0.7]\ntrain [epoch 7 batch 45]: : 1440it [00:04, 331.13it/s, mean train loss=0.38]\nvalidate [batch 12]: : 384it [00:01, 290.47it/s, mean loss=0.644, macro f1=0.705]\ntrain [epoch 8 batch 45]: : 1440it [00:04, 340.20it/s, mean train loss=0.336]\nvalidate [batch 12]: : 384it [00:01, 289.67it/s, mean loss=0.608, macro f1=0.737]\ntrain [epoch 9 batch 45]: : 1440it [00:04, 335.89it/s, mean train loss=0.28]\nvalidate [batch 12]: : 384it [00:01, 288.72it/s, mean loss=0.637, macro f1=0.728]\ntrain [epoch 10 batch 45]: : 1440it [00:05, 287.43it/s, mean train loss=0.279]\nvalidate [batch 12]: : 384it [00:01, 231.61it/s, mean loss=0.562, macro f1=0.707]\ntrain [epoch 11 batch 45]: : 1440it [00:05, 269.83it/s, mean train loss=0.266]\nvalidate [batch 12]: : 384it [00:01, 219.01it/s, mean loss=0.583, macro f1=0.724]\ntrain [epoch 12 batch 45]: : 1440it [00:05, 266.61it/s, mean train loss=0.223]\nvalidate [batch 12]: : 384it [00:01, 286.00it/s, mean loss=0.56, macro f1=0.771]\ntrain [epoch 13 batch 45]: : 1440it [00:04, 333.57it/s, mean train loss=0.198]\nvalidate [batch 12]: : 384it [00:01, 288.31it/s, mean loss=0.617, macro f1=0.704]\ntrain [epoch 14 batch 45]: : 1440it [00:04, 337.87it/s, mean train loss=0.198]\nvalidate [batch 12]: : 384it [00:01, 288.03it/s, mean loss=0.535, macro f1=0.734]\ntrain [epoch 15 batch 45]: : 1440it [00:04, 332.86it/s, mean train loss=0.167]\nvalidate [batch 12]: : 384it [00:01, 289.15it/s, mean loss=0.51, macro f1=0.769]\ntrain [epoch 16 batch 45]: : 1440it [00:04, 337.92it/s, mean train loss=0.168]\nvalidate [batch 12]: : 384it [00:01, 285.19it/s, mean loss=0.603, macro f1=0.735]\ntrain [epoch 17 batch 45]: : 1440it [00:04, 319.90it/s, mean train loss=0.158]\nvalidate [batch 12]: : 384it [00:01, 286.55it/s, mean loss=0.574, macro f1=0.782]\n\n\nEarly stopping at epoch 17\n\n\ntest [batch 29]: : 928it [00:02, 318.95it/s, mean loss=0.591, macro f1=0.745]\n\n\nTrial 10 fold 2\n\n\ntrain [epoch 1 batch 45]: : 1440it [00:04, 333.32it/s, mean train loss=1.35]\nvalidate [batch 12]: : 384it [00:01, 280.28it/s, mean loss=1.11, macro f1=0.584]\ntrain [epoch 2 batch 45]: : 1440it [00:04, 334.12it/s, mean train loss=0.881]\nvalidate [batch 12]: : 384it [00:01, 279.90it/s, mean loss=0.902, macro f1=0.615]\ntrain [epoch 3 batch 45]: : 1440it [00:04, 336.04it/s, mean train loss=0.717]\nvalidate [batch 12]: : 384it [00:01, 287.66it/s, mean loss=0.82, macro f1=0.62]\ntrain [epoch 4 batch 45]: : 1440it [00:04, 334.65it/s, mean train loss=0.646]\nvalidate [batch 12]: : 384it [00:01, 252.66it/s, mean loss=0.769, macro f1=0.692]\ntrain [epoch 5 batch 45]: : 1440it [00:04, 336.40it/s, mean train loss=0.569]\nvalidate [batch 12]: : 384it [00:01, 290.23it/s, mean loss=0.771, macro f1=0.622]\ntrain [epoch 6 batch 45]: : 1440it [00:04, 333.54it/s, mean train loss=0.491]\nvalidate [batch 12]: : 384it [00:01, 285.41it/s, mean loss=0.716, macro f1=0.698]\ntrain [epoch 7 batch 45]: : 1440it [00:04, 335.40it/s, mean train loss=0.414]\nvalidate [batch 12]: : 384it [00:01, 282.40it/s, mean loss=0.687, macro f1=0.697]\ntrain [epoch 8 batch 45]: : 1440it [00:04, 335.09it/s, mean train loss=0.377]\nvalidate [batch 12]: : 384it [00:01, 290.41it/s, mean loss=0.665, macro f1=0.687]\ntrain [epoch 9 batch 45]: : 1440it [00:04, 333.35it/s, mean train loss=0.341]\nvalidate [batch 12]: : 384it [00:01, 287.47it/s, mean loss=0.685, macro f1=0.709]\ntrain [epoch 10 batch 45]: : 1440it [00:04, 324.42it/s, mean train loss=0.287]\nvalidate [batch 12]: : 384it [00:01, 289.24it/s, mean loss=0.631, macro f1=0.698]\ntrain [epoch 11 batch 45]: : 1440it [00:04, 335.40it/s, mean train loss=0.282]\nvalidate [batch 12]: : 384it [00:01, 289.14it/s, mean loss=0.613, macro f1=0.749]\ntrain [epoch 12 batch 45]: : 1440it [00:04, 337.26it/s, mean train loss=0.253]\nvalidate [batch 12]: : 384it [00:01, 286.20it/s, mean loss=0.66, macro f1=0.733]\ntrain [epoch 13 batch 45]: : 1440it [00:04, 336.64it/s, mean train loss=0.243]\nvalidate [batch 12]: : 384it [00:01, 287.89it/s, mean loss=0.582, macro f1=0.725]\ntrain [epoch 14 batch 45]: : 1440it [00:04, 335.10it/s, mean train loss=0.197]\nvalidate [batch 12]: : 384it [00:01, 285.83it/s, mean loss=0.611, macro f1=0.742]\ntrain [epoch 15 batch 45]: : 1440it [00:04, 289.73it/s, mean train loss=0.201]\nvalidate [batch 12]: : 384it [00:01, 264.93it/s, mean loss=0.62, macro f1=0.721]\n\n\nEarly stopping at epoch 15\n\n\ntest [batch 29]: : 928it [00:03, 276.18it/s, mean loss=0.598, macro f1=0.707]\n\n\nTrial 10 fold 3\n\n\ntrain [epoch 1 batch 45]: : 1440it [00:05, 280.70it/s, mean train loss=1.37]\nvalidate [batch 12]: : 384it [00:01, 264.18it/s, mean loss=1.12, macro f1=0.522]\ntrain [epoch 2 batch 45]: : 1440it [00:04, 294.85it/s, mean train loss=0.877]\nvalidate [batch 12]: : 384it [00:01, 288.91it/s, mean loss=0.887, macro f1=0.598]\ntrain [epoch 3 batch 45]: : 1440it [00:04, 332.97it/s, mean train loss=0.662]\nvalidate [batch 12]: : 384it [00:01, 286.97it/s, mean loss=0.83, macro f1=0.62]\ntrain [epoch 4 batch 45]: : 1440it [00:04, 333.91it/s, mean train loss=0.592]\nvalidate [batch 12]: : 384it [00:01, 240.65it/s, mean loss=0.758, macro f1=0.714]\ntrain [epoch 5 batch 45]: : 1440it [00:04, 337.80it/s, mean train loss=0.489]\nvalidate [batch 12]: : 384it [00:01, 288.10it/s, mean loss=0.745, macro f1=0.703]\ntrain [epoch 6 batch 45]: : 1440it [00:04, 338.90it/s, mean train loss=0.4]\nvalidate [batch 12]: : 384it [00:01, 284.86it/s, mean loss=0.667, macro f1=0.676]\ntrain [epoch 7 batch 45]: : 1440it [00:04, 339.06it/s, mean train loss=0.376]\nvalidate [batch 12]: : 384it [00:01, 287.64it/s, mean loss=0.677, macro f1=0.689]\ntrain [epoch 8 batch 45]: : 1440it [00:04, 336.74it/s, mean train loss=0.305]\nvalidate [batch 12]: : 384it [00:01, 289.36it/s, mean loss=0.671, macro f1=0.661]\n\n\nEarly stopping at epoch 8\n\n\ntest [batch 29]: : 928it [00:02, 320.94it/s, mean loss=0.633, macro f1=0.683]\n[I 2024-10-19 23:47:48,103] Trial 10 finished with value: 0.7117729729542099 and parameters: {'batch_size': 32, 'lr': 0.0002329857865536498, 'weight_decay': 0.0038215851878374606, 'reduce_lr_factor': 0.09308742500540024, 'momentum': 0.8052673117171141, 'early_stop_patience': 2, 'resize_length': 96, 'grayscale': False, 'jitter': False, 'horizontal_flip': False, 'rotation': False, 'mlp_n_layers': 5, 'mlp_layer_1_size': 256, 'mlp_layer_2_size': 512, 'mlp_layer_3_size': 512, 'mlp_activation_f': 'relu'}. Best is trial 10 with value: 0.7117729729542099.\n\n\nTrial 11 params: {'batch_size': 32, 'lr': 0.00019298475863362353, 'weight_decay': 0.003512101966155307, 'reduce_lr_factor': 0.09326175431644539, 'momentum': 0.800792649502636, 'early_stop_patience': 2, 'resize_length': 96, 'grayscale': False, 'jitter': False, 'horizontal_flip': False, 'rotation': False, 'mlp_n_layers': 5, 'mlp_layer_1_size': 256, 'mlp_layer_2_size': 512, 'mlp_layer_3_size': 512, 'mlp_activation_f': 'relu'}\nTrial 11 fold 1\n\n\ntrain [epoch 1 batch 45]: : 1440it [00:04, 316.61it/s, mean train loss=1.44]\nvalidate [batch 12]: : 384it [00:01, 285.76it/s, mean loss=1.12, macro f1=0.531]\ntrain [epoch 2 batch 45]: : 1440it [00:04, 339.78it/s, mean train loss=0.921]\nvalidate [batch 12]: : 384it [00:01, 293.73it/s, mean loss=0.943, macro f1=0.609]\ntrain [epoch 3 batch 45]: : 1440it [00:04, 337.48it/s, mean train loss=0.753]\nvalidate [batch 12]: : 384it [00:01, 290.91it/s, mean loss=0.822, macro f1=0.631]\ntrain [epoch 4 batch 45]: : 1440it [00:04, 333.43it/s, mean train loss=0.648]\nvalidate [batch 12]: : 384it [00:01, 286.28it/s, mean loss=0.75, macro f1=0.687]\ntrain [epoch 5 batch 45]: : 1440it [00:04, 337.42it/s, mean train loss=0.561]\nvalidate [batch 12]: : 384it [00:01, 290.33it/s, mean loss=0.719, macro f1=0.667]\ntrain [epoch 6 batch 45]: : 1440it [00:04, 337.68it/s, mean train loss=0.49]\nvalidate [batch 12]: : 384it [00:01, 227.31it/s, mean loss=0.669, macro f1=0.712]\ntrain [epoch 7 batch 45]: : 1440it [00:04, 337.26it/s, mean train loss=0.431]\nvalidate [batch 12]: : 384it [00:01, 289.04it/s, mean loss=0.695, macro f1=0.7]\ntrain [epoch 8 batch 45]: : 1440it [00:04, 335.06it/s, mean train loss=0.391]\nvalidate [batch 12]: : 384it [00:01, 289.10it/s, mean loss=0.653, macro f1=0.698]\ntrain [epoch 9 batch 45]: : 1440it [00:04, 339.19it/s, mean train loss=0.327]\nvalidate [batch 12]: : 384it [00:01, 283.12it/s, mean loss=0.619, macro f1=0.732]\ntrain [epoch 10 batch 45]: : 1440it [00:04, 338.14it/s, mean train loss=0.314]\nvalidate [batch 12]: : 384it [00:01, 293.01it/s, mean loss=0.597, macro f1=0.746]\ntrain [epoch 11 batch 45]: : 1440it [00:04, 336.71it/s, mean train loss=0.267]\nvalidate [batch 12]: : 384it [00:01, 287.89it/s, mean loss=0.61, macro f1=0.737]\ntrain [epoch 12 batch 45]: : 1440it [00:04, 325.21it/s, mean train loss=0.261]\nvalidate [batch 12]: : 384it [00:01, 290.02it/s, mean loss=0.581, macro f1=0.702]\ntrain [epoch 13 batch 45]: : 1440it [00:04, 333.79it/s, mean train loss=0.243]\nvalidate [batch 12]: : 384it [00:01, 272.34it/s, mean loss=0.526, macro f1=0.715]\ntrain [epoch 14 batch 45]: : 1440it [00:05, 260.43it/s, mean train loss=0.231]\nvalidate [batch 12]: : 384it [00:01, 228.54it/s, mean loss=0.52, macro f1=0.794]\ntrain [epoch 15 batch 45]: : 1440it [00:05, 269.85it/s, mean train loss=0.204]\nvalidate [batch 12]: : 384it [00:01, 238.74it/s, mean loss=0.537, macro f1=0.769]\ntrain [epoch 16 batch 45]: : 1440it [00:04, 313.46it/s, mean train loss=0.198]\nvalidate [batch 12]: : 384it [00:01, 264.63it/s, mean loss=0.569, macro f1=0.752]\n\n\nEarly stopping at epoch 16\n\n\ntest [batch 29]: : 928it [00:03, 291.03it/s, mean loss=0.608, macro f1=0.752]\n\n\nTrial 11 fold 2\n\n\ntrain [epoch 1 batch 45]: : 1440it [00:04, 333.70it/s, mean train loss=1.42]\nvalidate [batch 12]: : 384it [00:01, 291.62it/s, mean loss=1.16, macro f1=0.508]\ntrain [epoch 2 batch 45]: : 1440it [00:04, 337.26it/s, mean train loss=0.939]\nvalidate [batch 12]: : 384it [00:01, 284.96it/s, mean loss=0.897, macro f1=0.602]\ntrain [epoch 3 batch 45]: : 1440it [00:04, 337.99it/s, mean train loss=0.77]\nvalidate [batch 12]: : 384it [00:01, 292.61it/s, mean loss=0.839, macro f1=0.625]\ntrain [epoch 4 batch 45]: : 1440it [00:04, 338.09it/s, mean train loss=0.644]\nvalidate [batch 12]: : 384it [00:01, 293.96it/s, mean loss=0.786, macro f1=0.635]\ntrain [epoch 5 batch 45]: : 1440it [00:04, 331.50it/s, mean train loss=0.592]\nvalidate [batch 12]: : 384it [00:01, 234.48it/s, mean loss=0.772, macro f1=0.694]\ntrain [epoch 6 batch 45]: : 1440it [00:04, 334.86it/s, mean train loss=0.519]\nvalidate [batch 12]: : 384it [00:01, 291.38it/s, mean loss=0.731, macro f1=0.69]\ntrain [epoch 7 batch 45]: : 1440it [00:04, 338.79it/s, mean train loss=0.476]\nvalidate [batch 12]: : 384it [00:01, 287.13it/s, mean loss=0.669, macro f1=0.694]\ntrain [epoch 8 batch 45]: : 1440it [00:04, 336.26it/s, mean train loss=0.434]\nvalidate [batch 12]: : 384it [00:01, 294.18it/s, mean loss=0.701, macro f1=0.706]\ntrain [epoch 9 batch 45]: : 1440it [00:04, 337.19it/s, mean train loss=0.364]\nvalidate [batch 12]: : 384it [00:01, 292.69it/s, mean loss=0.661, macro f1=0.75]\ntrain [epoch 10 batch 45]: : 1440it [00:04, 339.65it/s, mean train loss=0.336]\nvalidate [batch 12]: : 384it [00:01, 288.83it/s, mean loss=0.67, macro f1=0.733]\ntrain [epoch 11 batch 45]: : 1440it [00:04, 318.47it/s, mean train loss=0.32]\nvalidate [batch 12]: : 384it [00:01, 291.06it/s, mean loss=0.638, macro f1=0.701]\ntrain [epoch 12 batch 45]: : 1440it [00:04, 337.42it/s, mean train loss=0.295]\nvalidate [batch 12]: : 384it [00:01, 287.04it/s, mean loss=0.604, macro f1=0.751]\ntrain [epoch 13 batch 45]: : 1440it [00:04, 338.40it/s, mean train loss=0.273]\nvalidate [batch 12]: : 384it [00:01, 291.56it/s, mean loss=0.653, macro f1=0.754]\ntrain [epoch 14 batch 45]: : 1440it [00:04, 336.84it/s, mean train loss=0.238]\nvalidate [batch 12]: : 384it [00:01, 293.01it/s, mean loss=0.608, macro f1=0.735]\n\n\nEarly stopping at epoch 14\n\n\ntest [batch 29]: : 928it [00:02, 320.39it/s, mean loss=0.58, macro f1=0.74]\n\n\nTrial 11 fold 3\n\n\ntrain [epoch 1 batch 45]: : 1440it [00:04, 338.61it/s, mean train loss=1.46]\nvalidate [batch 12]: : 384it [00:01, 256.24it/s, mean loss=1.22, macro f1=0.473]\ntrain [epoch 2 batch 45]: : 1440it [00:04, 327.64it/s, mean train loss=0.949]\nvalidate [batch 12]: : 384it [00:01, 285.79it/s, mean loss=0.946, macro f1=0.603]\ntrain [epoch 3 batch 45]: : 1440it [00:04, 335.68it/s, mean train loss=0.736]\nvalidate [batch 12]: : 384it [00:01, 290.92it/s, mean loss=0.85, macro f1=0.637]\ntrain [epoch 4 batch 45]: : 1440it [00:04, 332.63it/s, mean train loss=0.667]\nvalidate [batch 12]: : 384it [00:01, 287.85it/s, mean loss=0.802, macro f1=0.624]\ntrain [epoch 5 batch 45]: : 1440it [00:05, 273.01it/s, mean train loss=0.552]\nvalidate [batch 12]: : 384it [00:01, 236.70it/s, mean loss=0.813, macro f1=0.668]\ntrain [epoch 6 batch 45]: : 1440it [00:04, 305.21it/s, mean train loss=0.478]\nvalidate [batch 12]: : 384it [00:01, 230.17it/s, mean loss=0.706, macro f1=0.699]\ntrain [epoch 7 batch 45]: : 1440it [00:05, 266.87it/s, mean train loss=0.428]\nvalidate [batch 12]: : 384it [00:01, 265.89it/s, mean loss=0.69, macro f1=0.717]\ntrain [epoch 8 batch 45]: : 1440it [00:04, 336.40it/s, mean train loss=0.407]\nvalidate [batch 12]: : 384it [00:01, 290.03it/s, mean loss=0.721, macro f1=0.677]\ntrain [epoch 9 batch 45]: : 1440it [00:04, 335.36it/s, mean train loss=0.344]\nvalidate [batch 12]: : 384it [00:01, 290.81it/s, mean loss=0.676, macro f1=0.749]\ntrain [epoch 10 batch 45]: : 1440it [00:04, 338.86it/s, mean train loss=0.304]\nvalidate [batch 12]: : 384it [00:01, 288.83it/s, mean loss=0.639, macro f1=0.683]\ntrain [epoch 11 batch 45]: : 1440it [00:04, 335.84it/s, mean train loss=0.291]\nvalidate [batch 12]: : 384it [00:01, 289.28it/s, mean loss=0.676, macro f1=0.741]\ntrain [epoch 12 batch 45]: : 1440it [00:04, 316.24it/s, mean train loss=0.254]\nvalidate [batch 12]: : 384it [00:01, 282.50it/s, mean loss=0.628, macro f1=0.715]\ntrain [epoch 13 batch 45]: : 1440it [00:04, 339.54it/s, mean train loss=0.213]\nvalidate [batch 12]: : 384it [00:01, 291.29it/s, mean loss=0.62, macro f1=0.753]\ntrain [epoch 14 batch 45]: : 1440it [00:04, 335.40it/s, mean train loss=0.222]\nvalidate [batch 12]: : 384it [00:01, 289.54it/s, mean loss=0.604, macro f1=0.677]\ntrain [epoch 15 batch 45]: : 1440it [00:04, 330.81it/s, mean train loss=0.215]\nvalidate [batch 12]: : 384it [00:01, 292.03it/s, mean loss=0.665, macro f1=0.701]\ntrain [epoch 16 batch 45]: : 1440it [00:04, 334.05it/s, mean train loss=0.183]\nvalidate [batch 12]: : 384it [00:01, 289.44it/s, mean loss=0.606, macro f1=0.725]\n\n\nEarly stopping at epoch 16\n\n\ntest [batch 29]: : 928it [00:03, 306.65it/s, mean loss=0.558, macro f1=0.7]\n[I 2024-10-19 23:52:39,549] Trial 11 finished with value: 0.7305797239149951 and parameters: {'batch_size': 32, 'lr': 0.00019298475863362353, 'weight_decay': 0.003512101966155307, 'reduce_lr_factor': 0.09326175431644539, 'momentum': 0.800792649502636, 'early_stop_patience': 2, 'resize_length': 96, 'grayscale': False, 'jitter': False, 'horizontal_flip': False, 'rotation': False, 'mlp_n_layers': 5, 'mlp_layer_1_size': 256, 'mlp_layer_2_size': 512, 'mlp_layer_3_size': 512, 'mlp_activation_f': 'relu'}. Best is trial 11 with value: 0.7305797239149951.\n\n\nTrial 12 params: {'batch_size': 32, 'lr': 0.0001281119397065285, 'weight_decay': 0.004109676468829812, 'reduce_lr_factor': 0.0980354227939941, 'momentum': 0.8050855805255551, 'early_stop_patience': 2, 'resize_length': 96, 'grayscale': False, 'jitter': False, 'horizontal_flip': False, 'rotation': False, 'mlp_n_layers': 5, 'mlp_layer_1_size': 256, 'mlp_layer_2_size': 512, 'mlp_layer_3_size': 512, 'mlp_activation_f': 'relu'}\nTrial 12 fold 1\n\n\ntrain [epoch 1 batch 45]: : 1440it [00:04, 335.89it/s, mean train loss=1.44]\nvalidate [batch 12]: : 384it [00:01, 289.30it/s, mean loss=1.32, macro f1=0.467]\ntrain [epoch 2 batch 45]: : 1440it [00:04, 338.82it/s, mean train loss=0.965]\nvalidate [batch 12]: : 384it [00:01, 284.86it/s, mean loss=1, macro f1=0.534]\ntrain [epoch 3 batch 45]: : 1440it [00:04, 332.79it/s, mean train loss=0.834]\nvalidate [batch 12]: : 384it [00:01, 292.14it/s, mean loss=0.911, macro f1=0.581]\ntrain [epoch 4 batch 45]: : 1440it [00:04, 333.22it/s, mean train loss=0.7]\nvalidate [batch 12]: : 384it [00:01, 289.65it/s, mean loss=0.85, macro f1=0.613]\ntrain [epoch 5 batch 45]: : 1440it [00:04, 337.42it/s, mean train loss=0.662]\nvalidate [batch 12]: : 384it [00:01, 291.34it/s, mean loss=0.789, macro f1=0.613]\ntrain [epoch 6 batch 45]: : 1440it [00:04, 314.61it/s, mean train loss=0.591]\nvalidate [batch 12]: : 384it [00:01, 288.48it/s, mean loss=0.767, macro f1=0.672]\n[I 2024-10-19 23:53:16,211] Trial 12 pruned. \n\n\nTrial 13 params: {'batch_size': 32, 'lr': 0.0002784740828436272, 'weight_decay': 0.0011123062199767338, 'reduce_lr_factor': 0.08429081321822332, 'momentum': 0.806176734526658, 'early_stop_patience': 2, 'resize_length': 96, 'grayscale': False, 'jitter': False, 'horizontal_flip': False, 'rotation': False, 'mlp_n_layers': 5, 'mlp_layer_1_size': 256, 'mlp_layer_2_size': 512, 'mlp_layer_3_size': 512, 'mlp_activation_f': 'relu'}\nTrial 13 fold 1\n\n\ntrain [epoch 1 batch 45]: : 1440it [00:04, 337.35it/s, mean train loss=1.39]\nvalidate [batch 12]: : 384it [00:01, 282.05it/s, mean loss=1.15, macro f1=0.566]\ntrain [epoch 2 batch 45]: : 1440it [00:04, 336.78it/s, mean train loss=0.843]\nvalidate [batch 12]: : 384it [00:01, 288.47it/s, mean loss=0.94, macro f1=0.588]\ntrain [epoch 3 batch 45]: : 1440it [00:04, 333.44it/s, mean train loss=0.657]\nvalidate [batch 12]: : 384it [00:01, 288.08it/s, mean loss=0.803, macro f1=0.645]\ntrain [epoch 4 batch 45]: : 1440it [00:04, 336.30it/s, mean train loss=0.594]\nvalidate [batch 12]: : 384it [00:01, 290.12it/s, mean loss=0.731, macro f1=0.643]\ntrain [epoch 5 batch 45]: : 1440it [00:05, 272.79it/s, mean train loss=0.479]\nvalidate [batch 12]: : 384it [00:01, 279.26it/s, mean loss=0.689, macro f1=0.651]\ntrain [epoch 6 batch 45]: : 1440it [00:05, 261.37it/s, mean train loss=0.401]\nvalidate [batch 12]: : 384it [00:01, 227.85it/s, mean loss=0.678, macro f1=0.62]\n[I 2024-10-19 23:53:55,149] Trial 13 pruned. \n\n\nTrial 14 params: {'batch_size': 32, 'lr': 0.0002988026568338186, 'weight_decay': 0.004920242157193899, 'reduce_lr_factor': 0.0854873294208598, 'momentum': 0.8478496511845682, 'early_stop_patience': 2, 'resize_length': 32, 'grayscale': False, 'jitter': False, 'horizontal_flip': False, 'rotation': False, 'mlp_n_layers': 5, 'mlp_layer_1_size': 256, 'mlp_layer_2_size': 512, 'mlp_layer_3_size': 1024, 'mlp_activation_f': 'relu'}\nTrial 14 fold 1\n\n\ntrain [epoch 1 batch 45]: : 1440it [00:05, 271.93it/s, mean train loss=1.53]\nvalidate [batch 12]: : 384it [00:01, 278.90it/s, mean loss=1.22, macro f1=0.445]\ntrain [epoch 2 batch 45]: : 1440it [00:04, 339.25it/s, mean train loss=0.879]\nvalidate [batch 12]: : 384it [00:01, 285.44it/s, mean loss=0.9, macro f1=0.57]\ntrain [epoch 3 batch 45]: : 1440it [00:04, 340.53it/s, mean train loss=0.705]\nvalidate [batch 12]: : 384it [00:01, 255.08it/s, mean loss=0.769, macro f1=0.583]\ntrain [epoch 4 batch 45]: : 1440it [00:04, 328.00it/s, mean train loss=0.654]\nvalidate [batch 12]: : 384it [00:01, 287.06it/s, mean loss=0.725, macro f1=0.602]\ntrain [epoch 5 batch 45]: : 1440it [00:04, 337.20it/s, mean train loss=0.548]\nvalidate [batch 12]: : 384it [00:01, 288.99it/s, mean loss=0.672, macro f1=0.661]\ntrain [epoch 6 batch 45]: : 1440it [00:04, 337.49it/s, mean train loss=0.52]\nvalidate [batch 12]: : 384it [00:01, 285.82it/s, mean loss=0.671, macro f1=0.642]\n[I 2024-10-19 23:54:32,805] Trial 14 pruned. \n\n\nTrial 15 params: {'batch_size': 32, 'lr': 1.9502245825872046e-05, 'weight_decay': 0.002640899275157483, 'reduce_lr_factor': 0.08642302061376703, 'momentum': 0.8317387070980279, 'early_stop_patience': 2, 'resize_length': 64, 'grayscale': False, 'jitter': False, 'horizontal_flip': False, 'rotation': False, 'mlp_n_layers': 5, 'mlp_layer_1_size': 256, 'mlp_layer_2_size': 512, 'mlp_layer_3_size': 256, 'mlp_activation_f': 'relu'}\nTrial 15 fold 1\n\n\ntrain [epoch 1 batch 45]: : 1440it [00:04, 340.15it/s, mean train loss=2.05]\nvalidate [batch 12]: : 384it [00:01, 290.17it/s, mean loss=1.85, macro f1=0.257]\ntrain [epoch 2 batch 45]: : 1440it [00:04, 339.01it/s, mean train loss=1.71]\nvalidate [batch 12]: : 384it [00:01, 291.67it/s, mean loss=1.6, macro f1=0.322]\ntrain [epoch 3 batch 45]: : 1440it [00:04, 318.07it/s, mean train loss=1.54]\nvalidate [batch 12]: : 384it [00:01, 291.75it/s, mean loss=1.51, macro f1=0.402]\ntrain [epoch 4 batch 45]: : 1440it [00:04, 338.85it/s, mean train loss=1.44]\nvalidate [batch 12]: : 384it [00:01, 289.53it/s, mean loss=1.4, macro f1=0.429]\ntrain [epoch 5 batch 45]: : 1440it [00:04, 338.49it/s, mean train loss=1.33]\nvalidate [batch 12]: : 384it [00:01, 283.95it/s, mean loss=1.36, macro f1=0.449]\ntrain [epoch 6 batch 45]: : 1440it [00:04, 339.73it/s, mean train loss=1.3]\nvalidate [batch 12]: : 384it [00:01, 290.92it/s, mean loss=1.29, macro f1=0.52]\n[I 2024-10-19 23:55:09,032] Trial 15 pruned. \n\n\nTrial 16 params: {'batch_size': 32, 'lr': 0.0003172086425995281, 'weight_decay': 0.0019517426716198715, 'reduce_lr_factor': 0.09943916989261473, 'momentum': 0.9433310742867136, 'early_stop_patience': 2, 'resize_length': 128, 'grayscale': False, 'jitter': False, 'horizontal_flip': False, 'rotation': False, 'mlp_n_layers': 5, 'mlp_layer_1_size': 256, 'mlp_layer_2_size': 512, 'mlp_layer_3_size': 512, 'mlp_activation_f': 'relu'}\nTrial 16 fold 1\n\n\ntrain [epoch 1 batch 45]: : 1440it [00:04, 332.93it/s, mean train loss=1.13]\nvalidate [batch 12]: : 384it [00:01, 290.09it/s, mean loss=0.872, macro f1=0.505]\ntrain [epoch 2 batch 45]: : 1440it [00:04, 321.25it/s, mean train loss=0.686]\nvalidate [batch 12]: : 384it [00:01, 287.15it/s, mean loss=0.682, macro f1=0.643]\ntrain [epoch 3 batch 45]: : 1440it [00:04, 328.29it/s, mean train loss=0.489]\nvalidate [batch 12]: : 384it [00:01, 288.87it/s, mean loss=0.619, macro f1=0.673]\ntrain [epoch 4 batch 45]: : 1440it [00:04, 333.74it/s, mean train loss=0.398]\nvalidate [batch 12]: : 384it [00:01, 288.06it/s, mean loss=0.624, macro f1=0.661]\ntrain [epoch 5 batch 45]: : 1440it [00:04, 332.33it/s, mean train loss=0.325]\nvalidate [batch 12]: : 384it [00:01, 284.98it/s, mean loss=0.541, macro f1=0.724]\ntrain [epoch 6 batch 45]: : 1440it [00:04, 327.55it/s, mean train loss=0.256]\nvalidate [batch 12]: : 384it [00:01, 289.78it/s, mean loss=0.56, macro f1=0.707]\ntrain [epoch 7 batch 45]: : 1440it [00:04, 314.34it/s, mean train loss=0.22]\nvalidate [batch 12]: : 384it [00:01, 285.15it/s, mean loss=0.517, macro f1=0.755]\ntrain [epoch 8 batch 45]: : 1440it [00:04, 330.44it/s, mean train loss=0.173]\nvalidate [batch 12]: : 384it [00:01, 288.93it/s, mean loss=0.477, macro f1=0.753]\ntrain [epoch 9 batch 45]: : 1440it [00:04, 320.98it/s, mean train loss=0.156]\nvalidate [batch 12]: : 384it [00:01, 256.67it/s, mean loss=0.471, macro f1=0.74]\ntrain [epoch 10 batch 45]: : 1440it [00:05, 265.99it/s, mean train loss=0.132]\nvalidate [batch 12]: : 384it [00:01, 233.04it/s, mean loss=0.521, macro f1=0.767]\ntrain [epoch 11 batch 45]: : 1440it [00:05, 264.26it/s, mean train loss=0.11]\nvalidate [batch 12]: : 384it [00:01, 243.51it/s, mean loss=0.611, macro f1=0.734]\n\n\nEarly stopping at epoch 11\n\n\ntest [batch 29]: : 928it [00:03, 305.43it/s, mean loss=0.645, macro f1=0.736]\n\n\nTrial 16 fold 2\n\n\ntrain [epoch 1 batch 45]: : 1440it [00:04, 333.69it/s, mean train loss=1.27]\nvalidate [batch 12]: : 384it [00:01, 291.16it/s, mean loss=0.868, macro f1=0.586]\ntrain [epoch 2 batch 45]: : 1440it [00:04, 330.52it/s, mean train loss=0.658]\nvalidate [batch 12]: : 384it [00:01, 290.32it/s, mean loss=0.675, macro f1=0.672]\ntrain [epoch 3 batch 45]: : 1440it [00:04, 333.49it/s, mean train loss=0.494]\nvalidate [batch 12]: : 384it [00:01, 290.38it/s, mean loss=0.69, macro f1=0.665]\ntrain [epoch 4 batch 45]: : 1440it [00:04, 331.04it/s, mean train loss=0.404]\nvalidate [batch 12]: : 384it [00:01, 290.76it/s, mean loss=0.569, macro f1=0.742]\ntrain [epoch 5 batch 45]: : 1440it [00:04, 334.50it/s, mean train loss=0.338]\nvalidate [batch 12]: : 384it [00:01, 288.41it/s, mean loss=0.616, macro f1=0.706]\ntrain [epoch 6 batch 45]: : 1440it [00:04, 315.71it/s, mean train loss=0.259]\nvalidate [batch 12]: : 384it [00:01, 290.48it/s, mean loss=0.532, macro f1=0.755]\ntrain [epoch 7 batch 45]: : 1440it [00:04, 329.93it/s, mean train loss=0.2]\nvalidate [batch 12]: : 384it [00:01, 289.06it/s, mean loss=0.553, macro f1=0.774]\ntrain [epoch 8 batch 45]: : 1440it [00:04, 334.14it/s, mean train loss=0.167]\nvalidate [batch 12]: : 384it [00:01, 292.31it/s, mean loss=0.611, macro f1=0.747]\n\n\nEarly stopping at epoch 8\n\n\ntest [batch 29]: : 928it [00:02, 316.85it/s, mean loss=0.599, macro f1=0.741]\n\n\nTrial 16 fold 3\n\n\ntrain [epoch 1 batch 45]: : 1440it [00:04, 331.67it/s, mean train loss=1.19]\nvalidate [batch 12]: : 384it [00:01, 291.93it/s, mean loss=0.881, macro f1=0.54]\ntrain [epoch 2 batch 45]: : 1440it [00:04, 331.76it/s, mean train loss=0.633]\nvalidate [batch 12]: : 384it [00:01, 258.35it/s, mean loss=0.713, macro f1=0.671]\ntrain [epoch 3 batch 45]: : 1440it [00:04, 325.00it/s, mean train loss=0.533]\nvalidate [batch 12]: : 384it [00:01, 290.50it/s, mean loss=0.783, macro f1=0.649]\ntrain [epoch 4 batch 45]: : 1440it [00:04, 332.33it/s, mean train loss=0.399]\nvalidate [batch 12]: : 384it [00:01, 290.40it/s, mean loss=0.62, macro f1=0.723]\ntrain [epoch 5 batch 45]: : 1440it [00:04, 333.15it/s, mean train loss=0.319]\nvalidate [batch 12]: : 384it [00:01, 287.15it/s, mean loss=0.634, macro f1=0.701]\ntrain [epoch 6 batch 45]: : 1440it [00:04, 333.74it/s, mean train loss=0.243]\nvalidate [batch 12]: : 384it [00:01, 288.14it/s, mean loss=0.605, macro f1=0.705]\ntrain [epoch 7 batch 45]: : 1440it [00:04, 330.37it/s, mean train loss=0.21]\nvalidate [batch 12]: : 384it [00:01, 289.68it/s, mean loss=0.711, macro f1=0.678]\ntrain [epoch 8 batch 45]: : 1440it [00:04, 320.77it/s, mean train loss=0.183]\nvalidate [batch 12]: : 384it [00:01, 290.24it/s, mean loss=0.584, macro f1=0.694]\ntrain [epoch 9 batch 45]: : 1440it [00:04, 329.90it/s, mean train loss=0.146]\nvalidate [batch 12]: : 384it [00:01, 286.31it/s, mean loss=0.555, macro f1=0.738]\ntrain [epoch 10 batch 45]: : 1440it [00:04, 332.42it/s, mean train loss=0.137]\nvalidate [batch 12]: : 384it [00:01, 291.24it/s, mean loss=0.547, macro f1=0.712]\ntrain [epoch 11 batch 45]: : 1440it [00:04, 332.69it/s, mean train loss=0.108]\nvalidate [batch 12]: : 384it [00:01, 278.47it/s, mean loss=0.602, macro f1=0.728]\ntrain [epoch 12 batch 45]: : 1440it [00:05, 263.66it/s, mean train loss=0.104]\nvalidate [batch 12]: : 384it [00:01, 227.60it/s, mean loss=0.541, macro f1=0.719]\ntrain [epoch 13 batch 45]: : 1440it [00:05, 270.84it/s, mean train loss=0.086]\nvalidate [batch 12]: : 384it [00:01, 276.83it/s, mean loss=0.609, macro f1=0.704]\ntrain [epoch 14 batch 45]: : 1440it [00:04, 318.91it/s, mean train loss=0.0831]\nvalidate [batch 12]: : 384it [00:01, 269.90it/s, mean loss=0.664, macro f1=0.693]\n\n\nEarly stopping at epoch 14\n\n\ntest [batch 29]: : 928it [00:02, 316.82it/s, mean loss=0.637, macro f1=0.707]\n[I 2024-10-19 23:58:44,012] Trial 16 finished with value: 0.7278696750205723 and parameters: {'batch_size': 32, 'lr': 0.0003172086425995281, 'weight_decay': 0.0019517426716198715, 'reduce_lr_factor': 0.09943916989261473, 'momentum': 0.9433310742867136, 'early_stop_patience': 2, 'resize_length': 128, 'grayscale': False, 'jitter': False, 'horizontal_flip': False, 'rotation': False, 'mlp_n_layers': 5, 'mlp_layer_1_size': 256, 'mlp_layer_2_size': 512, 'mlp_layer_3_size': 512, 'mlp_activation_f': 'relu'}. Best is trial 11 with value: 0.7305797239149951.\n\n\nTrial 17 params: {'batch_size': 24, 'lr': 0.00040797051594697656, 'weight_decay': 5.339270186246678e-06, 'reduce_lr_factor': 0.07854333142924635, 'momentum': 0.950538140281347, 'early_stop_patience': 1, 'resize_length': 128, 'grayscale': False, 'jitter': False, 'horizontal_flip': False, 'rotation': False, 'mlp_n_layers': 4, 'mlp_layer_1_size': 256, 'mlp_layer_2_size': 1024, 'mlp_activation_f': 'relu'}\nTrial 17 fold 1\n\n\ntrain [epoch 1 batch 60]: : 1440it [00:04, 325.91it/s, mean train loss=1.03]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 265.74it/s, mean loss=0.726, macro f1=0.567]\ntrain [epoch 2 batch 60]: : 1440it [00:04, 324.97it/s, mean train loss=0.589]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 267.38it/s, mean loss=0.613, macro f1=0.669]\ntrain [epoch 3 batch 60]: : 1440it [00:04, 319.28it/s, mean train loss=0.43]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 251.53it/s, mean loss=0.545, macro f1=0.722]\ntrain [epoch 4 batch 60]: : 1440it [00:04, 329.06it/s, mean train loss=0.334]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 268.27it/s, mean loss=0.599, macro f1=0.645]\n\n\nEarly stopping at epoch 4\n\n\ntest [batch 38]: : 912it [00:02, 313.15it/s, mean loss=0.627, macro f1=0.659]\n\n\nTrial 17 fold 2\n\n\ntrain [epoch 1 batch 60]: : 1440it [00:04, 325.60it/s, mean train loss=0.999]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 269.54it/s, mean loss=0.74, macro f1=0.601]\ntrain [epoch 2 batch 60]: : 1440it [00:04, 324.99it/s, mean train loss=0.571]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 268.44it/s, mean loss=0.602, macro f1=0.61]\ntrain [epoch 3 batch 60]: : 1440it [00:04, 328.56it/s, mean train loss=0.447]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 270.82it/s, mean loss=0.678, macro f1=0.632]\n\n\nEarly stopping at epoch 3\n\n\ntest [batch 38]: : 912it [00:03, 275.15it/s, mean loss=0.642, macro f1=0.671]\n\n\nTrial 17 fold 3\n\n\ntrain [epoch 1 batch 60]: : 1440it [00:04, 323.82it/s, mean train loss=1.01]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 267.66it/s, mean loss=0.758, macro f1=0.59]\ntrain [epoch 2 batch 60]: : 1440it [00:04, 325.61it/s, mean train loss=0.557]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 267.23it/s, mean loss=0.587, macro f1=0.645]\ntrain [epoch 3 batch 60]: : 1440it [00:04, 325.49it/s, mean train loss=0.449]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 261.71it/s, mean loss=0.607, macro f1=0.688]\n\n\nEarly stopping at epoch 3\n\n\ntest [batch 38]: : 912it [00:02, 311.67it/s, mean loss=0.614, macro f1=0.678]\n[I 2024-10-19 23:59:56,071] Trial 17 finished with value: 0.6695770711293214 and parameters: {'batch_size': 24, 'lr': 0.00040797051594697656, 'weight_decay': 5.339270186246678e-06, 'reduce_lr_factor': 0.07854333142924635, 'momentum': 0.950538140281347, 'early_stop_patience': 1, 'resize_length': 128, 'grayscale': False, 'jitter': False, 'horizontal_flip': False, 'rotation': False, 'mlp_n_layers': 4, 'mlp_layer_1_size': 256, 'mlp_layer_2_size': 1024, 'mlp_activation_f': 'relu'}. Best is trial 11 with value: 0.7305797239149951.\n\n\nTrial 18 params: {'batch_size': 32, 'lr': 0.0006094583766468982, 'weight_decay': 0.0017801296557513435, 'reduce_lr_factor': 0.09860499834085121, 'momentum': 0.9394135106187597, 'early_stop_patience': 1, 'resize_length': 160, 'grayscale': False, 'jitter': False, 'horizontal_flip': False, 'rotation': False, 'mlp_n_layers': 5, 'mlp_layer_1_size': 256, 'mlp_layer_2_size': 512, 'mlp_layer_3_size': 512, 'mlp_activation_f': 'relu'}\nTrial 18 fold 1\n\n\ntrain [epoch 1 batch 45]: : 1440it [00:04, 327.04it/s, mean train loss=1.08]\nvalidate [batch 12]: : 384it [00:01, 282.65it/s, mean loss=0.837, macro f1=0.512]\ntrain [epoch 2 batch 45]: : 1440it [00:04, 315.54it/s, mean train loss=0.554]\nvalidate [batch 12]: : 384it [00:01, 288.51it/s, mean loss=0.653, macro f1=0.667]\ntrain [epoch 3 batch 45]: : 1440it [00:04, 325.98it/s, mean train loss=0.445]\nvalidate [batch 12]: : 384it [00:01, 289.30it/s, mean loss=0.574, macro f1=0.724]\ntrain [epoch 4 batch 45]: : 1440it [00:04, 328.13it/s, mean train loss=0.322]\nvalidate [batch 12]: : 384it [00:01, 285.77it/s, mean loss=0.522, macro f1=0.749]\ntrain [epoch 5 batch 45]: : 1440it [00:04, 325.02it/s, mean train loss=0.236]\nvalidate [batch 12]: : 384it [00:01, 291.97it/s, mean loss=0.627, macro f1=0.668]\n\n\nEarly stopping at epoch 5\n\n\ntest [batch 29]: : 928it [00:02, 321.74it/s, mean loss=0.664, macro f1=0.704]\n\n\nTrial 18 fold 2\n\n\ntrain [epoch 1 batch 45]: : 1440it [00:04, 323.10it/s, mean train loss=1.07]\nvalidate [batch 12]: : 384it [00:01, 269.82it/s, mean loss=0.782, macro f1=0.569]\ntrain [epoch 2 batch 45]: : 1440it [00:05, 274.48it/s, mean train loss=0.577]\nvalidate [batch 12]: : 384it [00:01, 239.93it/s, mean loss=0.707, macro f1=0.667]\ntrain [epoch 3 batch 45]: : 1440it [00:05, 261.57it/s, mean train loss=0.499]\nvalidate [batch 12]: : 384it [00:01, 231.58it/s, mean loss=0.585, macro f1=0.72]\ntrain [epoch 4 batch 45]: : 1440it [00:05, 287.93it/s, mean train loss=0.354]\nvalidate [batch 12]: : 384it [00:01, 261.13it/s, mean loss=0.581, macro f1=0.708]\ntrain [epoch 5 batch 45]: : 1440it [00:04, 291.11it/s, mean train loss=0.315]\nvalidate [batch 12]: : 384it [00:01, 291.49it/s, mean loss=0.578, macro f1=0.747]\ntrain [epoch 6 batch 45]: : 1440it [00:04, 318.57it/s, mean train loss=0.227]\nvalidate [batch 12]: : 384it [00:01, 280.47it/s, mean loss=0.516, macro f1=0.747]\ntrain [epoch 7 batch 45]: : 1440it [00:04, 327.46it/s, mean train loss=0.165]\nvalidate [batch 12]: : 384it [00:01, 288.64it/s, mean loss=0.566, macro f1=0.74]\n\n\nEarly stopping at epoch 7\n\n\ntest [batch 29]: : 928it [00:02, 320.63it/s, mean loss=0.531, macro f1=0.724]\n\n\nTrial 18 fold 3\n\n\ntrain [epoch 1 batch 45]: : 1440it [00:04, 327.74it/s, mean train loss=1.1]\nvalidate [batch 12]: : 384it [00:01, 293.16it/s, mean loss=0.875, macro f1=0.512]\ntrain [epoch 2 batch 45]: : 1440it [00:04, 329.63it/s, mean train loss=0.648]\nvalidate [batch 12]: : 384it [00:01, 286.65it/s, mean loss=0.652, macro f1=0.654]\ntrain [epoch 3 batch 45]: : 1440it [00:04, 329.26it/s, mean train loss=0.461]\nvalidate [batch 12]: : 384it [00:01, 291.37it/s, mean loss=0.636, macro f1=0.746]\ntrain [epoch 4 batch 45]: : 1440it [00:04, 314.83it/s, mean train loss=0.346]\nvalidate [batch 12]: : 384it [00:01, 287.90it/s, mean loss=0.669, macro f1=0.683]\n\n\nEarly stopping at epoch 4\n\n\ntest [batch 29]: : 928it [00:02, 319.55it/s, mean loss=0.62, macro f1=0.676]\n[I 2024-10-20 00:01:47,844] Trial 18 finished with value: 0.7014137777813739 and parameters: {'batch_size': 32, 'lr': 0.0006094583766468982, 'weight_decay': 0.0017801296557513435, 'reduce_lr_factor': 0.09860499834085121, 'momentum': 0.9394135106187597, 'early_stop_patience': 1, 'resize_length': 160, 'grayscale': False, 'jitter': False, 'horizontal_flip': False, 'rotation': False, 'mlp_n_layers': 5, 'mlp_layer_1_size': 256, 'mlp_layer_2_size': 512, 'mlp_layer_3_size': 512, 'mlp_activation_f': 'relu'}. Best is trial 11 with value: 0.7305797239149951.\n\n\nTrial 19 params: {'batch_size': 32, 'lr': 0.0003689161140662125, 'weight_decay': 0.005486335654053626, 'reduce_lr_factor': 0.033106589596079095, 'momentum': 0.9853840928002924, 'early_stop_patience': 2, 'resize_length': 64, 'grayscale': False, 'jitter': False, 'horizontal_flip': False, 'rotation': False, 'mlp_n_layers': 5, 'mlp_layer_1_size': 256, 'mlp_layer_2_size': 512, 'mlp_layer_3_size': 256, 'mlp_activation_f': 'relu'}\nTrial 19 fold 1\n\n\ntrain [epoch 1 batch 45]: : 1440it [00:04, 338.31it/s, mean train loss=1.26]\nvalidate [batch 12]: : 384it [00:01, 292.19it/s, mean loss=0.919, macro f1=0.491]\ntrain [epoch 2 batch 45]: : 1440it [00:04, 342.02it/s, mean train loss=0.67]\nvalidate [batch 12]: : 384it [00:01, 290.04it/s, mean loss=0.68, macro f1=0.658]\ntrain [epoch 3 batch 45]: : 1440it [00:04, 339.35it/s, mean train loss=0.545]\nvalidate [batch 12]: : 384it [00:01, 292.30it/s, mean loss=0.586, macro f1=0.704]\ntrain [epoch 4 batch 45]: : 1440it [00:04, 340.55it/s, mean train loss=0.433]\nvalidate [batch 12]: : 384it [00:01, 288.48it/s, mean loss=0.539, macro f1=0.726]\ntrain [epoch 5 batch 45]: : 1440it [00:04, 322.25it/s, mean train loss=0.332]\nvalidate [batch 12]: : 384it [00:01, 287.78it/s, mean loss=0.53, macro f1=0.726]\ntrain [epoch 6 batch 45]: : 1440it [00:04, 339.30it/s, mean train loss=0.302]\nvalidate [batch 12]: : 384it [00:01, 292.36it/s, mean loss=0.549, macro f1=0.735]\ntrain [epoch 7 batch 45]: : 1440it [00:04, 341.31it/s, mean train loss=0.25]\nvalidate [batch 12]: : 384it [00:01, 287.84it/s, mean loss=0.504, macro f1=0.762]\ntrain [epoch 8 batch 45]: : 1440it [00:04, 335.98it/s, mean train loss=0.199]\nvalidate [batch 12]: : 384it [00:01, 290.16it/s, mean loss=0.469, macro f1=0.744]\ntrain [epoch 9 batch 45]: : 1440it [00:04, 341.96it/s, mean train loss=0.182]\nvalidate [batch 12]: : 384it [00:01, 291.60it/s, mean loss=0.503, macro f1=0.767]\ntrain [epoch 10 batch 45]: : 1440it [00:04, 318.65it/s, mean train loss=0.181]\nvalidate [batch 12]: : 384it [00:01, 290.40it/s, mean loss=0.588, macro f1=0.749]\n\n\nEarly stopping at epoch 10\n\n\ntest [batch 29]: : 928it [00:02, 321.55it/s, mean loss=0.723, macro f1=0.759]\n\n\nTrial 19 fold 2\n\n\ntrain [epoch 1 batch 45]: : 1440it [00:04, 337.54it/s, mean train loss=1.31]\nvalidate [batch 12]: : 384it [00:01, 292.93it/s, mean loss=1.02, macro f1=0.516]\ntrain [epoch 2 batch 45]: : 1440it [00:04, 310.29it/s, mean train loss=0.68]\nvalidate [batch 12]: : 384it [00:01, 259.52it/s, mean loss=0.703, macro f1=0.641]\ntrain [epoch 3 batch 45]: : 1440it [00:05, 262.56it/s, mean train loss=0.52]\nvalidate [batch 12]: : 384it [00:01, 280.16it/s, mean loss=0.614, macro f1=0.66]\ntrain [epoch 4 batch 45]: : 1440it [00:05, 279.30it/s, mean train loss=0.435]\nvalidate [batch 12]: : 384it [00:01, 234.63it/s, mean loss=0.55, macro f1=0.71]\ntrain [epoch 5 batch 45]: : 1440it [00:04, 320.61it/s, mean train loss=0.367]\nvalidate [batch 12]: : 384it [00:01, 290.65it/s, mean loss=0.565, macro f1=0.76]\ntrain [epoch 6 batch 45]: : 1440it [00:04, 340.14it/s, mean train loss=0.304]\nvalidate [batch 12]: : 384it [00:01, 289.89it/s, mean loss=0.56, macro f1=0.748]\n\n\nEarly stopping at epoch 6\n\n\ntest [batch 29]: : 928it [00:02, 316.69it/s, mean loss=0.544, macro f1=0.728]\n\n\nTrial 19 fold 3\n\n\ntrain [epoch 1 batch 45]: : 1440it [00:04, 341.42it/s, mean train loss=1.25]\nvalidate [batch 12]: : 384it [00:01, 291.45it/s, mean loss=0.995, macro f1=0.426]\ntrain [epoch 2 batch 45]: : 1440it [00:04, 337.80it/s, mean train loss=0.742]\nvalidate [batch 12]: : 384it [00:01, 289.41it/s, mean loss=0.712, macro f1=0.65]\ntrain [epoch 3 batch 45]: : 1440it [00:04, 322.70it/s, mean train loss=0.513]\nvalidate [batch 12]: : 384it [00:01, 285.94it/s, mean loss=0.627, macro f1=0.717]\ntrain [epoch 4 batch 45]: : 1440it [00:04, 338.93it/s, mean train loss=0.407]\nvalidate [batch 12]: : 384it [00:01, 290.89it/s, mean loss=0.505, macro f1=0.743]\ntrain [epoch 5 batch 45]: : 1440it [00:04, 340.54it/s, mean train loss=0.364]\nvalidate [batch 12]: : 384it [00:01, 280.68it/s, mean loss=0.581, macro f1=0.732]\ntrain [epoch 6 batch 45]: : 1440it [00:04, 321.22it/s, mean train loss=0.276]\nvalidate [batch 12]: : 384it [00:01, 290.06it/s, mean loss=0.562, macro f1=0.712]\n\n\nEarly stopping at epoch 6\n\n\ntest [batch 29]: : 928it [00:02, 321.30it/s, mean loss=0.564, macro f1=0.684]\n[I 2024-10-20 00:04:11,983] Trial 19 finished with value: 0.7238061057192202 and parameters: {'batch_size': 32, 'lr': 0.0003689161140662125, 'weight_decay': 0.005486335654053626, 'reduce_lr_factor': 0.033106589596079095, 'momentum': 0.9853840928002924, 'early_stop_patience': 2, 'resize_length': 64, 'grayscale': False, 'jitter': False, 'horizontal_flip': False, 'rotation': False, 'mlp_n_layers': 5, 'mlp_layer_1_size': 256, 'mlp_layer_2_size': 512, 'mlp_layer_3_size': 256, 'mlp_activation_f': 'relu'}. Best is trial 11 with value: 0.7305797239149951.\n\n\nTrial 20 params: {'batch_size': 24, 'lr': 2.465084092827328e-05, 'weight_decay': 0.0021603795369289007, 'reduce_lr_factor': 0.0611390638743071, 'momentum': 0.9295361802517457, 'early_stop_patience': 2, 'resize_length': 128, 'grayscale': False, 'jitter': False, 'horizontal_flip': False, 'rotation': False, 'mlp_n_layers': 4, 'mlp_layer_1_size': 256, 'mlp_layer_2_size': 512, 'mlp_activation_f': 'relu'}\nTrial 20 fold 1\n\n\ntrain [epoch 1 batch 60]: : 1440it [00:04, 321.86it/s, mean train loss=1.6]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 265.36it/s, mean loss=1.19, macro f1=0.459]\ntrain [epoch 2 batch 60]: : 1440it [00:04, 305.27it/s, mean train loss=1.07]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 267.29it/s, mean loss=1.01, macro f1=0.575]\ntrain [epoch 3 batch 60]: : 1440it [00:04, 327.25it/s, mean train loss=0.919]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 267.31it/s, mean loss=0.91, macro f1=0.612]\ntrain [epoch 4 batch 60]: : 1440it [00:04, 325.88it/s, mean train loss=0.789]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 266.95it/s, mean loss=0.857, macro f1=0.602]\ntrain [epoch 5 batch 60]: : 1440it [00:04, 325.68it/s, mean train loss=0.753]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 261.91it/s, mean loss=0.792, macro f1=0.638]\ntrain [epoch 6 batch 60]: : 1440it [00:04, 323.94it/s, mean train loss=0.668]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 264.40it/s, mean loss=0.767, macro f1=0.664]\n[I 2024-10-20 00:04:49,570] Trial 20 pruned. \n\n\nTrial 21 params: {'batch_size': 32, 'lr': 0.0003514663712832598, 'weight_decay': 0.005635507431427942, 'reduce_lr_factor': 0.029753600418150557, 'momentum': 0.9872940644377142, 'early_stop_patience': 2, 'resize_length': 64, 'grayscale': False, 'jitter': False, 'horizontal_flip': False, 'rotation': False, 'mlp_n_layers': 5, 'mlp_layer_1_size': 256, 'mlp_layer_2_size': 512, 'mlp_layer_3_size': 256, 'mlp_activation_f': 'relu'}\nTrial 21 fold 1\n\n\ntrain [epoch 1 batch 45]: : 1440it [00:04, 304.34it/s, mean train loss=1.35]\nvalidate [batch 12]: : 384it [00:01, 286.40it/s, mean loss=1.03, macro f1=0.421]\ntrain [epoch 2 batch 45]: : 1440it [00:04, 336.89it/s, mean train loss=0.719]\nvalidate [batch 12]: : 384it [00:01, 280.27it/s, mean loss=0.727, macro f1=0.582]\ntrain [epoch 3 batch 45]: : 1440it [00:04, 337.93it/s, mean train loss=0.557]\nvalidate [batch 12]: : 384it [00:01, 289.77it/s, mean loss=0.619, macro f1=0.659]\ntrain [epoch 4 batch 45]: : 1440it [00:04, 336.96it/s, mean train loss=0.406]\nvalidate [batch 12]: : 384it [00:01, 290.80it/s, mean loss=0.601, macro f1=0.68]\ntrain [epoch 5 batch 45]: : 1440it [00:04, 338.75it/s, mean train loss=0.325]\nvalidate [batch 12]: : 384it [00:01, 287.99it/s, mean loss=0.489, macro f1=0.716]\ntrain [epoch 6 batch 45]: : 1440it [00:04, 290.40it/s, mean train loss=0.304]\nvalidate [batch 12]: : 384it [00:01, 257.36it/s, mean loss=0.485, macro f1=0.754]\ntrain [epoch 7 batch 45]: : 1440it [00:05, 283.92it/s, mean train loss=0.277]\nvalidate [batch 12]: : 384it [00:01, 232.25it/s, mean loss=0.514, macro f1=0.737]\ntrain [epoch 8 batch 45]: : 1440it [00:05, 266.56it/s, mean train loss=0.227]\nvalidate [batch 12]: : 384it [00:01, 277.39it/s, mean loss=0.467, macro f1=0.759]\ntrain [epoch 9 batch 45]: : 1440it [00:04, 328.69it/s, mean train loss=0.191]\nvalidate [batch 12]: : 384it [00:01, 289.07it/s, mean loss=0.415, macro f1=0.772]\ntrain [epoch 10 batch 45]: : 1440it [00:04, 336.91it/s, mean train loss=0.171]\nvalidate [batch 12]: : 384it [00:01, 276.86it/s, mean loss=0.42, macro f1=0.779]\ntrain [epoch 11 batch 45]: : 1440it [00:04, 326.34it/s, mean train loss=0.122]\nvalidate [batch 12]: : 384it [00:01, 285.62it/s, mean loss=0.515, macro f1=0.787]\n\n\nEarly stopping at epoch 11\n\n\ntest [batch 29]: : 928it [00:02, 320.06it/s, mean loss=0.586, macro f1=0.772]\n\n\nTrial 21 fold 2\n\n\ntrain [epoch 1 batch 45]: : 1440it [00:04, 336.93it/s, mean train loss=1.35]\nvalidate [batch 12]: : 384it [00:01, 283.44it/s, mean loss=1.01, macro f1=0.491]\ntrain [epoch 2 batch 45]: : 1440it [00:04, 336.27it/s, mean train loss=0.821]\nvalidate [batch 12]: : 384it [00:01, 290.17it/s, mean loss=0.669, macro f1=0.609]\ntrain [epoch 3 batch 45]: : 1440it [00:04, 336.83it/s, mean train loss=0.539]\nvalidate [batch 12]: : 384it [00:01, 292.29it/s, mean loss=0.646, macro f1=0.649]\ntrain [epoch 4 batch 45]: : 1440it [00:04, 337.70it/s, mean train loss=0.479]\nvalidate [batch 12]: : 384it [00:01, 290.91it/s, mean loss=0.539, macro f1=0.685]\ntrain [epoch 5 batch 45]: : 1440it [00:04, 315.52it/s, mean train loss=0.432]\nvalidate [batch 12]: : 384it [00:01, 284.92it/s, mean loss=0.567, macro f1=0.722]\ntrain [epoch 6 batch 45]: : 1440it [00:04, 341.37it/s, mean train loss=0.349]\nvalidate [batch 12]: : 384it [00:01, 291.38it/s, mean loss=0.477, macro f1=0.776]\ntrain [epoch 7 batch 45]: : 1440it [00:04, 338.89it/s, mean train loss=0.263]\nvalidate [batch 12]: : 384it [00:01, 290.67it/s, mean loss=0.532, macro f1=0.753]\ntrain [epoch 8 batch 45]: : 1440it [00:04, 340.17it/s, mean train loss=0.206]\nvalidate [batch 12]: : 384it [00:01, 291.87it/s, mean loss=0.549, macro f1=0.759]\n\n\nEarly stopping at epoch 8\n\n\ntest [batch 29]: : 928it [00:02, 315.86it/s, mean loss=0.536, macro f1=0.749]\n\n\nTrial 21 fold 3\n\n\ntrain [epoch 1 batch 45]: : 1440it [00:04, 339.85it/s, mean train loss=1.3]\nvalidate [batch 12]: : 384it [00:01, 254.36it/s, mean loss=1.1, macro f1=0.336]\ntrain [epoch 2 batch 45]: : 1440it [00:04, 328.76it/s, mean train loss=0.72]\nvalidate [batch 12]: : 384it [00:01, 292.60it/s, mean loss=0.752, macro f1=0.572]\ntrain [epoch 3 batch 45]: : 1440it [00:04, 341.06it/s, mean train loss=0.565]\nvalidate [batch 12]: : 384it [00:01, 292.54it/s, mean loss=0.63, macro f1=0.716]\ntrain [epoch 4 batch 45]: : 1440it [00:04, 336.56it/s, mean train loss=0.42]\nvalidate [batch 12]: : 384it [00:01, 292.08it/s, mean loss=0.539, macro f1=0.705]\ntrain [epoch 5 batch 45]: : 1440it [00:04, 338.37it/s, mean train loss=0.37]\nvalidate [batch 12]: : 384it [00:01, 284.94it/s, mean loss=0.548, macro f1=0.712]\ntrain [epoch 6 batch 45]: : 1440it [00:04, 340.20it/s, mean train loss=0.299]\nvalidate [batch 12]: : 384it [00:01, 291.07it/s, mean loss=0.489, macro f1=0.77]\ntrain [epoch 7 batch 45]: : 1440it [00:04, 322.16it/s, mean train loss=0.248]\nvalidate [batch 12]: : 384it [00:01, 290.99it/s, mean loss=0.53, macro f1=0.704]\ntrain [epoch 8 batch 45]: : 1440it [00:04, 342.25it/s, mean train loss=0.236]\nvalidate [batch 12]: : 384it [00:01, 291.01it/s, mean loss=0.557, macro f1=0.747]\n\n\nEarly stopping at epoch 8\n\n\ntest [batch 29]: : 928it [00:03, 279.59it/s, mean loss=0.522, macro f1=0.746]\n[I 2024-10-20 00:07:44,438] Trial 21 finished with value: 0.7554649753224488 and parameters: {'batch_size': 32, 'lr': 0.0003514663712832598, 'weight_decay': 0.005635507431427942, 'reduce_lr_factor': 0.029753600418150557, 'momentum': 0.9872940644377142, 'early_stop_patience': 2, 'resize_length': 64, 'grayscale': False, 'jitter': False, 'horizontal_flip': False, 'rotation': False, 'mlp_n_layers': 5, 'mlp_layer_1_size': 256, 'mlp_layer_2_size': 512, 'mlp_layer_3_size': 256, 'mlp_activation_f': 'relu'}. Best is trial 21 with value: 0.7554649753224488.\n\n\nTrial 22 params: {'batch_size': 32, 'lr': 0.000202737946258828, 'weight_decay': 0.009833824537831395, 'reduce_lr_factor': 0.02851445389188707, 'momentum': 0.9865118683640303, 'early_stop_patience': 2, 'resize_length': 64, 'grayscale': False, 'jitter': False, 'horizontal_flip': False, 'rotation': False, 'mlp_n_layers': 5, 'mlp_layer_1_size': 256, 'mlp_layer_2_size': 512, 'mlp_layer_3_size': 256, 'mlp_activation_f': 'relu'}\nTrial 22 fold 1\n\n\ntrain [epoch 1 batch 45]: : 1440it [00:04, 323.38it/s, mean train loss=1.44]\nvalidate [batch 12]: : 384it [00:01, 237.98it/s, mean loss=1.03, macro f1=0.447]\ntrain [epoch 2 batch 45]: : 1440it [00:05, 260.76it/s, mean train loss=0.818]\nvalidate [batch 12]: : 384it [00:01, 234.38it/s, mean loss=0.751, macro f1=0.589]\ntrain [epoch 3 batch 45]: : 1440it [00:05, 269.62it/s, mean train loss=0.59]\nvalidate [batch 12]: : 384it [00:01, 290.13it/s, mean loss=0.644, macro f1=0.695]\ntrain [epoch 4 batch 45]: : 1440it [00:04, 338.90it/s, mean train loss=0.48]\nvalidate [batch 12]: : 384it [00:01, 289.29it/s, mean loss=0.556, macro f1=0.705]\ntrain [epoch 5 batch 45]: : 1440it [00:04, 338.29it/s, mean train loss=0.423]\nvalidate [batch 12]: : 384it [00:01, 287.31it/s, mean loss=0.544, macro f1=0.731]\ntrain [epoch 6 batch 45]: : 1440it [00:04, 340.61it/s, mean train loss=0.372]\nvalidate [batch 12]: : 384it [00:01, 284.78it/s, mean loss=0.519, macro f1=0.735]\ntrain [epoch 7 batch 45]: : 1440it [00:04, 339.96it/s, mean train loss=0.277]\nvalidate [batch 12]: : 384it [00:01, 288.48it/s, mean loss=0.506, macro f1=0.729]\ntrain [epoch 8 batch 45]: : 1440it [00:04, 337.61it/s, mean train loss=0.243]\nvalidate [batch 12]: : 384it [00:01, 266.14it/s, mean loss=0.539, macro f1=0.745]\ntrain [epoch 9 batch 45]: : 1440it [00:04, 338.21it/s, mean train loss=0.215]\nvalidate [batch 12]: : 384it [00:01, 288.87it/s, mean loss=0.43, macro f1=0.765]\ntrain [epoch 10 batch 45]: : 1440it [00:04, 336.71it/s, mean train loss=0.184]\nvalidate [batch 12]: : 384it [00:01, 289.53it/s, mean loss=0.456, macro f1=0.757]\ntrain [epoch 11 batch 45]: : 1440it [00:04, 339.61it/s, mean train loss=0.158]\nvalidate [batch 12]: : 384it [00:01, 283.95it/s, mean loss=0.441, macro f1=0.779]\n\n\nEarly stopping at epoch 11\n\n\ntest [batch 29]: : 928it [00:02, 319.66it/s, mean loss=0.581, macro f1=0.749]\n\n\nTrial 22 fold 2\n\n\ntrain [epoch 1 batch 45]: : 1440it [00:04, 341.32it/s, mean train loss=1.43]\nvalidate [batch 12]: : 384it [00:01, 286.00it/s, mean loss=1.08, macro f1=0.447]\ntrain [epoch 2 batch 45]: : 1440it [00:04, 319.26it/s, mean train loss=0.839]\nvalidate [batch 12]: : 384it [00:01, 269.00it/s, mean loss=0.775, macro f1=0.621]\ntrain [epoch 3 batch 45]: : 1440it [00:04, 333.94it/s, mean train loss=0.613]\nvalidate [batch 12]: : 384it [00:01, 288.86it/s, mean loss=0.666, macro f1=0.657]\ntrain [epoch 4 batch 45]: : 1440it [00:04, 336.47it/s, mean train loss=0.562]\nvalidate [batch 12]: : 384it [00:01, 289.34it/s, mean loss=0.58, macro f1=0.73]\ntrain [epoch 5 batch 45]: : 1440it [00:04, 336.25it/s, mean train loss=0.456]\nvalidate [batch 12]: : 384it [00:01, 290.24it/s, mean loss=0.525, macro f1=0.714]\ntrain [epoch 6 batch 45]: : 1440it [00:04, 340.02it/s, mean train loss=0.342]\nvalidate [batch 12]: : 384it [00:01, 287.17it/s, mean loss=0.607, macro f1=0.741]\ntrain [epoch 7 batch 45]: : 1440it [00:04, 336.92it/s, mean train loss=0.277]\nvalidate [batch 12]: : 384it [00:01, 275.38it/s, mean loss=0.525, macro f1=0.776]\ntrain [epoch 8 batch 45]: : 1440it [00:04, 319.44it/s, mean train loss=0.26]\nvalidate [batch 12]: : 384it [00:01, 292.10it/s, mean loss=0.526, macro f1=0.749]\ntrain [epoch 9 batch 45]: : 1440it [00:04, 338.48it/s, mean train loss=0.222]\nvalidate [batch 12]: : 384it [00:01, 291.73it/s, mean loss=0.553, macro f1=0.757]\n\n\nEarly stopping at epoch 9\n\n\ntest [batch 29]: : 928it [00:02, 318.64it/s, mean loss=0.509, macro f1=0.744]\n\n\nTrial 22 fold 3\n\n\ntrain [epoch 1 batch 45]: : 1440it [00:04, 339.79it/s, mean train loss=1.41]\nvalidate [batch 12]: : 384it [00:01, 287.29it/s, mean loss=1.02, macro f1=0.402]\ntrain [epoch 2 batch 45]: : 1440it [00:04, 336.77it/s, mean train loss=0.777]\nvalidate [batch 12]: : 384it [00:01, 228.00it/s, mean loss=0.778, macro f1=0.587]\ntrain [epoch 3 batch 45]: : 1440it [00:05, 262.03it/s, mean train loss=0.602]\nvalidate [batch 12]: : 384it [00:01, 245.85it/s, mean loss=0.68, macro f1=0.634]\ntrain [epoch 4 batch 45]: : 1440it [00:04, 312.71it/s, mean train loss=0.5]\nvalidate [batch 12]: : 384it [00:01, 235.10it/s, mean loss=0.585, macro f1=0.681]\ntrain [epoch 5 batch 45]: : 1440it [00:05, 283.48it/s, mean train loss=0.412]\nvalidate [batch 12]: : 384it [00:01, 268.14it/s, mean loss=0.544, macro f1=0.707]\ntrain [epoch 6 batch 45]: : 1440it [00:04, 340.32it/s, mean train loss=0.334]\nvalidate [batch 12]: : 384it [00:01, 286.41it/s, mean loss=0.53, macro f1=0.746]\ntrain [epoch 7 batch 45]: : 1440it [00:04, 338.34it/s, mean train loss=0.294]\nvalidate [batch 12]: : 384it [00:01, 288.59it/s, mean loss=0.521, macro f1=0.716]\ntrain [epoch 8 batch 45]: : 1440it [00:04, 321.23it/s, mean train loss=0.228]\nvalidate [batch 12]: : 384it [00:01, 289.19it/s, mean loss=0.538, macro f1=0.721]\ntrain [epoch 9 batch 45]: : 1440it [00:04, 337.19it/s, mean train loss=0.219]\nvalidate [batch 12]: : 384it [00:01, 289.31it/s, mean loss=0.489, macro f1=0.759]\ntrain [epoch 10 batch 45]: : 1440it [00:04, 335.63it/s, mean train loss=0.179]\nvalidate [batch 12]: : 384it [00:01, 289.85it/s, mean loss=0.559, macro f1=0.742]\ntrain [epoch 11 batch 45]: : 1440it [00:04, 338.04it/s, mean train loss=0.159]\nvalidate [batch 12]: : 384it [00:01, 290.46it/s, mean loss=0.609, macro f1=0.772]\n\n\nEarly stopping at epoch 11\n\n\ntest [batch 29]: : 928it [00:02, 313.30it/s, mean loss=0.503, macro f1=0.749]\n[I 2024-10-20 00:11:05,377] Trial 22 finished with value: 0.7474029635781916 and parameters: {'batch_size': 32, 'lr': 0.000202737946258828, 'weight_decay': 0.009833824537831395, 'reduce_lr_factor': 0.02851445389188707, 'momentum': 0.9865118683640303, 'early_stop_patience': 2, 'resize_length': 64, 'grayscale': False, 'jitter': False, 'horizontal_flip': False, 'rotation': False, 'mlp_n_layers': 5, 'mlp_layer_1_size': 256, 'mlp_layer_2_size': 512, 'mlp_layer_3_size': 256, 'mlp_activation_f': 'relu'}. Best is trial 21 with value: 0.7554649753224488.\n\n\nTrial 23 params: {'batch_size': 32, 'lr': 0.00017571495299283302, 'weight_decay': 0.009726305764949416, 'reduce_lr_factor': 0.02894806164879947, 'momentum': 0.9737434356784762, 'early_stop_patience': 2, 'resize_length': 64, 'grayscale': False, 'jitter': False, 'horizontal_flip': False, 'rotation': False, 'mlp_n_layers': 5, 'mlp_layer_1_size': 256, 'mlp_layer_2_size': 512, 'mlp_layer_3_size': 256, 'mlp_activation_f': 'relu'}\nTrial 23 fold 1\n\n\ntrain [epoch 1 batch 45]: : 1440it [00:04, 338.24it/s, mean train loss=1.42]\nvalidate [batch 12]: : 384it [00:01, 290.04it/s, mean loss=1.02, macro f1=0.478]\ntrain [epoch 2 batch 45]: : 1440it [00:04, 322.06it/s, mean train loss=0.859]\nvalidate [batch 12]: : 384it [00:01, 283.55it/s, mean loss=0.775, macro f1=0.582]\ntrain [epoch 3 batch 45]: : 1440it [00:04, 339.55it/s, mean train loss=0.652]\nvalidate [batch 12]: : 384it [00:01, 289.28it/s, mean loss=0.682, macro f1=0.667]\ntrain [epoch 4 batch 45]: : 1440it [00:04, 340.80it/s, mean train loss=0.497]\nvalidate [batch 12]: : 384it [00:01, 289.77it/s, mean loss=0.637, macro f1=0.686]\ntrain [epoch 5 batch 45]: : 1440it [00:04, 339.12it/s, mean train loss=0.425]\nvalidate [batch 12]: : 384it [00:01, 290.76it/s, mean loss=0.584, macro f1=0.724]\ntrain [epoch 6 batch 45]: : 1440it [00:04, 339.53it/s, mean train loss=0.345]\nvalidate [batch 12]: : 384it [00:01, 291.77it/s, mean loss=0.559, macro f1=0.74]\ntrain [epoch 7 batch 45]: : 1440it [00:04, 329.29it/s, mean train loss=0.298]\nvalidate [batch 12]: : 384it [00:01, 280.47it/s, mean loss=0.522, macro f1=0.716]\ntrain [epoch 8 batch 45]: : 1440it [00:04, 339.76it/s, mean train loss=0.241]\nvalidate [batch 12]: : 384it [00:01, 286.05it/s, mean loss=0.524, macro f1=0.764]\ntrain [epoch 9 batch 45]: : 1440it [00:04, 339.84it/s, mean train loss=0.233]\nvalidate [batch 12]: : 384it [00:01, 290.45it/s, mean loss=0.497, macro f1=0.743]\ntrain [epoch 10 batch 45]: : 1440it [00:04, 336.80it/s, mean train loss=0.218]\nvalidate [batch 12]: : 384it [00:01, 290.46it/s, mean loss=0.444, macro f1=0.805]\ntrain [epoch 11 batch 45]: : 1440it [00:04, 339.22it/s, mean train loss=0.2]\nvalidate [batch 12]: : 384it [00:01, 289.99it/s, mean loss=0.528, macro f1=0.773]\ntrain [epoch 12 batch 45]: : 1440it [00:04, 336.77it/s, mean train loss=0.15]\nvalidate [batch 12]: : 384it [00:01, 290.61it/s, mean loss=0.508, macro f1=0.767]\n\n\nEarly stopping at epoch 12\n\n\ntest [batch 29]: : 928it [00:03, 279.97it/s, mean loss=0.57, macro f1=0.758]\n\n\nTrial 23 fold 2\n\n\ntrain [epoch 1 batch 45]: : 1440it [00:04, 335.34it/s, mean train loss=1.51]\nvalidate [batch 12]: : 384it [00:01, 254.09it/s, mean loss=1.09, macro f1=0.437]\ntrain [epoch 2 batch 45]: : 1440it [00:04, 298.29it/s, mean train loss=0.864]\nvalidate [batch 12]: : 384it [00:01, 238.32it/s, mean loss=0.809, macro f1=0.571]\ntrain [epoch 3 batch 45]: : 1440it [00:05, 266.27it/s, mean train loss=0.679]\nvalidate [batch 12]: : 384it [00:01, 233.46it/s, mean loss=0.694, macro f1=0.661]\ntrain [epoch 4 batch 45]: : 1440it [00:04, 297.27it/s, mean train loss=0.552]\nvalidate [batch 12]: : 384it [00:01, 281.14it/s, mean loss=0.62, macro f1=0.65]\ntrain [epoch 5 batch 45]: : 1440it [00:04, 306.57it/s, mean train loss=0.482]\nvalidate [batch 12]: : 384it [00:01, 291.46it/s, mean loss=0.622, macro f1=0.668]\ntrain [epoch 6 batch 45]: : 1440it [00:04, 338.67it/s, mean train loss=0.379]\nvalidate [batch 12]: : 384it [00:01, 292.26it/s, mean loss=0.632, macro f1=0.745]\n\n\nEarly stopping at epoch 6\n\n\ntest [batch 29]: : 928it [00:02, 316.50it/s, mean loss=0.581, macro f1=0.722]\n\n\nTrial 23 fold 3\n\n\ntrain [epoch 1 batch 45]: : 1440it [00:04, 339.08it/s, mean train loss=1.49]\nvalidate [batch 12]: : 384it [00:01, 291.83it/s, mean loss=1.11, macro f1=0.437]\ntrain [epoch 2 batch 45]: : 1440it [00:04, 338.02it/s, mean train loss=0.811]\nvalidate [batch 12]: : 384it [00:01, 290.40it/s, mean loss=0.764, macro f1=0.566]\ntrain [epoch 3 batch 45]: : 1440it [00:04, 337.19it/s, mean train loss=0.632]\nvalidate [batch 12]: : 384it [00:01, 291.03it/s, mean loss=0.682, macro f1=0.635]\ntrain [epoch 4 batch 45]: : 1440it [00:04, 302.42it/s, mean train loss=0.545]\nvalidate [batch 12]: : 384it [00:01, 283.37it/s, mean loss=0.674, macro f1=0.715]\ntrain [epoch 5 batch 45]: : 1440it [00:04, 337.40it/s, mean train loss=0.428]\nvalidate [batch 12]: : 384it [00:01, 277.56it/s, mean loss=0.648, macro f1=0.673]\ntrain [epoch 6 batch 45]: : 1440it [00:04, 315.50it/s, mean train loss=0.362]\nvalidate [batch 12]: : 384it [00:01, 275.64it/s, mean loss=0.567, macro f1=0.733]\ntrain [epoch 7 batch 45]: : 1440it [00:04, 330.34it/s, mean train loss=0.328]\nvalidate [batch 12]: : 384it [00:01, 274.43it/s, mean loss=0.625, macro f1=0.705]\ntrain [epoch 8 batch 45]: : 1440it [00:04, 323.70it/s, mean train loss=0.281]\nvalidate [batch 12]: : 384it [00:01, 268.20it/s, mean loss=0.517, macro f1=0.747]\ntrain [epoch 9 batch 45]: : 1440it [00:04, 303.44it/s, mean train loss=0.241]\nvalidate [batch 12]: : 384it [00:01, 276.61it/s, mean loss=0.522, macro f1=0.744]\ntrain [epoch 10 batch 45]: : 1440it [00:04, 326.51it/s, mean train loss=0.189]\nvalidate [batch 12]: : 384it [00:01, 277.63it/s, mean loss=0.482, macro f1=0.769]\ntrain [epoch 11 batch 45]: : 1440it [00:04, 330.01it/s, mean train loss=0.205]\nvalidate [batch 12]: : 384it [00:01, 280.44it/s, mean loss=0.561, macro f1=0.715]\ntrain [epoch 12 batch 45]: : 1440it [00:04, 329.34it/s, mean train loss=0.152]\nvalidate [batch 12]: : 384it [00:01, 274.07it/s, mean loss=0.508, macro f1=0.754]\n\n\nEarly stopping at epoch 12\n\n\ntest [batch 29]: : 928it [00:03, 308.06it/s, mean loss=0.524, macro f1=0.742]\n[I 2024-10-20 00:14:19,623] Trial 23 finished with value: 0.7407869084118612 and parameters: {'batch_size': 32, 'lr': 0.00017571495299283302, 'weight_decay': 0.009726305764949416, 'reduce_lr_factor': 0.02894806164879947, 'momentum': 0.9737434356784762, 'early_stop_patience': 2, 'resize_length': 64, 'grayscale': False, 'jitter': False, 'horizontal_flip': False, 'rotation': False, 'mlp_n_layers': 5, 'mlp_layer_1_size': 256, 'mlp_layer_2_size': 512, 'mlp_layer_3_size': 256, 'mlp_activation_f': 'relu'}. Best is trial 21 with value: 0.7554649753224488.\n\n\nTrial 24 params: {'batch_size': 32, 'lr': 0.00011760046913718452, 'weight_decay': 0.0095020853707793, 'reduce_lr_factor': 0.027691349431280918, 'momentum': 0.9891129573812659, 'early_stop_patience': 2, 'resize_length': 32, 'grayscale': False, 'jitter': False, 'horizontal_flip': False, 'rotation': False, 'mlp_n_layers': 5, 'mlp_layer_1_size': 256, 'mlp_layer_2_size': 1024, 'mlp_layer_3_size': 256, 'mlp_activation_f': 'relu'}\nTrial 24 fold 1\n\n\ntrain [epoch 1 batch 45]: : 1440it [00:04, 321.38it/s, mean train loss=1.7]\nvalidate [batch 12]: : 384it [00:01, 272.59it/s, mean loss=1.25, macro f1=0.409]\ntrain [epoch 2 batch 45]: : 1440it [00:04, 329.51it/s, mean train loss=0.913]\nvalidate [batch 12]: : 384it [00:01, 274.09it/s, mean loss=0.843, macro f1=0.58]\ntrain [epoch 3 batch 45]: : 1440it [00:04, 332.47it/s, mean train loss=0.718]\nvalidate [batch 12]: : 384it [00:01, 275.59it/s, mean loss=0.711, macro f1=0.613]\ntrain [epoch 4 batch 45]: : 1440it [00:04, 329.29it/s, mean train loss=0.622]\nvalidate [batch 12]: : 384it [00:01, 280.73it/s, mean loss=0.686, macro f1=0.648]\ntrain [epoch 5 batch 45]: : 1440it [00:04, 335.76it/s, mean train loss=0.51]\nvalidate [batch 12]: : 384it [00:01, 270.25it/s, mean loss=0.594, macro f1=0.675]\ntrain [epoch 6 batch 45]: : 1440it [00:04, 314.30it/s, mean train loss=0.433]\nvalidate [batch 12]: : 384it [00:01, 222.25it/s, mean loss=0.56, macro f1=0.717]\ntrain [epoch 7 batch 45]: : 1440it [00:05, 262.02it/s, mean train loss=0.392]\nvalidate [batch 12]: : 384it [00:01, 263.97it/s, mean loss=0.542, macro f1=0.718]\ntrain [epoch 8 batch 45]: : 1440it [00:04, 308.88it/s, mean train loss=0.359]\nvalidate [batch 12]: : 384it [00:01, 269.02it/s, mean loss=0.533, macro f1=0.724]\n[I 2024-10-20 00:15:11,184] Trial 24 pruned. \n\n\n\n\nmlp_study.best_params\n\n\n{'batch_size': 32,\n 'lr': 0.0003514663712832598,\n 'weight_decay': 0.005635507431427942,\n 'reduce_lr_factor': 0.029753600418150557,\n 'momentum': 0.9872940644377142,\n 'early_stop_patience': 2,\n 'resize_length': 64,\n 'grayscale': False,\n 'jitter': False,\n 'horizontal_flip': False,\n 'rotation': False,\n 'mlp_n_layers': 5,\n 'mlp_layer_1_size': 256,\n 'mlp_layer_2_size': 512,\n 'mlp_layer_3_size': 256,\n 'mlp_activation_f': 'relu'}\n\n\n\nInterestingly, the hyperparameter tuning results indicate that the MLP performed best without performing any data augmentations. However, images had to be resized to 64 by 64. The size of each layer also seems rather small.\n\n\nVGG-16\nFor the custom VGG16 model, there is only one model-specific hyperparameter to tune since the model’s architecture is mostly fixed. The hyperparameter is whether to use batch normalisation. As mentioned earlier, I found that the model struggled to learn (loss would not decrease) without applying batch normalisation after each convolutional layer. The reason could be because the VGG-16 architecture is deep, so it is prone to the vanishing / exploding gradient problem. Batch normalisation may have helped to stabilise the gradients.\n\ndef suggest_vgg16_params(trial: optuna.trial.Trial):\n    trial.suggest_categorical(\"vgg16_batch_norm\", [True, False])\n\ndef vgg16_from_params(params: dict) -&gt; VGG16:\n    return VGG16(10, batch_norm=params[\"vgg16_batch_norm\"])\n\n\nvgg16_study = optuna.create_study(\n    study_name=\"VGG16 study\",\n    sampler=optuna.samplers.TPESampler(seed=0),\n    pruner=optuna.pruners.MedianPruner(n_startup_trials=3, n_warmup_steps=5),\n    directions=[\"maximize\"],\n    storage=optuna.storages.JournalStorage(\n        optuna.storages.journal.JournalFileBackend(\"vgg16_journal.log\"),\n    ),\n    load_if_exists=True,\n)\n\nvgg16_study.optimize(\n    make_objective(\n        suggest_vgg16_params,\n        vgg16_from_params,\n        ds_train,\n    ),\n    n_trials=N_TRIALS,\n    gc_after_trial=True\n)\n\n\n[I 2024-10-19 19:11:00,993] A new study created in Journal with name: VGG16 study\n\n\nTrial 0 params: {'batch_size': 32, 'lr': 0.0007180374727086954, 'weight_decay': 0.006027633760716439, 'reduce_lr_factor': 0.05903948646972072, 'momentum': 0.8804944118743919, 'early_stop_patience': 2, 'resize_length': 128, 'grayscale': False, 'jitter': False, 'horizontal_flip': False, 'rotation': True, 'vgg16_batch_norm': True}\nTrial 0 fold 1\n\n\ntrain [epoch 1 batch 45]: : 1440it [00:14, 100.87it/s, mean train loss=1.67]\nvalidate [batch 12]: : 384it [00:02, 170.65it/s, mean loss=1.89, macro f1=0.0796]\ntrain [epoch 2 batch 45]: : 1440it [00:12, 116.36it/s, mean train loss=1.14]\nvalidate [batch 12]: : 384it [00:01, 198.92it/s, mean loss=0.905, macro f1=0.4]\ntrain [epoch 3 batch 45]: : 1440it [00:12, 117.19it/s, mean train loss=0.754]\nvalidate [batch 12]: : 384it [00:01, 199.23it/s, mean loss=0.579, macro f1=0.696]\ntrain [epoch 4 batch 45]: : 1440it [00:12, 116.30it/s, mean train loss=0.511]\nvalidate [batch 12]: : 384it [00:01, 196.39it/s, mean loss=0.438, macro f1=0.775]\ntrain [epoch 5 batch 45]: : 1440it [00:12, 116.73it/s, mean train loss=0.432]\nvalidate [batch 12]: : 384it [00:01, 201.75it/s, mean loss=0.449, macro f1=0.763]\ntrain [epoch 6 batch 45]: : 1440it [00:12, 115.05it/s, mean train loss=0.389]\nvalidate [batch 12]: : 384it [00:01, 199.72it/s, mean loss=0.426, macro f1=0.803]\ntrain [epoch 7 batch 45]: : 1440it [00:12, 115.94it/s, mean train loss=0.337]\nvalidate [batch 12]: : 384it [00:01, 201.13it/s, mean loss=0.485, macro f1=0.781]\ntrain [epoch 8 batch 45]: : 1440it [00:12, 114.43it/s, mean train loss=0.29]\nvalidate [batch 12]: : 384it [00:01, 199.99it/s, mean loss=0.592, macro f1=0.748]\n\n\nEarly stopping at epoch 8\n\n\ntest [batch 29]: : 928it [00:04, 213.72it/s, mean loss=0.498, macro f1=0.78]\n\n\nTrial 0 fold 2\n\n\ntrain [epoch 1 batch 45]: : 1440it [00:12, 114.99it/s, mean train loss=1.65]\nvalidate [batch 12]: : 384it [00:01, 193.17it/s, mean loss=1.63, macro f1=0.124]\ntrain [epoch 2 batch 45]: : 1440it [00:12, 112.77it/s, mean train loss=1.17]\nvalidate [batch 12]: : 384it [00:01, 197.98it/s, mean loss=0.934, macro f1=0.442]\ntrain [epoch 3 batch 45]: : 1440it [00:12, 113.72it/s, mean train loss=0.766]\nvalidate [batch 12]: : 384it [00:01, 192.40it/s, mean loss=0.517, macro f1=0.685]\ntrain [epoch 4 batch 45]: : 1440it [00:12, 112.18it/s, mean train loss=0.534]\nvalidate [batch 12]: : 384it [00:01, 199.63it/s, mean loss=0.499, macro f1=0.73]\ntrain [epoch 5 batch 45]: : 1440it [00:12, 112.55it/s, mean train loss=0.485]\nvalidate [batch 12]: : 384it [00:01, 198.74it/s, mean loss=0.591, macro f1=0.784]\ntrain [epoch 6 batch 45]: : 1440it [00:13, 109.01it/s, mean train loss=0.386]\nvalidate [batch 12]: : 384it [00:01, 198.99it/s, mean loss=0.43, macro f1=0.811]\ntrain [epoch 7 batch 45]: : 1440it [00:12, 112.68it/s, mean train loss=0.31]\nvalidate [batch 12]: : 384it [00:01, 194.11it/s, mean loss=0.68, macro f1=0.785]\ntrain [epoch 8 batch 45]: : 1440it [00:12, 111.20it/s, mean train loss=0.319]\nvalidate [batch 12]: : 384it [00:01, 200.87it/s, mean loss=0.386, macro f1=0.833]\ntrain [epoch 9 batch 45]: : 1440it [00:12, 113.16it/s, mean train loss=0.257]\nvalidate [batch 12]: : 384it [00:01, 195.65it/s, mean loss=0.471, macro f1=0.857]\ntrain [epoch 10 batch 45]: : 1440it [00:12, 111.59it/s, mean train loss=0.261]\nvalidate [batch 12]: : 384it [00:02, 189.58it/s, mean loss=0.459, macro f1=0.831]\n\n\nEarly stopping at epoch 10\n\n\ntest [batch 29]: : 928it [00:04, 211.92it/s, mean loss=0.438, macro f1=0.815]\n\n\nTrial 0 fold 3\n\n\ntrain [epoch 1 batch 45]: : 1440it [00:12, 112.76it/s, mean train loss=1.62]\nvalidate [batch 12]: : 384it [00:01, 195.99it/s, mean loss=1.72, macro f1=0.0926]\ntrain [epoch 2 batch 45]: : 1440it [00:12, 111.21it/s, mean train loss=1.13]\nvalidate [batch 12]: : 384it [00:01, 192.86it/s, mean loss=0.829, macro f1=0.463]\ntrain [epoch 3 batch 45]: : 1440it [00:12, 112.39it/s, mean train loss=0.72]\nvalidate [batch 12]: : 384it [00:01, 194.04it/s, mean loss=0.556, macro f1=0.627]\ntrain [epoch 4 batch 45]: : 1440it [00:12, 111.76it/s, mean train loss=0.533]\nvalidate [batch 12]: : 384it [00:02, 191.32it/s, mean loss=0.522, macro f1=0.709]\ntrain [epoch 5 batch 45]: : 1440it [00:12, 112.57it/s, mean train loss=0.476]\nvalidate [batch 12]: : 384it [00:01, 193.47it/s, mean loss=0.452, macro f1=0.743]\ntrain [epoch 6 batch 45]: : 1440it [00:12, 111.67it/s, mean train loss=0.362]\nvalidate [batch 12]: : 384it [00:01, 193.37it/s, mean loss=0.44, macro f1=0.809]\ntrain [epoch 7 batch 45]: : 1440it [00:12, 112.56it/s, mean train loss=0.338]\nvalidate [batch 12]: : 384it [00:01, 194.23it/s, mean loss=0.347, macro f1=0.819]\ntrain [epoch 8 batch 45]: : 1440it [00:12, 111.76it/s, mean train loss=0.314]\nvalidate [batch 12]: : 384it [00:01, 193.10it/s, mean loss=0.536, macro f1=0.794]\ntrain [epoch 9 batch 45]: : 1440it [00:12, 112.79it/s, mean train loss=0.263]\nvalidate [batch 12]: : 384it [00:01, 197.93it/s, mean loss=0.965, macro f1=0.628]\n\n\nEarly stopping at epoch 9\n\n\ntest [batch 29]: : 928it [00:04, 211.59it/s, mean loss=1.01, macro f1=0.662]\n[I 2024-10-19 19:18:03,075] Trial 0 finished with value: 0.7523225350223695 and parameters: {'batch_size': 32, 'lr': 0.0007180374727086954, 'weight_decay': 0.006027633760716439, 'reduce_lr_factor': 0.05903948646972072, 'momentum': 0.8804944118743919, 'early_stop_patience': 2, 'resize_length': 128, 'grayscale': False, 'jitter': False, 'horizontal_flip': False, 'rotation': True, 'vgg16_batch_norm': True}. Best is trial 0 with value: 0.7523225350223695.\n\n\nTrial 1 params: {'batch_size': 32, 'lr': 0.000780375183440352, 'weight_decay': 0.008700121482468192, 'reduce_lr_factor': 0.09807565080094877, 'momentum': 0.9518401272011775, 'early_stop_patience': 2, 'resize_length': 192, 'grayscale': False, 'jitter': False, 'horizontal_flip': True, 'rotation': False, 'vgg16_batch_norm': False}\nTrial 1 fold 1\n\n\ntrain [epoch 1 batch 45]: : 1440it [00:19, 73.40it/s, mean train loss=2.28]\nvalidate [batch 12]: : 384it [00:02, 143.77it/s, mean loss=2.25, macro f1=0.0581]\ntrain [epoch 2 batch 45]: : 1440it [00:19, 73.40it/s, mean train loss=2.22]\nvalidate [batch 12]: : 384it [00:02, 144.38it/s, mean loss=2.18, macro f1=0.0581]\ntrain [epoch 3 batch 45]: : 1440it [00:19, 73.52it/s, mean train loss=2.16]\nvalidate [batch 12]: : 384it [00:02, 145.34it/s, mean loss=2.12, macro f1=0.0581]\ntrain [epoch 4 batch 45]: : 1440it [00:19, 75.23it/s, mean train loss=2.1]\nvalidate [batch 12]: : 384it [00:02, 142.81it/s, mean loss=2.06, macro f1=0.0581]\ntrain [epoch 5 batch 45]: : 1440it [00:19, 75.19it/s, mean train loss=2.06]\nvalidate [batch 12]: : 384it [00:02, 144.68it/s, mean loss=2.02, macro f1=0.0581]\n[I 2024-10-19 19:19:55,855] Trial 1 pruned. Mean validation loss is still above or equal to 1.5 at the 5th epoch.\n\n\nTrial 2 params: {'batch_size': 24, 'lr': 0.0006214591421051183, 'weight_decay': 0.006120957227224214, 'reduce_lr_factor': 0.06552405971872813, 'momentum': 0.9793121349177786, 'early_stop_patience': 3, 'resize_length': 96, 'grayscale': False, 'jitter': False, 'horizontal_flip': True, 'rotation': False, 'vgg16_batch_norm': False}\nTrial 2 fold 1\n\n\ntrain [epoch 1 batch 60]: : 1440it [00:09, 144.38it/s, mean train loss=2.27]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 199.36it/s, mean loss=2.21, macro f1=0.0581]\ntrain [epoch 2 batch 60]: : 1440it [00:10, 142.60it/s, mean train loss=2.16]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 196.57it/s, mean loss=2.08, macro f1=0.0581]\ntrain [epoch 3 batch 60]: : 1440it [00:09, 144.39it/s, mean train loss=2.03]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 207.55it/s, mean loss=1.98, macro f1=0.0581]\ntrain [epoch 4 batch 60]: : 1440it [00:10, 143.66it/s, mean train loss=1.94]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 201.52it/s, mean loss=1.93, macro f1=0.0581]\ntrain [epoch 5 batch 60]: : 1440it [00:10, 141.98it/s, mean train loss=1.93]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 210.07it/s, mean loss=1.92, macro f1=0.0581]\n[I 2024-10-19 19:20:57,225] Trial 2 pruned. Mean validation loss is still above or equal to 1.5 at the 5th epoch.\n\n\nTrial 3 params: {'batch_size': 24, 'lr': 0.000988490099678634, 'weight_decay': 0.0010204481074802807, 'reduce_lr_factor': 0.028798908048535125, 'momentum': 0.8306488083981494, 'early_stop_patience': 2, 'resize_length': 64, 'grayscale': True, 'jitter': True, 'horizontal_flip': True, 'rotation': False, 'vgg16_batch_norm': True}\nTrial 3 fold 1\n\n\ntrain [epoch 1 batch 60]: : 1440it [00:09, 146.73it/s, mean train loss=1.49]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 203.88it/s, mean loss=1.06, macro f1=0.348]\ntrain [epoch 2 batch 60]: : 1440it [00:10, 143.80it/s, mean train loss=0.836]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 201.12it/s, mean loss=0.727, macro f1=0.466]\ntrain [epoch 3 batch 60]: : 1440it [00:09, 147.44it/s, mean train loss=0.599]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 237.76it/s, mean loss=0.604, macro f1=0.615]\ntrain [epoch 4 batch 60]: : 1440it [00:08, 170.02it/s, mean train loss=0.505]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 234.33it/s, mean loss=0.581, macro f1=0.644]\ntrain [epoch 5 batch 60]: : 1440it [00:08, 167.54it/s, mean train loss=0.47]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 213.05it/s, mean loss=0.534, macro f1=0.738]\ntrain [epoch 6 batch 60]: : 1440it [00:09, 148.33it/s, mean train loss=0.359]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 218.60it/s, mean loss=0.396, macro f1=0.807]\ntrain [epoch 7 batch 60]: : 1440it [00:09, 151.78it/s, mean train loss=0.306]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 215.62it/s, mean loss=0.63, macro f1=0.681]\ntrain [epoch 8 batch 60]: : 1440it [00:09, 155.29it/s, mean train loss=0.301]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 226.55it/s, mean loss=0.5, macro f1=0.807]\n\n\nEarly stopping at epoch 8\n\n\ntest [batch 38]: : 912it [00:03, 241.26it/s, mean loss=0.492, macro f1=0.788]\n\n\nTrial 3 fold 2\n\n\ntrain [epoch 1 batch 60]: : 1440it [00:09, 159.14it/s, mean train loss=1.48]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 203.72it/s, mean loss=1.04, macro f1=0.283]\ntrain [epoch 2 batch 60]: : 1440it [00:08, 160.07it/s, mean train loss=0.965]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 231.34it/s, mean loss=0.75, macro f1=0.524]\ntrain [epoch 3 batch 60]: : 1440it [00:09, 158.21it/s, mean train loss=0.664]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 206.40it/s, mean loss=0.647, macro f1=0.608]\ntrain [epoch 4 batch 60]: : 1440it [00:09, 156.54it/s, mean train loss=0.55]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 229.36it/s, mean loss=0.463, macro f1=0.661]\ntrain [epoch 5 batch 60]: : 1440it [00:09, 159.86it/s, mean train loss=0.45]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 225.62it/s, mean loss=0.966, macro f1=0.569]\ntrain [epoch 6 batch 60]: : 1440it [00:09, 154.53it/s, mean train loss=0.354]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 221.09it/s, mean loss=0.361, macro f1=0.818]\ntrain [epoch 7 batch 60]: : 1440it [00:09, 158.33it/s, mean train loss=0.279]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 226.56it/s, mean loss=0.435, macro f1=0.787]\ntrain [epoch 8 batch 60]: : 1440it [00:08, 160.70it/s, mean train loss=0.259]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 205.83it/s, mean loss=0.343, macro f1=0.786]\ntrain [epoch 9 batch 60]: : 1440it [00:09, 150.06it/s, mean train loss=0.206]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 225.38it/s, mean loss=0.771, macro f1=0.687]\ntrain [epoch 10 batch 60]: : 1440it [00:09, 155.78it/s, mean train loss=0.21]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 219.03it/s, mean loss=0.31, macro f1=0.845]\ntrain [epoch 11 batch 60]: : 1440it [00:09, 144.30it/s, mean train loss=0.154]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 200.78it/s, mean loss=0.641, macro f1=0.769]\ntrain [epoch 12 batch 60]: : 1440it [00:09, 154.75it/s, mean train loss=0.179]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 228.11it/s, mean loss=0.505, macro f1=0.81]\n\n\nEarly stopping at epoch 12\n\n\ntest [batch 38]: : 912it [00:04, 223.73it/s, mean loss=0.562, macro f1=0.791]\n\n\nTrial 3 fold 3\n\n\ntrain [epoch 1 batch 60]: : 1440it [00:09, 146.65it/s, mean train loss=1.45]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 184.34it/s, mean loss=1.11, macro f1=0.267]\ntrain [epoch 2 batch 60]: : 1440it [00:09, 147.96it/s, mean train loss=0.903]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 236.92it/s, mean loss=0.671, macro f1=0.554]\ntrain [epoch 3 batch 60]: : 1440it [00:08, 162.29it/s, mean train loss=0.65]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 254.00it/s, mean loss=0.592, macro f1=0.582]\ntrain [epoch 4 batch 60]: : 1440it [00:08, 160.81it/s, mean train loss=0.523]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 211.14it/s, mean loss=0.563, macro f1=0.672]\ntrain [epoch 5 batch 60]: : 1440it [00:09, 156.72it/s, mean train loss=0.412]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 221.52it/s, mean loss=0.74, macro f1=0.645]\ntrain [epoch 6 batch 60]: : 1440it [00:09, 147.93it/s, mean train loss=0.384]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 237.38it/s, mean loss=0.469, macro f1=0.74]\ntrain [epoch 7 batch 60]: : 1440it [00:09, 144.91it/s, mean train loss=0.351]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 200.00it/s, mean loss=0.625, macro f1=0.666]\ntrain [epoch 8 batch 60]: : 1440it [00:09, 145.31it/s, mean train loss=0.302]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 202.47it/s, mean loss=0.414, macro f1=0.767]\ntrain [epoch 9 batch 60]: : 1440it [00:09, 146.82it/s, mean train loss=0.241]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 241.98it/s, mean loss=0.523, macro f1=0.807]\ntrain [epoch 10 batch 60]: : 1440it [00:08, 165.72it/s, mean train loss=0.195]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 236.47it/s, mean loss=0.337, macro f1=0.849]\ntrain [epoch 11 batch 60]: : 1440it [00:09, 154.30it/s, mean train loss=0.146]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 243.67it/s, mean loss=0.398, macro f1=0.841]\ntrain [epoch 12 batch 60]: : 1440it [00:08, 162.94it/s, mean train loss=0.196]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 236.83it/s, mean loss=0.497, macro f1=0.806]\n\n\nEarly stopping at epoch 12\n\n\ntest [batch 38]: : 912it [00:04, 213.17it/s, mean loss=0.482, macro f1=0.816]\n[I 2024-10-19 19:27:12,999] Trial 3 finished with value: 0.7984415820297991 and parameters: {'batch_size': 24, 'lr': 0.000988490099678634, 'weight_decay': 0.0010204481074802807, 'reduce_lr_factor': 0.028798908048535125, 'momentum': 0.8306488083981494, 'early_stop_patience': 2, 'resize_length': 64, 'grayscale': True, 'jitter': True, 'horizontal_flip': True, 'rotation': False, 'vgg16_batch_norm': True}. Best is trial 3 with value: 0.7984415820297991.\n\n\nTrial 4 params: {'batch_size': 32, 'lr': 0.00010513742381502343, 'weight_decay': 0.009764594650133959, 'reduce_lr_factor': 0.05217860814829315, 'momentum': 0.985584606756164, 'early_stop_patience': 2, 'resize_length': 192, 'grayscale': False, 'jitter': False, 'horizontal_flip': False, 'rotation': True, 'vgg16_batch_norm': True}\nTrial 4 fold 1\n\n\ntrain [epoch 1 batch 45]: : 1440it [00:21, 66.17it/s, mean train loss=1.99]\nvalidate [batch 12]: : 384it [00:02, 142.38it/s, mean loss=1.89, macro f1=0.0624]\ntrain [epoch 2 batch 45]: : 1440it [00:21, 65.77it/s, mean train loss=1.5]\nvalidate [batch 12]: : 384it [00:02, 142.00it/s, mean loss=1.26, macro f1=0.227]\ntrain [epoch 3 batch 45]: : 1440it [00:21, 66.62it/s, mean train loss=1.21]\nvalidate [batch 12]: : 384it [00:02, 142.28it/s, mean loss=0.99, macro f1=0.392]\ntrain [epoch 4 batch 45]: : 1440it [00:21, 66.09it/s, mean train loss=0.926]\nvalidate [batch 12]: : 384it [00:02, 142.99it/s, mean loss=0.735, macro f1=0.469]\ntrain [epoch 5 batch 45]: : 1440it [00:21, 66.28it/s, mean train loss=0.703]\nvalidate [batch 12]: : 384it [00:02, 140.95it/s, mean loss=0.597, macro f1=0.697]\ntrain [epoch 6 batch 45]: : 1440it [00:21, 66.35it/s, mean train loss=0.525]\nvalidate [batch 12]: : 384it [00:02, 143.14it/s, mean loss=0.517, macro f1=0.744]\ntrain [epoch 7 batch 45]: : 1440it [00:21, 66.30it/s, mean train loss=0.503]\nvalidate [batch 12]: : 384it [00:02, 144.00it/s, mean loss=0.477, macro f1=0.72]\ntrain [epoch 8 batch 45]: : 1440it [00:21, 66.19it/s, mean train loss=0.445]\nvalidate [batch 12]: : 384it [00:02, 142.65it/s, mean loss=0.491, macro f1=0.71]\ntrain [epoch 9 batch 45]: : 1440it [00:21, 66.44it/s, mean train loss=0.369]\nvalidate [batch 12]: : 384it [00:02, 143.21it/s, mean loss=0.447, macro f1=0.749]\ntrain [epoch 10 batch 45]: : 1440it [00:21, 66.60it/s, mean train loss=0.365]\nvalidate [batch 12]: : 384it [00:02, 143.87it/s, mean loss=0.563, macro f1=0.721]\ntrain [epoch 11 batch 45]: : 1440it [00:21, 66.41it/s, mean train loss=0.356]\nvalidate [batch 12]: : 384it [00:02, 142.29it/s, mean loss=0.477, macro f1=0.734]\n\n\nEarly stopping at epoch 11\n\n\ntest [batch 29]: : 928it [00:06, 144.26it/s, mean loss=0.475, macro f1=0.737]\n\n\nTrial 4 fold 2\n\n\ntrain [epoch 1 batch 45]: : 1440it [00:22, 65.30it/s, mean train loss=1.98]\nvalidate [batch 12]: : 384it [00:02, 139.44it/s, mean loss=1.87, macro f1=0.0509]\ntrain [epoch 2 batch 45]: : 1440it [00:22, 65.18it/s, mean train loss=1.55]\nvalidate [batch 12]: : 384it [00:02, 128.87it/s, mean loss=1.26, macro f1=0.182]\ntrain [epoch 3 batch 45]: : 1440it [00:22, 65.41it/s, mean train loss=1.19]\nvalidate [batch 12]: : 384it [00:02, 138.24it/s, mean loss=1.08, macro f1=0.277]\ntrain [epoch 4 batch 45]: : 1440it [00:22, 64.80it/s, mean train loss=1.03]\nvalidate [batch 12]: : 384it [00:02, 136.42it/s, mean loss=0.805, macro f1=0.575]\ntrain [epoch 5 batch 45]: : 1440it [00:21, 65.92it/s, mean train loss=0.768]\nvalidate [batch 12]: : 384it [00:02, 143.70it/s, mean loss=0.667, macro f1=0.611]\ntrain [epoch 6 batch 45]: : 1440it [00:21, 66.43it/s, mean train loss=0.591]\nvalidate [batch 12]: : 384it [00:02, 139.04it/s, mean loss=0.573, macro f1=0.653]\ntrain [epoch 7 batch 45]: : 1440it [00:21, 65.66it/s, mean train loss=0.536]\nvalidate [batch 12]: : 384it [00:02, 133.96it/s, mean loss=0.5, macro f1=0.753]\ntrain [epoch 8 batch 45]: : 1440it [00:22, 65.03it/s, mean train loss=0.441]\nvalidate [batch 12]: : 384it [00:02, 132.92it/s, mean loss=0.439, macro f1=0.733]\ntrain [epoch 9 batch 45]: : 1440it [00:22, 64.81it/s, mean train loss=0.391]\nvalidate [batch 12]: : 384it [00:02, 134.53it/s, mean loss=0.509, macro f1=0.776]\ntrain [epoch 10 batch 45]: : 1440it [00:21, 65.91it/s, mean train loss=0.365]\nvalidate [batch 12]: : 384it [00:02, 142.00it/s, mean loss=0.443, macro f1=0.771]\n\n\nEarly stopping at epoch 10\n\n\ntest [batch 29]: : 928it [00:06, 142.47it/s, mean loss=0.44, macro f1=0.739]\n\n\nTrial 4 fold 3\n\n\ntrain [epoch 1 batch 45]: : 1440it [00:21, 65.69it/s, mean train loss=1.96]\nvalidate [batch 12]: : 384it [00:02, 134.33it/s, mean loss=1.87, macro f1=0.151]\ntrain [epoch 2 batch 45]: : 1440it [00:22, 65.19it/s, mean train loss=1.51]\nvalidate [batch 12]: : 384it [00:02, 135.61it/s, mean loss=1.27, macro f1=0.186]\ntrain [epoch 3 batch 45]: : 1440it [00:21, 65.57it/s, mean train loss=1.12]\nvalidate [batch 12]: : 384it [00:02, 135.01it/s, mean loss=1.05, macro f1=0.29]\ntrain [epoch 4 batch 45]: : 1440it [00:22, 65.37it/s, mean train loss=0.955]\nvalidate [batch 12]: : 384it [00:02, 141.50it/s, mean loss=0.81, macro f1=0.491]\ntrain [epoch 5 batch 45]: : 1440it [00:21, 66.59it/s, mean train loss=0.72]\nvalidate [batch 12]: : 384it [00:02, 140.94it/s, mean loss=0.631, macro f1=0.602]\ntrain [epoch 6 batch 45]: : 1440it [00:21, 65.70it/s, mean train loss=0.618]\nvalidate [batch 12]: : 384it [00:02, 140.75it/s, mean loss=0.528, macro f1=0.67]\ntrain [epoch 7 batch 45]: : 1440it [00:22, 65.08it/s, mean train loss=0.512]\nvalidate [batch 12]: : 384it [00:02, 137.98it/s, mean loss=0.526, macro f1=0.626]\ntrain [epoch 8 batch 45]: : 1440it [00:21, 66.01it/s, mean train loss=0.449]\nvalidate [batch 12]: : 384it [00:02, 141.96it/s, mean loss=0.488, macro f1=0.65]\ntrain [epoch 9 batch 45]: : 1440it [00:21, 65.88it/s, mean train loss=0.424]\nvalidate [batch 12]: : 384it [00:02, 133.52it/s, mean loss=0.437, macro f1=0.769]\ntrain [epoch 10 batch 45]: : 1440it [00:22, 65.30it/s, mean train loss=0.402]\nvalidate [batch 12]: : 384it [00:02, 135.72it/s, mean loss=0.456, macro f1=0.726]\ntrain [epoch 11 batch 45]: : 1440it [00:22, 65.41it/s, mean train loss=0.347]\nvalidate [batch 12]: : 384it [00:02, 141.40it/s, mean loss=0.505, macro f1=0.681]\n\n\nEarly stopping at epoch 11\n\n\ntest [batch 29]: : 928it [00:06, 145.51it/s, mean loss=0.513, macro f1=0.722]\n[I 2024-10-19 19:40:53,521] Trial 4 finished with value: 0.7328167133703358 and parameters: {'batch_size': 32, 'lr': 0.00010513742381502343, 'weight_decay': 0.009764594650133959, 'reduce_lr_factor': 0.05217860814829315, 'momentum': 0.985584606756164, 'early_stop_patience': 2, 'resize_length': 192, 'grayscale': False, 'jitter': False, 'horizontal_flip': False, 'rotation': True, 'vgg16_batch_norm': True}. Best is trial 3 with value: 0.7984415820297991.\n\n\nTrial 5 params: {'batch_size': 24, 'lr': 0.0005280155729320327, 'weight_decay': 0.0009394051075844168, 'reduce_lr_factor': 0.061835184600056145, 'momentum': 0.9765662775394807, 'early_stop_patience': 1, 'resize_length': 160, 'grayscale': False, 'jitter': True, 'horizontal_flip': True, 'rotation': True, 'vgg16_batch_norm': True}\nTrial 5 fold 1\n\n\ntrain [epoch 1 batch 60]: : 1440it [00:19, 72.31it/s, mean train loss=1.73]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:02&lt;00:00, 146.71it/s, mean loss=1.13, macro f1=0.342]\ntrain [epoch 2 batch 60]: : 1440it [00:19, 72.67it/s, mean train loss=1.05]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:02&lt;00:00, 150.47it/s, mean loss=1.59, macro f1=0.477]\n\n\nEarly stopping at epoch 2\n\n\ntest [batch 38]: : 912it [00:05, 154.55it/s, mean loss=1.55, macro f1=0.475]\n\n\nTrial 5 fold 2\n\n\ntrain [epoch 1 batch 60]: : 1440it [00:19, 72.23it/s, mean train loss=1.69]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:02&lt;00:00, 147.72it/s, mean loss=1.42, macro f1=0.23]\ntrain [epoch 2 batch 60]: : 1440it [00:20, 71.90it/s, mean train loss=1.08]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:02&lt;00:00, 139.10it/s, mean loss=0.786, macro f1=0.572]\ntrain [epoch 3 batch 60]: : 1440it [00:19, 72.12it/s, mean train loss=0.64]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:02&lt;00:00, 149.50it/s, mean loss=0.565, macro f1=0.692]\ntrain [epoch 4 batch 60]: : 1440it [00:19, 72.48it/s, mean train loss=0.531]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:02&lt;00:00, 151.28it/s, mean loss=0.597, macro f1=0.669]\n\n\nEarly stopping at epoch 4\n\n\ntest [batch 38]: : 912it [00:05, 152.39it/s, mean loss=0.564, macro f1=0.715]\n\n\nTrial 5 fold 3\n\n\ntrain [epoch 1 batch 60]: : 1440it [00:19, 73.51it/s, mean train loss=1.68]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:02&lt;00:00, 149.51it/s, mean loss=1.19, macro f1=0.268]\ntrain [epoch 2 batch 60]: : 1440it [00:19, 73.40it/s, mean train loss=0.96]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:02&lt;00:00, 149.25it/s, mean loss=0.729, macro f1=0.593]\ntrain [epoch 3 batch 60]: : 1440it [00:19, 73.45it/s, mean train loss=0.669]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:02&lt;00:00, 150.29it/s, mean loss=0.672, macro f1=0.629]\ntrain [epoch 4 batch 60]: : 1440it [00:19, 73.64it/s, mean train loss=0.527]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:02&lt;00:00, 142.75it/s, mean loss=0.527, macro f1=0.702]\ntrain [epoch 5 batch 60]: : 1440it [00:19, 73.12it/s, mean train loss=0.527]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:02&lt;00:00, 149.07it/s, mean loss=0.495, macro f1=0.671]\ntrain [epoch 6 batch 60]: : 1440it [00:19, 73.57it/s, mean train loss=0.542]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:02&lt;00:00, 151.03it/s, mean loss=0.627, macro f1=0.708]\n[I 2024-10-19 19:45:37,597] Trial 5 pruned. \n\n\nTrial 6 params: {'batch_size': 32, 'lr': 0.0009625666596662639, 'weight_decay': 0.0024875314351995802, 'reduce_lr_factor': 0.06185416009760533, 'momentum': 0.9124879669416495, 'early_stop_patience': 2, 'resize_length': 64, 'grayscale': True, 'jitter': True, 'horizontal_flip': False, 'rotation': False, 'vgg16_batch_norm': False}\nTrial 6 fold 1\n\n\ntrain [epoch 1 batch 45]: : 1440it [00:08, 160.41it/s, mean train loss=2.29]\nvalidate [batch 12]: : 384it [00:01, 223.18it/s, mean loss=2.26, macro f1=0.0581]\ntrain [epoch 2 batch 45]: : 1440it [00:08, 178.33it/s, mean train loss=2.23]\nvalidate [batch 12]: : 384it [00:01, 245.00it/s, mean loss=2.2, macro f1=0.0581]\ntrain [epoch 3 batch 45]: : 1440it [00:08, 173.17it/s, mean train loss=2.19]\nvalidate [batch 12]: : 384it [00:01, 237.41it/s, mean loss=2.16, macro f1=0.0581]\ntrain [epoch 4 batch 45]: : 1440it [00:08, 173.86it/s, mean train loss=2.14]\nvalidate [batch 12]: : 384it [00:01, 234.44it/s, mean loss=2.11, macro f1=0.0581]\ntrain [epoch 5 batch 45]: : 1440it [00:08, 167.28it/s, mean train loss=2.12]\nvalidate [batch 12]: : 384it [00:01, 235.14it/s, mean loss=2.08, macro f1=0.0581]\n[I 2024-10-19 19:46:30,353] Trial 6 pruned. Mean validation loss is still above or equal to 1.5 at the 5th epoch.\n\n\nTrial 7 params: {'batch_size': 32, 'lr': 0.0007280017370214441, 'weight_decay': 0.005013243819267023, 'reduce_lr_factor': 0.09604752712509015, 'momentum': 0.9223581378536311, 'early_stop_patience': 2, 'resize_length': 160, 'grayscale': False, 'jitter': True, 'horizontal_flip': True, 'rotation': False, 'vgg16_batch_norm': False}\nTrial 7 fold 1\n\n\ntrain [epoch 1 batch 45]: : 1440it [00:16, 87.35it/s, mean train loss=2.28]\nvalidate [batch 12]: : 384it [00:02, 166.54it/s, mean loss=2.26, macro f1=0.0581]\ntrain [epoch 2 batch 45]: : 1440it [00:16, 88.73it/s, mean train loss=2.24]\nvalidate [batch 12]: : 384it [00:02, 163.50it/s, mean loss=2.22, macro f1=0.0581]\ntrain [epoch 3 batch 45]: : 1440it [00:16, 89.29it/s, mean train loss=2.19]\nvalidate [batch 12]: : 384it [00:02, 163.31it/s, mean loss=2.17, macro f1=0.0581]\ntrain [epoch 4 batch 45]: : 1440it [00:16, 87.08it/s, mean train loss=2.16]\nvalidate [batch 12]: : 384it [00:02, 166.39it/s, mean loss=2.14, macro f1=0.0581]\ntrain [epoch 5 batch 45]: : 1440it [00:16, 87.84it/s, mean train loss=2.14]\nvalidate [batch 12]: : 384it [00:02, 160.36it/s, mean loss=2.1, macro f1=0.0581]\n[I 2024-10-19 19:48:06,141] Trial 7 pruned. Mean validation loss is still above or equal to 1.5 at the 5th epoch.\n\n\nTrial 8 params: {'batch_size': 32, 'lr': 0.0006566688116585623, 'weight_decay': 0.006521032700016889, 'reduce_lr_factor': 0.04882765918905766, 'momentum': 0.9703438532117019, 'early_stop_patience': 2, 'resize_length': 128, 'grayscale': True, 'jitter': True, 'horizontal_flip': True, 'rotation': True, 'vgg16_batch_norm': True}\nTrial 8 fold 1\n\n\ntrain [epoch 1 batch 45]: : 1440it [00:14, 101.02it/s, mean train loss=1.69]\nvalidate [batch 12]: : 384it [00:01, 196.74it/s, mean loss=1.48, macro f1=0.188]\ntrain [epoch 2 batch 45]: : 1440it [00:14, 102.26it/s, mean train loss=1.18]\nvalidate [batch 12]: : 384it [00:02, 188.35it/s, mean loss=1.04, macro f1=0.334]\ntrain [epoch 3 batch 45]: : 1440it [00:14, 99.35it/s, mean train loss=0.88] \nvalidate [batch 12]: : 384it [00:02, 183.23it/s, mean loss=0.798, macro f1=0.463]\ntrain [epoch 4 batch 45]: : 1440it [00:14, 100.90it/s, mean train loss=0.778]\nvalidate [batch 12]: : 384it [00:02, 181.89it/s, mean loss=0.91, macro f1=0.401]\ntrain [epoch 5 batch 45]: : 1440it [00:14, 102.74it/s, mean train loss=0.723]\nvalidate [batch 12]: : 384it [00:02, 189.26it/s, mean loss=0.826, macro f1=0.565]\n\n\nEarly stopping at epoch 5\n\n\ntest [batch 29]: : 928it [00:04, 203.14it/s, mean loss=0.892, macro f1=0.546]\n\n\nTrial 8 fold 2\n\n\ntrain [epoch 1 batch 45]: : 1440it [00:14, 99.63it/s, mean train loss=1.7]\nvalidate [batch 12]: : 384it [00:02, 187.85it/s, mean loss=1.31, macro f1=0.22]\ntrain [epoch 2 batch 45]: : 1440it [00:14, 102.25it/s, mean train loss=1.26]\nvalidate [batch 12]: : 384it [00:02, 191.05it/s, mean loss=0.945, macro f1=0.399]\ntrain [epoch 3 batch 45]: : 1440it [00:13, 105.75it/s, mean train loss=0.911]\nvalidate [batch 12]: : 384it [00:01, 196.32it/s, mean loss=0.812, macro f1=0.434]\n[I 2024-10-19 19:50:24,152] Trial 8 pruned. \n\n\nTrial 9 params: {'batch_size': 32, 'lr': 0.0001325817830209471, 'weight_decay': 0.008480082293222344, 'reduce_lr_factor': 0.08265870628525096, 'momentum': 0.9081291403367727, 'early_stop_patience': 2, 'resize_length': 32, 'grayscale': True, 'jitter': False, 'horizontal_flip': True, 'rotation': False, 'vgg16_batch_norm': True}\nTrial 9 fold 1\n\n\ntrain [epoch 1 batch 45]: : 1440it [00:06, 208.49it/s, mean train loss=1.9]\nvalidate [batch 12]: : 384it [00:01, 247.77it/s, mean loss=1.77, macro f1=0.0744]\ntrain [epoch 2 batch 45]: : 1440it [00:06, 217.25it/s, mean train loss=1.39]\nvalidate [batch 12]: : 384it [00:01, 248.65it/s, mean loss=1.26, macro f1=0.191]\ntrain [epoch 3 batch 45]: : 1440it [00:06, 216.26it/s, mean train loss=1.15]\nvalidate [batch 12]: : 384it [00:01, 240.61it/s, mean loss=1.04, macro f1=0.487]\ntrain [epoch 4 batch 45]: : 1440it [00:07, 182.90it/s, mean train loss=0.944]\nvalidate [batch 12]: : 384it [00:01, 227.63it/s, mean loss=0.886, macro f1=0.434]\ntrain [epoch 5 batch 45]: : 1440it [00:07, 205.13it/s, mean train loss=0.803]\nvalidate [batch 12]: : 384it [00:01, 247.98it/s, mean loss=0.698, macro f1=0.562]\ntrain [epoch 6 batch 45]: : 1440it [00:06, 217.90it/s, mean train loss=0.625]\nvalidate [batch 12]: : 384it [00:01, 241.69it/s, mean loss=0.628, macro f1=0.539]\n[I 2024-10-19 19:51:18,034] Trial 9 pruned. \n\n\nTrial 10 params: {'batch_size': 24, 'lr': 0.0003311487468197181, 'weight_decay': 0.00020768834766933357, 'reduce_lr_factor': 0.015244949070858688, 'momentum': 0.8213427298202189, 'early_stop_patience': 3, 'resize_length': 32, 'grayscale': True, 'jitter': True, 'horizontal_flip': False, 'rotation': False, 'vgg16_batch_norm': True}\nTrial 10 fold 1\n\n\ntrain [epoch 1 batch 60]: : 1440it [00:07, 182.11it/s, mean train loss=1.68]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 204.31it/s, mean loss=1.3, macro f1=0.192]\ntrain [epoch 2 batch 60]: : 1440it [00:08, 177.95it/s, mean train loss=1.15]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 241.21it/s, mean loss=0.968, macro f1=0.388]\ntrain [epoch 3 batch 60]: : 1440it [00:07, 184.46it/s, mean train loss=0.835]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 250.22it/s, mean loss=0.699, macro f1=0.558]\ntrain [epoch 4 batch 60]: : 1440it [00:07, 184.33it/s, mean train loss=0.592]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 253.27it/s, mean loss=0.615, macro f1=0.522]\ntrain [epoch 5 batch 60]: : 1440it [00:08, 178.58it/s, mean train loss=0.437]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 270.69it/s, mean loss=0.679, macro f1=0.632]\ntrain [epoch 6 batch 60]: : 1440it [00:07, 202.59it/s, mean train loss=0.392]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 239.06it/s, mean loss=0.565, macro f1=0.665]\n[I 2024-10-19 19:52:16,445] Trial 10 pruned. \n\n\nTrial 11 params: {'batch_size': 24, 'lr': 0.0009785389379804816, 'weight_decay': 0.003223015461764898, 'reduce_lr_factor': 0.03006433458512642, 'momentum': 0.8470825958578355, 'early_stop_patience': 1, 'resize_length': 96, 'grayscale': True, 'jitter': False, 'horizontal_flip': False, 'rotation': True, 'vgg16_batch_norm': True}\nTrial 11 fold 1\n\n\ntrain [epoch 1 batch 60]: : 1440it [00:10, 134.19it/s, mean train loss=1.59]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 202.72it/s, mean loss=1.23, macro f1=0.301]\ntrain [epoch 2 batch 60]: : 1440it [00:10, 134.06it/s, mean train loss=1.16]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 206.54it/s, mean loss=0.807, macro f1=0.472]\ntrain [epoch 3 batch 60]: : 1440it [00:10, 133.92it/s, mean train loss=0.763]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 211.59it/s, mean loss=0.627, macro f1=0.559]\ntrain [epoch 4 batch 60]: : 1440it [00:10, 132.98it/s, mean train loss=0.628]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 196.06it/s, mean loss=0.614, macro f1=0.685]\ntrain [epoch 5 batch 60]: : 1440it [00:10, 139.06it/s, mean train loss=0.564]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 217.30it/s, mean loss=0.53, macro f1=0.698]\ntrain [epoch 6 batch 60]: : 1440it [00:10, 139.93it/s, mean train loss=0.446]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 215.87it/s, mean loss=0.473, macro f1=0.765]\n[I 2024-10-19 19:53:33,229] Trial 11 pruned. \n\n\nTrial 12 params: {'batch_size': 24, 'lr': 0.0008410583816770148, 'weight_decay': 0.00337328767144783, 'reduce_lr_factor': 0.035082533355625806, 'momentum': 0.8617079018609524, 'early_stop_patience': 1, 'resize_length': 96, 'grayscale': False, 'jitter': False, 'horizontal_flip': False, 'rotation': True, 'vgg16_batch_norm': True}\nTrial 12 fold 1\n\n\ntrain [epoch 1 batch 60]: : 1440it [00:10, 137.29it/s, mean train loss=1.66]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 209.48it/s, mean loss=1.24, macro f1=0.286]\ntrain [epoch 2 batch 60]: : 1440it [00:10, 138.87it/s, mean train loss=1.04]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 214.08it/s, mean loss=0.734, macro f1=0.656]\ntrain [epoch 3 batch 60]: : 1440it [00:10, 138.31it/s, mean train loss=0.663]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 200.52it/s, mean loss=0.68, macro f1=0.599]\ntrain [epoch 4 batch 60]: : 1440it [00:10, 136.52it/s, mean train loss=0.414]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 200.82it/s, mean loss=0.525, macro f1=0.762]\ntrain [epoch 5 batch 60]: : 1440it [00:10, 137.73it/s, mean train loss=0.44]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 210.41it/s, mean loss=0.468, macro f1=0.757]\ntrain [epoch 6 batch 60]: : 1440it [00:10, 135.44it/s, mean train loss=0.382]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 201.08it/s, mean loss=0.496, macro f1=0.784]\n[I 2024-10-19 19:54:49,350] Trial 12 pruned. \n\n\nTrial 13 params: {'batch_size': 24, 'lr': 0.0008576302353547708, 'weight_decay': 0.006391681065068446, 'reduce_lr_factor': 0.011662590535982367, 'momentum': 0.8025557342473153, 'early_stop_patience': 3, 'resize_length': 224, 'grayscale': True, 'jitter': True, 'horizontal_flip': False, 'rotation': True, 'vgg16_batch_norm': True}\nTrial 13 fold 1\n\n\ntrain [epoch 1 batch 60]: : 1440it [00:31, 46.19it/s, mean train loss=1.6]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 106.73it/s, mean loss=1.27, macro f1=0.267]\ntrain [epoch 2 batch 60]: : 1440it [00:31, 46.32it/s, mean train loss=1.11]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 107.08it/s, mean loss=0.877, macro f1=0.457]\ntrain [epoch 3 batch 60]: : 1440it [00:31, 46.39it/s, mean train loss=0.831]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 106.30it/s, mean loss=0.727, macro f1=0.522]\ntrain [epoch 4 batch 60]: : 1440it [00:31, 46.45it/s, mean train loss=0.727]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 107.48it/s, mean loss=0.616, macro f1=0.609]\ntrain [epoch 5 batch 60]: : 1440it [00:31, 46.44it/s, mean train loss=0.569]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 106.09it/s, mean loss=0.51, macro f1=0.746]\ntrain [epoch 6 batch 60]: : 1440it [00:31, 46.29it/s, mean train loss=0.509]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 106.96it/s, mean loss=0.474, macro f1=0.761]\n[I 2024-10-19 19:58:18,714] Trial 13 pruned. \n\n\nTrial 14 params: {'batch_size': 32, 'lr': 0.0003732888404852332, 'weight_decay': 0.0018890159484641644, 'reduce_lr_factor': 0.03510400357330958, 'momentum': 0.8673167170090301, 'early_stop_patience': 2, 'resize_length': 64, 'grayscale': True, 'jitter': True, 'horizontal_flip': True, 'rotation': False, 'vgg16_batch_norm': True}\nTrial 14 fold 1\n\n\ntrain [epoch 1 batch 45]: : 1440it [00:08, 171.21it/s, mean train loss=1.77]\nvalidate [batch 12]: : 384it [00:01, 252.89it/s, mean loss=1.85, macro f1=0.0509]\ntrain [epoch 2 batch 45]: : 1440it [00:08, 173.91it/s, mean train loss=1.2]\nvalidate [batch 12]: : 384it [00:01, 211.11it/s, mean loss=0.994, macro f1=0.371]\ntrain [epoch 3 batch 45]: : 1440it [00:08, 173.95it/s, mean train loss=0.97]\nvalidate [batch 12]: : 384it [00:01, 245.78it/s, mean loss=0.792, macro f1=0.479]\ntrain [epoch 4 batch 45]: : 1440it [00:08, 171.31it/s, mean train loss=0.715]\nvalidate [batch 12]: : 384it [00:01, 251.80it/s, mean loss=0.688, macro f1=0.484]\ntrain [epoch 5 batch 45]: : 1440it [00:08, 175.26it/s, mean train loss=0.589]\nvalidate [batch 12]: : 384it [00:01, 246.78it/s, mean loss=0.592, macro f1=0.596]\ntrain [epoch 6 batch 45]: : 1440it [00:08, 166.68it/s, mean train loss=0.548]\nvalidate [batch 12]: : 384it [00:01, 251.51it/s, mean loss=0.54, macro f1=0.658]\n[I 2024-10-19 19:59:21,146] Trial 14 pruned. \n\n\nTrial 15 params: {'batch_size': 24, 'lr': 0.00047572214685467765, 'weight_decay': 0.004737713732629837, 'reduce_lr_factor': 0.07534940915322018, 'momentum': 0.8825874139599189, 'early_stop_patience': 3, 'resize_length': 128, 'grayscale': False, 'jitter': False, 'horizontal_flip': False, 'rotation': True, 'vgg16_batch_norm': True}\nTrial 15 fold 1\n\n\ntrain [epoch 1 batch 60]: : 1440it [00:14, 102.72it/s, mean train loss=1.67]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 182.65it/s, mean loss=1.3, macro f1=0.202]\ntrain [epoch 2 batch 60]: : 1440it [00:14, 101.37it/s, mean train loss=1.19]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 182.43it/s, mean loss=0.89, macro f1=0.377]\ntrain [epoch 3 batch 60]: : 1440it [00:13, 103.24it/s, mean train loss=0.761]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 181.82it/s, mean loss=0.624, macro f1=0.623]\ntrain [epoch 4 batch 60]: : 1440it [00:14, 102.55it/s, mean train loss=0.592]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 183.40it/s, mean loss=0.458, macro f1=0.745]\ntrain [epoch 5 batch 60]: : 1440it [00:13, 103.48it/s, mean train loss=0.453]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 180.82it/s, mean loss=0.425, macro f1=0.768]\ntrain [epoch 6 batch 60]: : 1440it [00:14, 101.79it/s, mean train loss=0.416]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 181.83it/s, mean loss=0.447, macro f1=0.781]\n[I 2024-10-19 20:00:59,958] Trial 15 pruned. \n\n\nTrial 16 params: {'batch_size': 24, 'lr': 0.0008731032162838153, 'weight_decay': 0.004471853039748174, 'reduce_lr_factor': 0.0429246070738525, 'momentum': 0.8376014150056645, 'early_stop_patience': 1, 'resize_length': 64, 'grayscale': False, 'jitter': True, 'horizontal_flip': False, 'rotation': False, 'vgg16_batch_norm': True}\nTrial 16 fold 1\n\n\ntrain [epoch 1 batch 60]: : 1440it [00:09, 157.47it/s, mean train loss=1.45]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 229.93it/s, mean loss=1.02, macro f1=0.295]\ntrain [epoch 2 batch 60]: : 1440it [00:08, 161.15it/s, mean train loss=0.762]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 224.79it/s, mean loss=0.747, macro f1=0.622]\ntrain [epoch 3 batch 60]: : 1440it [00:09, 153.31it/s, mean train loss=0.475]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 203.95it/s, mean loss=0.447, macro f1=0.731]\ntrain [epoch 4 batch 60]: : 1440it [00:08, 161.07it/s, mean train loss=0.333]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 230.01it/s, mean loss=0.485, macro f1=0.75]\n\n\nEarly stopping at epoch 4\n\n\ntest [batch 38]: : 912it [00:03, 248.36it/s, mean loss=0.462, macro f1=0.786]\n\n\nTrial 16 fold 2\n\n\ntrain [epoch 1 batch 60]: : 1440it [00:08, 160.16it/s, mean train loss=1.52]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 218.59it/s, mean loss=1.05, macro f1=0.307]\ntrain [epoch 2 batch 60]: : 1440it [00:09, 154.12it/s, mean train loss=0.836]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 230.88it/s, mean loss=0.7, macro f1=0.571]\ntrain [epoch 3 batch 60]: : 1440it [00:08, 161.31it/s, mean train loss=0.505]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 231.74it/s, mean loss=0.512, macro f1=0.687]\ntrain [epoch 4 batch 60]: : 1440it [00:08, 161.10it/s, mean train loss=0.317]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 224.79it/s, mean loss=0.464, macro f1=0.821]\ntrain [epoch 5 batch 60]: : 1440it [00:09, 152.06it/s, mean train loss=0.248]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 229.22it/s, mean loss=0.544, macro f1=0.789]\n\n\nEarly stopping at epoch 5\n\n\ntest [batch 38]: : 912it [00:03, 246.18it/s, mean loss=0.507, macro f1=0.765]\n\n\nTrial 16 fold 3\n\n\ntrain [epoch 1 batch 60]: : 1440it [00:08, 160.63it/s, mean train loss=1.43]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 227.05it/s, mean loss=1.02, macro f1=0.367]\ntrain [epoch 2 batch 60]: : 1440it [00:09, 157.80it/s, mean train loss=0.761]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 195.16it/s, mean loss=0.572, macro f1=0.691]\ntrain [epoch 3 batch 60]: : 1440it [00:09, 158.07it/s, mean train loss=0.451]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 231.37it/s, mean loss=0.511, macro f1=0.706]\ntrain [epoch 4 batch 60]: : 1440it [00:08, 160.68it/s, mean train loss=0.34]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 229.84it/s, mean loss=0.379, macro f1=0.787]\ntrain [epoch 5 batch 60]: : 1440it [00:09, 159.20it/s, mean train loss=0.262]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 197.62it/s, mean loss=0.396, macro f1=0.761]\n\n\nEarly stopping at epoch 5\n\n\ntest [batch 38]: : 912it [00:03, 229.26it/s, mean loss=0.422, macro f1=0.803]\n[I 2024-10-19 20:03:48,241] Trial 16 finished with value: 0.7846964692207749 and parameters: {'batch_size': 24, 'lr': 0.0008731032162838153, 'weight_decay': 0.004471853039748174, 'reduce_lr_factor': 0.0429246070738525, 'momentum': 0.8376014150056645, 'early_stop_patience': 1, 'resize_length': 64, 'grayscale': False, 'jitter': True, 'horizontal_flip': False, 'rotation': False, 'vgg16_batch_norm': True}. Best is trial 3 with value: 0.7984415820297991.\n\n\nTrial 17 params: {'batch_size': 24, 'lr': 0.0009916011134837074, 'weight_decay': 0.0043502690778127765, 'reduce_lr_factor': 0.025162260173005026, 'momentum': 0.8374267532215826, 'early_stop_patience': 1, 'resize_length': 64, 'grayscale': True, 'jitter': True, 'horizontal_flip': True, 'rotation': False, 'vgg16_batch_norm': True}\nTrial 17 fold 1\n\n\ntrain [epoch 1 batch 60]: : 1440it [00:08, 160.16it/s, mean train loss=1.51]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 230.41it/s, mean loss=1.05, macro f1=0.324]\ntrain [epoch 2 batch 60]: : 1440it [00:08, 160.87it/s, mean train loss=0.95]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 228.07it/s, mean loss=0.757, macro f1=0.533]\ntrain [epoch 3 batch 60]: : 1440it [00:09, 156.36it/s, mean train loss=0.646]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 228.66it/s, mean loss=0.631, macro f1=0.555]\ntrain [epoch 4 batch 60]: : 1440it [00:08, 160.45it/s, mean train loss=0.554]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 227.19it/s, mean loss=0.573, macro f1=0.557]\ntrain [epoch 5 batch 60]: : 1440it [00:08, 160.90it/s, mean train loss=0.407]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 221.05it/s, mean loss=0.445, macro f1=0.74]\ntrain [epoch 6 batch 60]: : 1440it [00:09, 150.36it/s, mean train loss=0.376]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 217.21it/s, mean loss=0.447, macro f1=0.757]\n[I 2024-10-19 20:04:55,227] Trial 17 pruned. \n\n\nTrial 18 params: {'batch_size': 24, 'lr': 0.0008682908052384192, 'weight_decay': 0.0009480799177705037, 'reduce_lr_factor': 0.04269622508194973, 'momentum': 0.8061197489142833, 'early_stop_patience': 1, 'resize_length': 32, 'grayscale': True, 'jitter': True, 'horizontal_flip': True, 'rotation': False, 'vgg16_batch_norm': False}\nTrial 18 fold 1\n\n\ntrain [epoch 1 batch 60]: : 1440it [00:07, 183.70it/s, mean train loss=2.29]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 257.15it/s, mean loss=2.27, macro f1=0.0581]\ntrain [epoch 2 batch 60]: : 1440it [00:07, 185.40it/s, mean train loss=2.26]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 259.55it/s, mean loss=2.24, macro f1=0.0581]\ntrain [epoch 3 batch 60]: : 1440it [00:07, 180.87it/s, mean train loss=2.23]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 241.28it/s, mean loss=2.21, macro f1=0.0581]\ntrain [epoch 4 batch 60]: : 1440it [00:08, 177.00it/s, mean train loss=2.2]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 261.68it/s, mean loss=2.19, macro f1=0.0581]\ntrain [epoch 5 batch 60]: : 1440it [00:07, 186.95it/s, mean train loss=2.18]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 255.71it/s, mean loss=2.16, macro f1=0.0581]\n[I 2024-10-19 20:05:44,075] Trial 18 pruned. Mean validation loss is still above or equal to 1.5 at the 5th epoch.\n\n\nTrial 19 params: {'batch_size': 24, 'lr': 0.0008848104981850575, 'weight_decay': 0.0018407051447151832, 'reduce_lr_factor': 0.020811431988625707, 'momentum': 0.834584433021492, 'early_stop_patience': 1, 'resize_length': 64, 'grayscale': False, 'jitter': True, 'horizontal_flip': False, 'rotation': False, 'vgg16_batch_norm': True}\nTrial 19 fold 1\n\n\ntrain [epoch 1 batch 60]: : 1440it [00:08, 162.12it/s, mean train loss=1.49]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 236.92it/s, mean loss=1.08, macro f1=0.452]\ntrain [epoch 2 batch 60]: : 1440it [00:09, 156.33it/s, mean train loss=0.76]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 235.72it/s, mean loss=0.771, macro f1=0.6]\ntrain [epoch 3 batch 60]: : 1440it [00:08, 167.14it/s, mean train loss=0.435]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 234.66it/s, mean loss=0.552, macro f1=0.727]\ntrain [epoch 4 batch 60]: : 1440it [00:09, 158.00it/s, mean train loss=0.32]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 237.24it/s, mean loss=0.434, macro f1=0.779]\ntrain [epoch 5 batch 60]: : 1440it [00:09, 158.78it/s, mean train loss=0.261]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 234.50it/s, mean loss=0.473, macro f1=0.707]\n\n\nEarly stopping at epoch 5\n\n\ntest [batch 38]: : 912it [00:03, 245.15it/s, mean loss=0.509, macro f1=0.725]\n\n\nTrial 19 fold 2\n\n\ntrain [epoch 1 batch 60]: : 1440it [00:09, 157.47it/s, mean train loss=1.53]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 237.00it/s, mean loss=1.11, macro f1=0.393]\ntrain [epoch 2 batch 60]: : 1440it [00:08, 161.41it/s, mean train loss=0.888]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 237.84it/s, mean loss=0.761, macro f1=0.551]\ntrain [epoch 3 batch 60]: : 1440it [00:08, 164.49it/s, mean train loss=0.544]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 244.43it/s, mean loss=0.569, macro f1=0.698]\ntrain [epoch 4 batch 60]: : 1440it [00:09, 158.39it/s, mean train loss=0.326]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 240.81it/s, mean loss=0.45, macro f1=0.768]\ntrain [epoch 5 batch 60]: : 1440it [00:08, 167.72it/s, mean train loss=0.319]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 235.98it/s, mean loss=0.404, macro f1=0.838]\ntrain [epoch 6 batch 60]: : 1440it [00:09, 158.64it/s, mean train loss=0.196]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 233.37it/s, mean loss=0.36, macro f1=0.847]\ntrain [epoch 7 batch 60]: : 1440it [00:09, 157.94it/s, mean train loss=0.176]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 231.07it/s, mean loss=0.466, macro f1=0.8]\n\n\nEarly stopping at epoch 7\n\n\ntest [batch 38]: : 912it [00:03, 257.24it/s, mean loss=0.546, macro f1=0.765]\n\n\nTrial 19 fold 3\n\n\ntrain [epoch 1 batch 60]: : 1440it [00:09, 159.91it/s, mean train loss=1.48]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 233.60it/s, mean loss=1.1, macro f1=0.435]\ntrain [epoch 2 batch 60]: : 1440it [00:09, 154.02it/s, mean train loss=0.778]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 231.30it/s, mean loss=0.631, macro f1=0.611]\ntrain [epoch 3 batch 60]: : 1440it [00:08, 163.50it/s, mean train loss=0.475]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 247.78it/s, mean loss=0.53, macro f1=0.744]\ntrain [epoch 4 batch 60]: : 1440it [00:08, 160.74it/s, mean train loss=0.355]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 240.03it/s, mean loss=0.492, macro f1=0.745]\ntrain [epoch 5 batch 60]: : 1440it [00:09, 158.38it/s, mean train loss=0.25]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 238.98it/s, mean loss=0.377, macro f1=0.826]\ntrain [epoch 6 batch 60]: : 1440it [00:09, 156.92it/s, mean train loss=0.189]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 240.47it/s, mean loss=0.331, macro f1=0.835]\ntrain [epoch 7 batch 60]: : 1440it [00:08, 161.46it/s, mean train loss=0.217]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 237.31it/s, mean loss=0.352, macro f1=0.84]\n\n\nEarly stopping at epoch 7\n\n\ntest [batch 38]: : 912it [00:03, 250.36it/s, mean loss=0.446, macro f1=0.807]\n[I 2024-10-19 20:09:23,031] Trial 19 finished with value: 0.7655126149356413 and parameters: {'batch_size': 24, 'lr': 0.0008848104981850575, 'weight_decay': 0.0018407051447151832, 'reduce_lr_factor': 0.020811431988625707, 'momentum': 0.834584433021492, 'early_stop_patience': 1, 'resize_length': 64, 'grayscale': False, 'jitter': True, 'horizontal_flip': False, 'rotation': False, 'vgg16_batch_norm': True}. Best is trial 3 with value: 0.7984415820297991.\n\n\nTrial 20 params: {'batch_size': 24, 'lr': 1.061325571229696e-05, 'weight_decay': 0.00012321089739614753, 'reduce_lr_factor': 0.042649330962282274, 'momentum': 0.8200846878990533, 'early_stop_patience': 1, 'resize_length': 96, 'grayscale': True, 'jitter': True, 'horizontal_flip': False, 'rotation': False, 'vgg16_batch_norm': True}\nTrial 20 fold 1\n\n\ntrain [epoch 1 batch 60]: : 1440it [00:11, 120.04it/s, mean train loss=2.23]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 206.10it/s, mean loss=2.16, macro f1=0.156]\ntrain [epoch 2 batch 60]: : 1440it [00:11, 126.53it/s, mean train loss=2.12]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 205.53it/s, mean loss=2.01, macro f1=0.158]\ntrain [epoch 3 batch 60]: : 1440it [00:11, 122.54it/s, mean train loss=2.01]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 188.30it/s, mean loss=1.94, macro f1=0.176]\ntrain [epoch 4 batch 60]: : 1440it [00:11, 126.71it/s, mean train loss=1.94]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 207.82it/s, mean loss=1.87, macro f1=0.19]\ntrain [epoch 5 batch 60]: : 1440it [00:11, 126.80it/s, mean train loss=1.88]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 206.49it/s, mean loss=1.83, macro f1=0.19]\n[I 2024-10-19 20:10:32,179] Trial 20 pruned. Mean validation loss is still above or equal to 1.5 at the 5th epoch.\n\n\nTrial 21 params: {'batch_size': 24, 'lr': 0.0009151928451481278, 'weight_decay': 0.001643161022766956, 'reduce_lr_factor': 0.01695784221558099, 'momentum': 0.8389391528297435, 'early_stop_patience': 1, 'resize_length': 64, 'grayscale': False, 'jitter': True, 'horizontal_flip': False, 'rotation': False, 'vgg16_batch_norm': True}\nTrial 21 fold 1\n\n\ntrain [epoch 1 batch 60]: : 1440it [00:09, 150.63it/s, mean train loss=1.45]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 226.62it/s, mean loss=0.983, macro f1=0.522]\ntrain [epoch 2 batch 60]: : 1440it [00:08, 160.89it/s, mean train loss=0.726]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 231.20it/s, mean loss=0.665, macro f1=0.693]\ntrain [epoch 3 batch 60]: : 1440it [00:08, 161.09it/s, mean train loss=0.429]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 216.35it/s, mean loss=0.519, macro f1=0.756]\ntrain [epoch 4 batch 60]: : 1440it [00:09, 151.21it/s, mean train loss=0.334]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 229.33it/s, mean loss=0.43, macro f1=0.762]\ntrain [epoch 5 batch 60]: : 1440it [00:08, 161.05it/s, mean train loss=0.275]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 230.47it/s, mean loss=0.428, macro f1=0.795]\ntrain [epoch 6 batch 60]: : 1440it [00:08, 160.57it/s, mean train loss=0.156]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 223.72it/s, mean loss=0.475, macro f1=0.807]\n\n\nEarly stopping at epoch 6\n\n\ntest [batch 38]: : 912it [00:03, 247.95it/s, mean loss=0.417, macro f1=0.836]\n\n\nTrial 21 fold 2\n\n\ntrain [epoch 1 batch 60]: : 1440it [00:09, 149.76it/s, mean train loss=1.47]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 238.43it/s, mean loss=1.07, macro f1=0.304]\ntrain [epoch 2 batch 60]: : 1440it [00:08, 166.54it/s, mean train loss=0.837]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 245.84it/s, mean loss=0.604, macro f1=0.689]\ntrain [epoch 3 batch 60]: : 1440it [00:08, 164.09it/s, mean train loss=0.513]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 243.11it/s, mean loss=0.822, macro f1=0.669]\n\n\nEarly stopping at epoch 3\n\n\ntest [batch 38]: : 912it [00:03, 255.95it/s, mean loss=0.779, macro f1=0.645]\n\n\nTrial 21 fold 3\n\n\ntrain [epoch 1 batch 60]: : 1440it [00:09, 155.86it/s, mean train loss=1.46]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 233.65it/s, mean loss=0.992, macro f1=0.608]\ntrain [epoch 2 batch 60]: : 1440it [00:08, 161.35it/s, mean train loss=0.751]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 204.15it/s, mean loss=0.59, macro f1=0.696]\ntrain [epoch 3 batch 60]: : 1440it [00:09, 159.60it/s, mean train loss=0.483]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 229.42it/s, mean loss=0.429, macro f1=0.768]\ntrain [epoch 4 batch 60]: : 1440it [00:09, 158.07it/s, mean train loss=0.308]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 238.76it/s, mean loss=0.495, macro f1=0.763]\n\n\nEarly stopping at epoch 4\n\n\ntest [batch 38]: : 912it [00:03, 260.45it/s, mean loss=0.576, macro f1=0.763]\n[I 2024-10-19 20:13:07,988] Trial 21 finished with value: 0.7478920106660203 and parameters: {'batch_size': 24, 'lr': 0.0009151928451481278, 'weight_decay': 0.001643161022766956, 'reduce_lr_factor': 0.01695784221558099, 'momentum': 0.8389391528297435, 'early_stop_patience': 1, 'resize_length': 64, 'grayscale': False, 'jitter': True, 'horizontal_flip': False, 'rotation': False, 'vgg16_batch_norm': True}. Best is trial 3 with value: 0.7984415820297991.\n\n\nTrial 22 params: {'batch_size': 24, 'lr': 0.000807275411396522, 'weight_decay': 0.003030434542243136, 'reduce_lr_factor': 0.0233790072538175, 'momentum': 0.8539383714917498, 'early_stop_patience': 1, 'resize_length': 64, 'grayscale': False, 'jitter': True, 'horizontal_flip': False, 'rotation': False, 'vgg16_batch_norm': True}\nTrial 22 fold 1\n\n\ntrain [epoch 1 batch 60]: : 1440it [00:09, 157.83it/s, mean train loss=1.47]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 233.52it/s, mean loss=1.07, macro f1=0.476]\ntrain [epoch 2 batch 60]: : 1440it [00:08, 167.07it/s, mean train loss=0.759]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 202.36it/s, mean loss=0.572, macro f1=0.668]\ntrain [epoch 3 batch 60]: : 1440it [00:08, 161.72it/s, mean train loss=0.477]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 236.76it/s, mean loss=0.492, macro f1=0.735]\ntrain [epoch 4 batch 60]: : 1440it [00:08, 165.37it/s, mean train loss=0.343]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 233.48it/s, mean loss=0.531, macro f1=0.709]\n\n\nEarly stopping at epoch 4\n\n\ntest [batch 38]: : 912it [00:03, 256.19it/s, mean loss=0.52, macro f1=0.715]\n\n\nTrial 22 fold 2\n\n\ntrain [epoch 1 batch 60]: : 1440it [00:09, 157.39it/s, mean train loss=1.57]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 186.62it/s, mean loss=1.06, macro f1=0.45]\ntrain [epoch 2 batch 60]: : 1440it [00:09, 156.04it/s, mean train loss=0.868]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 214.11it/s, mean loss=0.804, macro f1=0.492]\ntrain [epoch 3 batch 60]: : 1440it [00:08, 161.00it/s, mean train loss=0.519]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 232.43it/s, mean loss=0.564, macro f1=0.662]\ntrain [epoch 4 batch 60]: : 1440it [00:08, 161.19it/s, mean train loss=0.361]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 214.25it/s, mean loss=0.522, macro f1=0.712]\ntrain [epoch 5 batch 60]: : 1440it [00:09, 155.10it/s, mean train loss=0.249]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 229.17it/s, mean loss=0.515, macro f1=0.808]\ntrain [epoch 6 batch 60]: : 1440it [00:08, 160.86it/s, mean train loss=0.26]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 229.36it/s, mean loss=0.473, macro f1=0.805]\n[I 2024-10-19 20:15:02,347] Trial 22 pruned. \n\n\nTrial 23 params: {'batch_size': 24, 'lr': 0.0008994724021985357, 'weight_decay': 0.00398890739787271, 'reduce_lr_factor': 0.02268743414914747, 'momentum': 0.8216647476937027, 'early_stop_patience': 1, 'resize_length': 32, 'grayscale': False, 'jitter': True, 'horizontal_flip': False, 'rotation': False, 'vgg16_batch_norm': True}\nTrial 23 fold 1\n\n\ntrain [epoch 1 batch 60]: : 1440it [00:07, 183.93it/s, mean train loss=1.41]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 239.79it/s, mean loss=1.11, macro f1=0.336]\ntrain [epoch 2 batch 60]: : 1440it [00:08, 176.84it/s, mean train loss=0.73]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 227.36it/s, mean loss=0.799, macro f1=0.587]\ntrain [epoch 3 batch 60]: : 1440it [00:07, 183.45it/s, mean train loss=0.437]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 240.45it/s, mean loss=0.775, macro f1=0.658]\ntrain [epoch 4 batch 60]: : 1440it [00:07, 183.56it/s, mean train loss=0.34]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 242.91it/s, mean loss=0.542, macro f1=0.752]\ntrain [epoch 5 batch 60]: : 1440it [00:07, 183.21it/s, mean train loss=0.284]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 228.38it/s, mean loss=0.669, macro f1=0.706]\n\n\nEarly stopping at epoch 5\n\n\ntest [batch 38]: : 912it [00:04, 209.84it/s, mean loss=0.653, macro f1=0.722]\n\n\nTrial 23 fold 2\n\n\ntrain [epoch 1 batch 60]: : 1440it [00:07, 181.85it/s, mean train loss=1.4]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 243.83it/s, mean loss=0.916, macro f1=0.523]\ntrain [epoch 2 batch 60]: : 1440it [00:07, 183.08it/s, mean train loss=0.625]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 244.55it/s, mean loss=0.624, macro f1=0.579]\ntrain [epoch 3 batch 60]: : 1440it [00:07, 182.69it/s, mean train loss=0.451]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 236.83it/s, mean loss=0.803, macro f1=0.672]\n\n\nEarly stopping at epoch 3\n\n\ntest [batch 38]: : 912it [00:04, 226.29it/s, mean loss=0.731, macro f1=0.697]\n\n\nTrial 23 fold 3\n\n\ntrain [epoch 1 batch 60]: : 1440it [00:08, 179.22it/s, mean train loss=1.39]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 245.18it/s, mean loss=0.889, macro f1=0.37]\ntrain [epoch 2 batch 60]: : 1440it [00:07, 182.83it/s, mean train loss=0.698]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 244.52it/s, mean loss=0.652, macro f1=0.646]\ntrain [epoch 3 batch 60]: : 1440it [00:08, 177.93it/s, mean train loss=0.48]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 245.99it/s, mean loss=0.579, macro f1=0.772]\ntrain [epoch 4 batch 60]: : 1440it [00:08, 171.50it/s, mean train loss=0.347]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 242.08it/s, mean loss=0.641, macro f1=0.747]\n\n\nEarly stopping at epoch 4\n\n\ntest [batch 38]: : 912it [00:03, 261.42it/s, mean loss=0.62, macro f1=0.739]\n[I 2024-10-19 20:17:14,048] Trial 23 finished with value: 0.7192742660990045 and parameters: {'batch_size': 24, 'lr': 0.0008994724021985357, 'weight_decay': 0.00398890739787271, 'reduce_lr_factor': 0.02268743414914747, 'momentum': 0.8216647476937027, 'early_stop_patience': 1, 'resize_length': 32, 'grayscale': False, 'jitter': True, 'horizontal_flip': False, 'rotation': False, 'vgg16_batch_norm': True}. Best is trial 3 with value: 0.7984415820297991.\n\n\nTrial 24 params: {'batch_size': 24, 'lr': 0.0005885471433678505, 'weight_decay': 0.0021420265945451033, 'reduce_lr_factor': 0.04064712585336279, 'momentum': 0.8321911216465314, 'early_stop_patience': 1, 'resize_length': 96, 'grayscale': False, 'jitter': True, 'horizontal_flip': False, 'rotation': False, 'vgg16_batch_norm': True}\nTrial 24 fold 1\n\n\ntrain [epoch 1 batch 60]: : 1440it [00:11, 126.65it/s, mean train loss=1.57]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 197.80it/s, mean loss=1.22, macro f1=0.242]\ntrain [epoch 2 batch 60]: : 1440it [00:11, 124.82it/s, mean train loss=0.987]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 198.43it/s, mean loss=0.809, macro f1=0.58]\ntrain [epoch 3 batch 60]: : 1440it [00:11, 125.21it/s, mean train loss=0.661]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 206.84it/s, mean loss=0.579, macro f1=0.67]\ntrain [epoch 4 batch 60]: : 1440it [00:11, 125.34it/s, mean train loss=0.466]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 205.72it/s, mean loss=0.49, macro f1=0.779]\ntrain [epoch 5 batch 60]: : 1440it [00:12, 119.67it/s, mean train loss=0.343]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 199.78it/s, mean loss=0.482, macro f1=0.738]\ntrain [epoch 6 batch 60]: : 1440it [00:11, 125.32it/s, mean train loss=0.254]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:01&lt;00:00, 206.65it/s, mean loss=0.494, macro f1=0.769]\n[I 2024-10-19 20:18:36,863] Trial 24 pruned. \n\n\n\n\nvgg16_study.best_params\n\n\n{'batch_size': 24,\n 'lr': 0.000988490099678634,\n 'weight_decay': 0.0010204481074802807,\n 'reduce_lr_factor': 0.028798908048535125,\n 'momentum': 0.8306488083981494,\n 'early_stop_patience': 2,\n 'resize_length': 64,\n 'grayscale': True,\n 'jitter': True,\n 'horizontal_flip': True,\n 'rotation': False,\n 'vgg16_batch_norm': True}\n\n\n\nLike the MLP, the best hyperparameters include resizing of the images to 64 by 64. However, this time, VGG-16 seems to perform best when some data augmentation is applied (colour jittering and random horizontal flip). The results also indicate that batch normalisation should be used.\n\n\nVGG-16 Pretrained\nNo model-specific hyperparameters were tuned for the pre-trained VGG-16 model since its architecture is fixed. However, in the future, I may want to consider whether to freeze the convolutional layers.\nAs mentioned earlier, no resizing or grayscaling was applied to the images since the pretrained model requires using the same transformations applied when it was trained on ImageNet. However, whether to use data augmentation is still part of the search space.\n\ndef vgg16_pretrained_from_params(params: dict) -&gt; torchvision.models.vgg.VGG:\n    return make_vgg16_pretrained()\n\n\nvgg16_pretrained_study = optuna.create_study(\n    study_name=\"VGG16 Pretrained study\",\n    sampler=optuna.samplers.TPESampler(seed=0),\n    pruner=optuna.pruners.MedianPruner(n_startup_trials=3, n_warmup_steps=5),\n    directions=[\"maximize\"],\n    storage=optuna.storages.JournalStorage(\n        optuna.storages.journal.JournalFileBackend(\"vgg16_pretrained_journal.log\"),\n    ),\n    load_if_exists=True,\n)\n\nvgg16_pretrained_study.optimize(\n    make_objective(\n        lambda trial: None,\n        vgg16_pretrained_from_params,\n        ds_train,\n        transforms_override=torchvision.models.VGG16_Weights.IMAGENET1K_V1.transforms(),\n    ),\n    n_trials=N_TRIALS,\n    gc_after_trial=True\n)\n\n\n[I 2024-10-19 20:19:05,248] A new study created in Journal with name: VGG16 Pretrained study\n\n\nTrial 0 params: {'batch_size': 32, 'lr': 0.0007180374727086954, 'weight_decay': 0.006027633760716439, 'reduce_lr_factor': 0.05903948646972072, 'momentum': 0.8804944118743919, 'early_stop_patience': 2, 'jitter': False, 'horizontal_flip': True, 'rotation': True}\nTrial 0 fold 1\n\n\ntrain [epoch 1 batch 45]: : 1440it [00:24, 59.23it/s, mean train loss=1.61]\nvalidate [batch 12]: : 384it [00:03, 122.75it/s, mean loss=1.08, macro f1=0.287]\ntrain [epoch 2 batch 45]: : 1440it [00:24, 57.80it/s, mean train loss=0.903]\nvalidate [batch 12]: : 384it [00:03, 125.08it/s, mean loss=0.706, macro f1=0.724]\ntrain [epoch 3 batch 45]: : 1440it [00:24, 58.55it/s, mean train loss=0.582]\nvalidate [batch 12]: : 384it [00:03, 127.50it/s, mean loss=0.481, macro f1=0.782]\ntrain [epoch 4 batch 45]: : 1440it [00:24, 58.39it/s, mean train loss=0.411]\nvalidate [batch 12]: : 384it [00:03, 126.71it/s, mean loss=0.373, macro f1=0.836]\ntrain [epoch 5 batch 45]: : 1440it [00:24, 58.35it/s, mean train loss=0.319]\nvalidate [batch 12]: : 384it [00:03, 119.95it/s, mean loss=0.356, macro f1=0.856]\ntrain [epoch 6 batch 45]: : 1440it [00:24, 58.67it/s, mean train loss=0.232]\nvalidate [batch 12]: : 384it [00:03, 127.35it/s, mean loss=0.275, macro f1=0.846]\ntrain [epoch 7 batch 45]: : 1440it [00:24, 58.40it/s, mean train loss=0.208]\nvalidate [batch 12]: : 384it [00:03, 125.92it/s, mean loss=0.392, macro f1=0.841]\ntrain [epoch 8 batch 45]: : 1440it [00:24, 58.06it/s, mean train loss=0.185]\nvalidate [batch 12]: : 384it [00:03, 125.74it/s, mean loss=0.328, macro f1=0.855]\n\n\nEarly stopping at epoch 8\n\n\ntest [batch 29]: : 928it [00:07, 131.38it/s, mean loss=0.237, macro f1=0.886]\n\n\nTrial 0 fold 2\n\n\ntrain [epoch 1 batch 45]: : 1440it [00:24, 58.28it/s, mean train loss=1.64]\nvalidate [batch 12]: : 384it [00:03, 126.31it/s, mean loss=1.18, macro f1=0.482]\ntrain [epoch 2 batch 45]: : 1440it [00:24, 58.14it/s, mean train loss=0.914]\nvalidate [batch 12]: : 384it [00:02, 128.37it/s, mean loss=0.646, macro f1=0.725]\ntrain [epoch 3 batch 45]: : 1440it [00:24, 58.52it/s, mean train loss=0.56]\nvalidate [batch 12]: : 384it [00:03, 123.05it/s, mean loss=0.421, macro f1=0.814]\ntrain [epoch 4 batch 45]: : 1440it [00:24, 58.69it/s, mean train loss=0.444]\nvalidate [batch 12]: : 384it [00:03, 122.46it/s, mean loss=0.384, macro f1=0.824]\ntrain [epoch 5 batch 45]: : 1440it [00:24, 58.27it/s, mean train loss=0.302]\nvalidate [batch 12]: : 384it [00:02, 128.27it/s, mean loss=0.331, macro f1=0.869]\ntrain [epoch 6 batch 45]: : 1440it [00:24, 58.43it/s, mean train loss=0.269]\nvalidate [batch 12]: : 384it [00:02, 128.06it/s, mean loss=0.278, macro f1=0.876]\ntrain [epoch 7 batch 45]: : 1440it [00:24, 58.21it/s, mean train loss=0.235]\nvalidate [batch 12]: : 384it [00:03, 127.73it/s, mean loss=0.33, macro f1=0.85]\ntrain [epoch 8 batch 45]: : 1440it [00:24, 58.33it/s, mean train loss=0.23]\nvalidate [batch 12]: : 384it [00:03, 115.43it/s, mean loss=0.309, macro f1=0.869]\n\n\nEarly stopping at epoch 8\n\n\ntest [batch 29]: : 928it [00:07, 131.49it/s, mean loss=0.365, macro f1=0.853]\n\n\nTrial 0 fold 3\n\n\ntrain [epoch 1 batch 45]: : 1440it [00:24, 58.75it/s, mean train loss=1.61]\nvalidate [batch 12]: : 384it [00:03, 124.03it/s, mean loss=1.07, macro f1=0.446]\ntrain [epoch 2 batch 45]: : 1440it [00:24, 58.70it/s, mean train loss=0.768]\nvalidate [batch 12]: : 384it [00:03, 127.65it/s, mean loss=0.655, macro f1=0.689]\ntrain [epoch 3 batch 45]: : 1440it [00:24, 58.08it/s, mean train loss=0.462]\nvalidate [batch 12]: : 384it [00:03, 126.18it/s, mean loss=0.429, macro f1=0.786]\ntrain [epoch 4 batch 45]: : 1440it [00:24, 58.28it/s, mean train loss=0.371]\nvalidate [batch 12]: : 384it [00:03, 126.48it/s, mean loss=0.397, macro f1=0.807]\ntrain [epoch 5 batch 45]: : 1440it [00:24, 58.26it/s, mean train loss=0.288]\nvalidate [batch 12]: : 384it [00:03, 127.55it/s, mean loss=0.311, macro f1=0.808]\ntrain [epoch 6 batch 45]: : 1440it [00:24, 58.58it/s, mean train loss=0.243]\nvalidate [batch 12]: : 384it [00:03, 124.19it/s, mean loss=0.287, macro f1=0.852]\ntrain [epoch 7 batch 45]: : 1440it [00:24, 58.62it/s, mean train loss=0.18]\nvalidate [batch 12]: : 384it [00:03, 125.50it/s, mean loss=0.309, macro f1=0.859]\ntrain [epoch 8 batch 45]: : 1440it [00:24, 58.31it/s, mean train loss=0.136]\nvalidate [batch 12]: : 384it [00:03, 127.37it/s, mean loss=0.313, macro f1=0.843]\n\n\nEarly stopping at epoch 8\n\n\ntest [batch 29]: : 928it [00:07, 128.64it/s, mean loss=0.27, macro f1=0.879]\n[I 2024-10-19 20:30:46,032] Trial 0 finished with value: 0.8728714092677149 and parameters: {'batch_size': 32, 'lr': 0.0007180374727086954, 'weight_decay': 0.006027633760716439, 'reduce_lr_factor': 0.05903948646972072, 'momentum': 0.8804944118743919, 'early_stop_patience': 2, 'jitter': False, 'horizontal_flip': True, 'rotation': True}. Best is trial 0 with value: 0.8728714092677149.\n\n\nTrial 1 params: {'batch_size': 32, 'lr': 0.0009263406719097344, 'weight_decay': 0.0007103605819788694, 'reduce_lr_factor': 0.017841636973138664, 'momentum': 0.8038414955136619, 'early_stop_patience': 3, 'jitter': False, 'horizontal_flip': True, 'rotation': False}\nTrial 1 fold 1\n\n\ntrain [epoch 1 batch 45]: : 1440it [00:24, 58.54it/s, mean train loss=1.49]\nvalidate [batch 12]: : 384it [00:03, 123.96it/s, mean loss=0.883, macro f1=0.565]\ntrain [epoch 2 batch 45]: : 1440it [00:24, 58.66it/s, mean train loss=0.625]\nvalidate [batch 12]: : 384it [00:03, 124.68it/s, mean loss=0.541, macro f1=0.732]\ntrain [epoch 3 batch 45]: : 1440it [00:24, 58.63it/s, mean train loss=0.396]\nvalidate [batch 12]: : 384it [00:03, 126.55it/s, mean loss=0.442, macro f1=0.809]\ntrain [epoch 4 batch 45]: : 1440it [00:24, 59.04it/s, mean train loss=0.288]\nvalidate [batch 12]: : 384it [00:03, 118.42it/s, mean loss=0.346, macro f1=0.851]\ntrain [epoch 5 batch 45]: : 1440it [00:24, 58.47it/s, mean train loss=0.223]\nvalidate [batch 12]: : 384it [00:03, 126.15it/s, mean loss=0.365, macro f1=0.839]\ntrain [epoch 6 batch 45]: : 1440it [00:24, 58.43it/s, mean train loss=0.176]\nvalidate [batch 12]: : 384it [00:03, 127.23it/s, mean loss=0.363, macro f1=0.847]\ntrain [epoch 7 batch 45]: : 1440it [00:24, 58.42it/s, mean train loss=0.166]\nvalidate [batch 12]: : 384it [00:03, 126.87it/s, mean loss=0.423, macro f1=0.789]\n\n\nEarly stopping at epoch 7\n\n\ntest [batch 29]: : 928it [00:07, 131.18it/s, mean loss=0.371, macro f1=0.837]\n\n\nTrial 1 fold 2\n\n\ntrain [epoch 1 batch 45]: : 1440it [00:24, 58.65it/s, mean train loss=1.55]\nvalidate [batch 12]: : 384it [00:03, 126.84it/s, mean loss=1.09, macro f1=0.373]\ntrain [epoch 2 batch 45]: : 1440it [00:24, 58.78it/s, mean train loss=0.955]\nvalidate [batch 12]: : 384it [00:03, 127.56it/s, mean loss=0.698, macro f1=0.697]\ntrain [epoch 3 batch 45]: : 1440it [00:24, 59.04it/s, mean train loss=0.612]\nvalidate [batch 12]: : 384it [00:03, 117.46it/s, mean loss=0.483, macro f1=0.779]\ntrain [epoch 4 batch 45]: : 1440it [00:24, 58.58it/s, mean train loss=0.472]\nvalidate [batch 12]: : 384it [00:03, 126.37it/s, mean loss=0.434, macro f1=0.759]\ntrain [epoch 5 batch 45]: : 1440it [00:24, 59.07it/s, mean train loss=0.384]\nvalidate [batch 12]: : 384it [00:03, 126.49it/s, mean loss=0.364, macro f1=0.833]\ntrain [epoch 6 batch 45]: : 1440it [00:24, 59.03it/s, mean train loss=0.299]\nvalidate [batch 12]: : 384it [00:03, 125.63it/s, mean loss=0.296, macro f1=0.87]\ntrain [epoch 7 batch 45]: : 1440it [00:24, 58.87it/s, mean train loss=0.261]\nvalidate [batch 12]: : 384it [00:03, 116.36it/s, mean loss=0.228, macro f1=0.891]\ntrain [epoch 8 batch 45]: : 1440it [00:24, 58.93it/s, mean train loss=0.199]\nvalidate [batch 12]: : 384it [00:03, 127.54it/s, mean loss=0.305, macro f1=0.897]\ntrain [epoch 9 batch 45]: : 1440it [00:24, 58.69it/s, mean train loss=0.157]\nvalidate [batch 12]: : 384it [00:03, 126.17it/s, mean loss=0.237, macro f1=0.894]\ntrain [epoch 10 batch 45]: : 1440it [00:24, 58.24it/s, mean train loss=0.141]\nvalidate [batch 12]: : 384it [00:03, 126.22it/s, mean loss=0.236, macro f1=0.902]\n\n\nEarly stopping at epoch 10\n\n\ntest [batch 29]: : 928it [00:07, 131.14it/s, mean loss=0.225, macro f1=0.882]\n\n\nTrial 1 fold 3\n\n\ntrain [epoch 1 batch 45]: : 1440it [00:24, 58.50it/s, mean train loss=1.41]\nvalidate [batch 12]: : 384it [00:03, 126.73it/s, mean loss=0.872, macro f1=0.709]\ntrain [epoch 2 batch 45]: : 1440it [00:24, 58.56it/s, mean train loss=0.656]\nvalidate [batch 12]: : 384it [00:03, 126.81it/s, mean loss=0.455, macro f1=0.75]\ntrain [epoch 3 batch 45]: : 1440it [00:24, 59.08it/s, mean train loss=0.396]\nvalidate [batch 12]: : 384it [00:03, 118.01it/s, mean loss=0.35, macro f1=0.826]\ntrain [epoch 4 batch 45]: : 1440it [00:24, 58.71it/s, mean train loss=0.299]\nvalidate [batch 12]: : 384it [00:03, 126.23it/s, mean loss=0.328, macro f1=0.841]\ntrain [epoch 5 batch 45]: : 1440it [00:24, 58.53it/s, mean train loss=0.245]\nvalidate [batch 12]: : 384it [00:03, 126.72it/s, mean loss=0.3, macro f1=0.871]\ntrain [epoch 6 batch 45]: : 1440it [00:24, 58.49it/s, mean train loss=0.195]\nvalidate [batch 12]: : 384it [00:03, 123.62it/s, mean loss=0.28, macro f1=0.868]\ntrain [epoch 7 batch 45]: : 1440it [00:24, 59.09it/s, mean train loss=0.165]\nvalidate [batch 12]: : 384it [00:03, 116.89it/s, mean loss=0.306, macro f1=0.88]\ntrain [epoch 8 batch 45]: : 1440it [00:24, 58.84it/s, mean train loss=0.176]\nvalidate [batch 12]: : 384it [00:03, 127.88it/s, mean loss=0.262, macro f1=0.897]\ntrain [epoch 9 batch 45]: : 1440it [00:24, 58.48it/s, mean train loss=0.13]\nvalidate [batch 12]: : 384it [00:03, 127.73it/s, mean loss=0.263, macro f1=0.886]\ntrain [epoch 10 batch 45]: : 1440it [00:24, 58.57it/s, mean train loss=0.11]\nvalidate [batch 12]: : 384it [00:03, 127.99it/s, mean loss=0.2, macro f1=0.899]\ntrain [epoch 11 batch 45]: : 1440it [00:24, 58.56it/s, mean train loss=0.115]\nvalidate [batch 12]: : 384it [00:03, 125.72it/s, mean loss=0.182, macro f1=0.889]\ntrain [epoch 12 batch 45]: : 1440it [00:24, 58.85it/s, mean train loss=0.0943]\nvalidate [batch 12]: : 384it [00:02, 128.17it/s, mean loss=0.213, macro f1=0.876]\ntrain [epoch 13 batch 45]: : 1440it [00:24, 58.42it/s, mean train loss=0.081]\nvalidate [batch 12]: : 384it [00:02, 128.01it/s, mean loss=0.217, macro f1=0.891]\ntrain [epoch 14 batch 45]: : 1440it [00:24, 58.64it/s, mean train loss=0.067]\nvalidate [batch 12]: : 384it [00:03, 127.61it/s, mean loss=0.178, macro f1=0.888]\ntrain [epoch 15 batch 45]: : 1440it [00:24, 58.61it/s, mean train loss=0.0602]\nvalidate [batch 12]: : 384it [00:03, 125.73it/s, mean loss=0.232, macro f1=0.88]\ntrain [epoch 16 batch 45]: : 1440it [00:24, 59.07it/s, mean train loss=0.0645]\nvalidate [batch 12]: : 384it [00:03, 114.82it/s, mean loss=0.253, macro f1=0.897]\ntrain [epoch 17 batch 45]: : 1440it [00:24, 58.55it/s, mean train loss=0.0456]\nvalidate [batch 12]: : 384it [00:03, 127.57it/s, mean loss=0.236, macro f1=0.9]\n\n\nEarly stopping at epoch 17\n\n\ntest [batch 29]: : 928it [00:07, 128.53it/s, mean loss=0.236, macro f1=0.909]\n[I 2024-10-19 20:47:03,893] Trial 1 finished with value: 0.875893723287808 and parameters: {'batch_size': 32, 'lr': 0.0009263406719097344, 'weight_decay': 0.0007103605819788694, 'reduce_lr_factor': 0.017841636973138664, 'momentum': 0.8038414955136619, 'early_stop_patience': 3, 'jitter': False, 'horizontal_flip': True, 'rotation': False}. Best is trial 1 with value: 0.875893723287808.\n\n\nTrial 2 params: {'batch_size': 24, 'lr': 0.0006435218111142486, 'weight_decay': 0.001433532874090464, 'reduce_lr_factor': 0.09502020253446256, 'momentum': 0.8991511811325137, 'early_stop_patience': 2, 'jitter': False, 'horizontal_flip': False, 'rotation': False}\nTrial 2 fold 1\n\n\ntrain [epoch 1 batch 60]: : 1440it [00:26, 53.77it/s, mean train loss=1.41]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 111.16it/s, mean loss=0.803, macro f1=0.616]\ntrain [epoch 2 batch 60]: : 1440it [00:26, 53.62it/s, mean train loss=0.62]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 112.70it/s, mean loss=0.568, macro f1=0.759]\ntrain [epoch 3 batch 60]: : 1440it [00:26, 53.48it/s, mean train loss=0.375]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.53it/s, mean loss=0.495, macro f1=0.765]\ntrain [epoch 4 batch 60]: : 1440it [00:26, 53.33it/s, mean train loss=0.29]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.87it/s, mean loss=0.372, macro f1=0.798]\ntrain [epoch 5 batch 60]: : 1440it [00:26, 53.41it/s, mean train loss=0.204]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.32it/s, mean loss=0.357, macro f1=0.845]\ntrain [epoch 6 batch 60]: : 1440it [00:26, 53.69it/s, mean train loss=0.146]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 104.24it/s, mean loss=0.413, macro f1=0.849]\ntrain [epoch 7 batch 60]: : 1440it [00:26, 53.61it/s, mean train loss=0.144]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.71it/s, mean loss=0.358, macro f1=0.856]\n\n\nEarly stopping at epoch 7\n\n\ntest [batch 38]: : 912it [00:08, 113.67it/s, mean loss=0.251, macro f1=0.9]\n\n\nTrial 2 fold 2\n\n\ntrain [epoch 1 batch 60]: : 1440it [00:26, 53.87it/s, mean train loss=1.35]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 104.45it/s, mean loss=0.646, macro f1=0.691]\ntrain [epoch 2 batch 60]: : 1440it [00:26, 53.51it/s, mean train loss=0.528]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 111.30it/s, mean loss=0.354, macro f1=0.863]\ntrain [epoch 3 batch 60]: : 1440it [00:27, 53.24it/s, mean train loss=0.258]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 111.33it/s, mean loss=0.263, macro f1=0.88]\ntrain [epoch 4 batch 60]: : 1440it [00:27, 53.21it/s, mean train loss=0.156]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.70it/s, mean loss=0.206, macro f1=0.913]\ntrain [epoch 5 batch 60]: : 1440it [00:26, 53.35it/s, mean train loss=0.129]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.38it/s, mean loss=0.241, macro f1=0.898]\ntrain [epoch 6 batch 60]: : 1440it [00:27, 53.21it/s, mean train loss=0.0876]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.28it/s, mean loss=0.3, macro f1=0.899]\n\n\nEarly stopping at epoch 6\n\n\ntest [batch 38]: : 912it [00:07, 119.76it/s, mean loss=0.306, macro f1=0.884]\n\n\nTrial 2 fold 3\n\n\ntrain [epoch 1 batch 60]: : 1440it [00:27, 53.22it/s, mean train loss=1.37]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 112.60it/s, mean loss=0.77, macro f1=0.545]\ntrain [epoch 2 batch 60]: : 1440it [00:26, 53.50it/s, mean train loss=0.502]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 110.95it/s, mean loss=0.496, macro f1=0.738]\ntrain [epoch 3 batch 60]: : 1440it [00:26, 53.66it/s, mean train loss=0.282]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 105.41it/s, mean loss=0.345, macro f1=0.852]\ntrain [epoch 4 batch 60]: : 1440it [00:27, 53.25it/s, mean train loss=0.22]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 107.32it/s, mean loss=0.257, macro f1=0.858]\ntrain [epoch 5 batch 60]: : 1440it [00:26, 53.46it/s, mean train loss=0.101]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 111.30it/s, mean loss=0.438, macro f1=0.804]\ntrain [epoch 6 batch 60]: : 1440it [00:26, 53.34it/s, mean train loss=0.104]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.06it/s, mean loss=0.321, macro f1=0.858]\n\n\nEarly stopping at epoch 6\n\n\ntest [batch 38]: : 912it [00:07, 119.63it/s, mean loss=0.252, macro f1=0.9]\n[I 2024-10-19 20:57:13,387] Trial 2 finished with value: 0.8944816106117478 and parameters: {'batch_size': 24, 'lr': 0.0006435218111142486, 'weight_decay': 0.001433532874090464, 'reduce_lr_factor': 0.09502020253446256, 'momentum': 0.8991511811325137, 'early_stop_patience': 2, 'jitter': False, 'horizontal_flip': False, 'rotation': False}. Best is trial 2 with value: 0.8944816106117478.\n\n\nTrial 3 params: {'batch_size': 32, 'lr': 0.0006207646569060094, 'weight_decay': 0.009437480785146241, 'reduce_lr_factor': 0.0713638269193135, 'momentum': 0.8683065011090194, 'early_stop_patience': 2, 'jitter': True, 'horizontal_flip': False, 'rotation': True}\nTrial 3 fold 1\n\n\ntrain [epoch 1 batch 45]: : 1440it [00:25, 57.20it/s, mean train loss=1.71]\nvalidate [batch 12]: : 384it [00:02, 129.49it/s, mean loss=1.21, macro f1=0.35]\ntrain [epoch 2 batch 45]: : 1440it [00:25, 56.97it/s, mean train loss=1.01]\nvalidate [batch 12]: : 384it [00:03, 122.05it/s, mean loss=0.673, macro f1=0.666]\ntrain [epoch 3 batch 45]: : 1440it [00:25, 57.45it/s, mean train loss=0.746]\nvalidate [batch 12]: : 384it [00:03, 117.92it/s, mean loss=0.677, macro f1=0.713]\ntrain [epoch 4 batch 45]: : 1440it [00:25, 57.26it/s, mean train loss=0.467]\nvalidate [batch 12]: : 384it [00:02, 129.56it/s, mean loss=0.469, macro f1=0.79]\ntrain [epoch 5 batch 45]: : 1440it [00:25, 56.89it/s, mean train loss=0.37]\nvalidate [batch 12]: : 384it [00:02, 129.32it/s, mean loss=0.365, macro f1=0.802]\ntrain [epoch 6 batch 45]: : 1440it [00:25, 56.73it/s, mean train loss=0.351]\nvalidate [batch 12]: : 384it [00:02, 129.46it/s, mean loss=0.331, macro f1=0.834]\n[I 2024-10-19 21:00:07,033] Trial 3 pruned. \n\n\nTrial 4 params: {'batch_size': 24, 'lr': 0.0003700736632331964, 'weight_decay': 0.0057019677041787965, 'reduce_lr_factor': 0.049474136211608837, 'momentum': 0.987791029231253, 'early_stop_patience': 1, 'jitter': True, 'horizontal_flip': True, 'rotation': True}\nTrial 4 fold 1\n\n\ntrain [epoch 1 batch 60]: : 1440it [00:27, 51.71it/s, mean train loss=1.59]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.16it/s, mean loss=1.61, macro f1=0.435]\ntrain [epoch 2 batch 60]: : 1440it [00:27, 51.79it/s, mean train loss=0.938]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 107.55it/s, mean loss=0.667, macro f1=0.705]\ntrain [epoch 3 batch 60]: : 1440it [00:27, 51.95it/s, mean train loss=0.568]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 109.71it/s, mean loss=0.513, macro f1=0.747]\ntrain [epoch 4 batch 60]: : 1440it [00:27, 51.83it/s, mean train loss=0.398]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 112.44it/s, mean loss=0.501, macro f1=0.811]\ntrain [epoch 5 batch 60]: : 1440it [00:27, 51.46it/s, mean train loss=0.355]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.16it/s, mean loss=0.418, macro f1=0.791]\ntrain [epoch 6 batch 60]: : 1440it [00:27, 51.59it/s, mean train loss=0.285]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 112.85it/s, mean loss=0.261, macro f1=0.885]\ntrain [epoch 7 batch 60]: : 1440it [00:27, 51.63it/s, mean train loss=0.184]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 112.90it/s, mean loss=0.238, macro f1=0.896]\ntrain [epoch 8 batch 60]: : 1440it [00:27, 51.67it/s, mean train loss=0.198]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.21it/s, mean loss=0.199, macro f1=0.903]\ntrain [epoch 9 batch 60]: : 1440it [00:27, 51.89it/s, mean train loss=0.156]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 104.40it/s, mean loss=0.253, macro f1=0.913]\n\n\nEarly stopping at epoch 9\n\n\ntest [batch 38]: : 912it [00:07, 119.49it/s, mean loss=0.265, macro f1=0.909]\n\n\nTrial 4 fold 2\n\n\ntrain [epoch 1 batch 60]: : 1440it [00:27, 51.54it/s, mean train loss=1.53]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.45it/s, mean loss=0.998, macro f1=0.379]\ntrain [epoch 2 batch 60]: : 1440it [00:27, 51.82it/s, mean train loss=0.928]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 105.14it/s, mean loss=0.548, macro f1=0.734]\ntrain [epoch 3 batch 60]: : 1440it [00:27, 51.82it/s, mean train loss=0.637]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.26it/s, mean loss=0.544, macro f1=0.673]\ntrain [epoch 4 batch 60]: : 1440it [00:27, 51.52it/s, mean train loss=0.481]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 112.23it/s, mean loss=0.421, macro f1=0.822]\ntrain [epoch 5 batch 60]: : 1440it [00:28, 51.30it/s, mean train loss=0.394]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.17it/s, mean loss=0.336, macro f1=0.833]\ntrain [epoch 6 batch 60]: : 1440it [00:27, 51.72it/s, mean train loss=0.336]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.36it/s, mean loss=0.277, macro f1=0.88]\ntrain [epoch 7 batch 60]: : 1440it [00:27, 51.55it/s, mean train loss=0.355]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.04it/s, mean loss=0.284, macro f1=0.88]\n\n\nEarly stopping at epoch 7\n\n\ntest [batch 38]: : 912it [00:07, 120.09it/s, mean loss=0.318, macro f1=0.819]\n\n\nTrial 4 fold 3\n\n\ntrain [epoch 1 batch 60]: : 1440it [00:27, 51.83it/s, mean train loss=1.52]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 112.81it/s, mean loss=0.837, macro f1=0.619]\ntrain [epoch 2 batch 60]: : 1440it [00:27, 51.59it/s, mean train loss=0.734]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 112.56it/s, mean loss=0.56, macro f1=0.717]\ntrain [epoch 3 batch 60]: : 1440it [00:27, 51.88it/s, mean train loss=0.56]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 103.18it/s, mean loss=0.439, macro f1=0.798]\ntrain [epoch 4 batch 60]: : 1440it [00:27, 51.87it/s, mean train loss=0.329]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 112.94it/s, mean loss=0.481, macro f1=0.81]\n\n\nEarly stopping at epoch 4\n\n\ntest [batch 38]: : 912it [00:07, 121.76it/s, mean loss=0.438, macro f1=0.85]\n[I 2024-10-19 21:11:04,546] Trial 4 finished with value: 0.8593910636015544 and parameters: {'batch_size': 24, 'lr': 0.0003700736632331964, 'weight_decay': 0.0057019677041787965, 'reduce_lr_factor': 0.049474136211608837, 'momentum': 0.987791029231253, 'early_stop_patience': 1, 'jitter': True, 'horizontal_flip': True, 'rotation': True}. Best is trial 2 with value: 0.8944816106117478.\n\n\nTrial 5 params: {'batch_size': 24, 'lr': 0.00011927138975266209, 'weight_decay': 0.006563295894652734, 'reduce_lr_factor': 0.022436465621375246, 'momentum': 0.8373506487192102, 'early_stop_patience': 2, 'jitter': True, 'horizontal_flip': True, 'rotation': True}\nTrial 5 fold 1\n\n\ntrain [epoch 1 batch 60]: : 1440it [00:27, 51.70it/s, mean train loss=1.99]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 112.72it/s, mean loss=1.79, macro f1=0.155]\ntrain [epoch 2 batch 60]: : 1440it [00:27, 52.04it/s, mean train loss=1.66]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.30it/s, mean loss=1.59, macro f1=0.151]\ntrain [epoch 3 batch 60]: : 1440it [00:27, 52.04it/s, mean train loss=1.47]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 104.71it/s, mean loss=1.32, macro f1=0.185]\ntrain [epoch 4 batch 60]: : 1440it [00:27, 51.89it/s, mean train loss=1.38]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.18it/s, mean loss=1.23, macro f1=0.228]\ntrain [epoch 5 batch 60]: : 1440it [00:27, 51.92it/s, mean train loss=1.21]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 111.25it/s, mean loss=1.05, macro f1=0.457]\ntrain [epoch 6 batch 60]: : 1440it [00:27, 51.68it/s, mean train loss=1.08]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 112.89it/s, mean loss=0.891, macro f1=0.584]\n[I 2024-10-19 21:14:14,601] Trial 5 pruned. \n\n\nTrial 6 params: {'batch_size': 32, 'lr': 0.0006087970645475956, 'weight_decay': 0.007392635793983017, 'reduce_lr_factor': 0.01352690130288886, 'momentum': 0.8537333228895179, 'early_stop_patience': 1, 'jitter': True, 'horizontal_flip': False, 'rotation': False}\nTrial 6 fold 1\n\n\ntrain [epoch 1 batch 45]: : 1440it [00:24, 57.67it/s, mean train loss=1.67]\nvalidate [batch 12]: : 384it [00:03, 127.57it/s, mean loss=1.18, macro f1=0.371]\ntrain [epoch 2 batch 45]: : 1440it [00:25, 57.05it/s, mean train loss=0.899]\nvalidate [batch 12]: : 384it [00:03, 126.63it/s, mean loss=0.607, macro f1=0.694]\ntrain [epoch 3 batch 45]: : 1440it [00:25, 57.58it/s, mean train loss=0.518]\nvalidate [batch 12]: : 384it [00:02, 128.15it/s, mean loss=0.456, macro f1=0.762]\ntrain [epoch 4 batch 45]: : 1440it [00:25, 57.33it/s, mean train loss=0.395]\nvalidate [batch 12]: : 384it [00:03, 126.34it/s, mean loss=0.346, macro f1=0.812]\ntrain [epoch 5 batch 45]: : 1440it [00:25, 57.50it/s, mean train loss=0.268]\nvalidate [batch 12]: : 384it [00:03, 127.33it/s, mean loss=0.318, macro f1=0.846]\ntrain [epoch 6 batch 45]: : 1440it [00:25, 57.29it/s, mean train loss=0.193]\nvalidate [batch 12]: : 384it [00:03, 125.01it/s, mean loss=0.298, macro f1=0.844]\n[I 2024-10-19 21:17:07,417] Trial 6 pruned. \n\n\nTrial 7 params: {'batch_size': 32, 'lr': 0.00027273559603005097, 'weight_decay': 0.005232480534666997, 'reduce_lr_factor': 0.018454645968259752, 'momentum': 0.9094298341556741, 'early_stop_patience': 3, 'jitter': False, 'horizontal_flip': False, 'rotation': True}\nTrial 7 fold 1\n\n\ntrain [epoch 1 batch 45]: : 1440it [00:24, 58.91it/s, mean train loss=1.9]\nvalidate [batch 12]: : 384it [00:03, 127.55it/s, mean loss=1.37, macro f1=0.182]\ntrain [epoch 2 batch 45]: : 1440it [00:24, 59.18it/s, mean train loss=1.2]\nvalidate [batch 12]: : 384it [00:03, 126.79it/s, mean loss=0.796, macro f1=0.603]\ntrain [epoch 3 batch 45]: : 1440it [00:24, 58.39it/s, mean train loss=0.794]\nvalidate [batch 12]: : 384it [00:03, 126.50it/s, mean loss=0.597, macro f1=0.665]\ntrain [epoch 4 batch 45]: : 1440it [00:24, 59.03it/s, mean train loss=0.612]\nvalidate [batch 12]: : 384it [00:03, 127.63it/s, mean loss=0.533, macro f1=0.727]\ntrain [epoch 5 batch 45]: : 1440it [00:24, 58.59it/s, mean train loss=0.449]\nvalidate [batch 12]: : 384it [00:03, 124.87it/s, mean loss=0.403, macro f1=0.808]\ntrain [epoch 6 batch 45]: : 1440it [00:24, 59.03it/s, mean train loss=0.375]\nvalidate [batch 12]: : 384it [00:03, 127.36it/s, mean loss=0.344, macro f1=0.794]\n[I 2024-10-19 21:19:56,656] Trial 7 pruned. \n\n\nTrial 8 params: {'batch_size': 32, 'lr': 2.9906470725618615e-05, 'weight_decay': 0.008289400292173631, 'reduce_lr_factor': 0.010422592857329237, 'momentum': 0.9287851419912837, 'early_stop_patience': 1, 'jitter': False, 'horizontal_flip': False, 'rotation': True}\nTrial 8 fold 1\n\n\ntrain [epoch 1 batch 45]: : 1440it [00:24, 58.88it/s, mean train loss=2.15]\nvalidate [batch 12]: : 384it [00:03, 127.36it/s, mean loss=1.93, macro f1=0.148]\ntrain [epoch 2 batch 45]: : 1440it [00:24, 58.44it/s, mean train loss=1.84]\nvalidate [batch 12]: : 384it [00:03, 118.95it/s, mean loss=1.68, macro f1=0.173]\ntrain [epoch 3 batch 45]: : 1440it [00:24, 59.31it/s, mean train loss=1.65]\nvalidate [batch 12]: : 384it [00:03, 126.62it/s, mean loss=1.49, macro f1=0.179]\ntrain [epoch 4 batch 45]: : 1440it [00:24, 58.74it/s, mean train loss=1.46]\nvalidate [batch 12]: : 384it [00:03, 127.47it/s, mean loss=1.36, macro f1=0.182]\ntrain [epoch 5 batch 45]: : 1440it [00:24, 58.99it/s, mean train loss=1.36]\nvalidate [batch 12]: : 384it [00:03, 127.77it/s, mean loss=1.23, macro f1=0.188]\ntrain [epoch 6 batch 45]: : 1440it [00:24, 58.89it/s, mean train loss=1.29]\nvalidate [batch 12]: : 384it [00:03, 125.03it/s, mean loss=1.15, macro f1=0.239]\n[I 2024-10-19 21:22:45,935] Trial 8 pruned. \n\n\nTrial 9 params: {'batch_size': 24, 'lr': 0.0009532215214018151, 'weight_decay': 0.004471253786176274, 'reduce_lr_factor': 0.0861767805224015, 'momentum': 0.9329010623103258, 'early_stop_patience': 1, 'jitter': True, 'horizontal_flip': True, 'rotation': True}\nTrial 9 fold 1\n\n\ntrain [epoch 1 batch 60]: : 1440it [00:27, 51.82it/s, mean train loss=1.45]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.38it/s, mean loss=0.697, macro f1=0.65]\ntrain [epoch 2 batch 60]: : 1440it [00:27, 51.85it/s, mean train loss=0.714]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 109.31it/s, mean loss=0.493, macro f1=0.772]\ntrain [epoch 3 batch 60]: : 1440it [00:27, 51.75it/s, mean train loss=0.45]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 112.85it/s, mean loss=0.313, macro f1=0.835]\ntrain [epoch 4 batch 60]: : 1440it [00:27, 51.73it/s, mean train loss=0.317]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 112.24it/s, mean loss=0.297, macro f1=0.859]\ntrain [epoch 5 batch 60]: : 1440it [00:28, 51.42it/s, mean train loss=0.216]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 112.08it/s, mean loss=0.324, macro f1=0.845]\n\n\nEarly stopping at epoch 5\n\n\ntest [batch 38]: : 912it [00:07, 121.50it/s, mean loss=0.25, macro f1=0.876]\n\n\nTrial 9 fold 2\n\n\ntrain [epoch 1 batch 60]: : 1440it [00:27, 51.85it/s, mean train loss=1.4]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 111.74it/s, mean loss=0.947, macro f1=0.615]\ntrain [epoch 2 batch 60]: : 1440it [00:28, 51.42it/s, mean train loss=0.806]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 112.31it/s, mean loss=0.511, macro f1=0.705]\ntrain [epoch 3 batch 60]: : 1440it [00:27, 51.77it/s, mean train loss=0.435]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 112.96it/s, mean loss=0.312, macro f1=0.847]\ntrain [epoch 4 batch 60]: : 1440it [00:28, 51.28it/s, mean train loss=0.354]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 112.51it/s, mean loss=0.251, macro f1=0.869]\ntrain [epoch 5 batch 60]: : 1440it [00:27, 51.60it/s, mean train loss=0.272]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 112.78it/s, mean loss=0.253, macro f1=0.893]\n\n\nEarly stopping at epoch 5\n\n\ntest [batch 38]: : 912it [00:07, 118.57it/s, mean loss=0.267, macro f1=0.873]\n\n\nTrial 9 fold 3\n\n\ntrain [epoch 1 batch 60]: : 1440it [00:28, 51.36it/s, mean train loss=1.27]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.18it/s, mean loss=0.64, macro f1=0.662]\ntrain [epoch 2 batch 60]: : 1440it [00:27, 51.75it/s, mean train loss=0.615]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 112.82it/s, mean loss=0.376, macro f1=0.807]\ntrain [epoch 3 batch 60]: : 1440it [00:28, 51.41it/s, mean train loss=0.331]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.02it/s, mean loss=0.285, macro f1=0.845]\ntrain [epoch 4 batch 60]: : 1440it [00:27, 51.72it/s, mean train loss=0.217]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 111.86it/s, mean loss=0.352, macro f1=0.857]\n\n\nEarly stopping at epoch 4\n\n\ntest [batch 38]: : 912it [00:07, 120.99it/s, mean loss=0.236, macro f1=0.911]\n[I 2024-10-19 21:30:35,179] Trial 9 finished with value: 0.8864634164154767 and parameters: {'batch_size': 24, 'lr': 0.0009532215214018151, 'weight_decay': 0.004471253786176274, 'reduce_lr_factor': 0.0861767805224015, 'momentum': 0.9329010623103258, 'early_stop_patience': 1, 'jitter': True, 'horizontal_flip': True, 'rotation': True}. Best is trial 2 with value: 0.8944816106117478.\n\n\nTrial 10 params: {'batch_size': 24, 'lr': 0.00041926610680704515, 'weight_decay': 0.00020768834766933357, 'reduce_lr_factor': 0.09366763195250474, 'momentum': 0.9731649469005796, 'early_stop_patience': 2, 'jitter': False, 'horizontal_flip': False, 'rotation': False}\nTrial 10 fold 1\n\n\ntrain [epoch 1 batch 60]: : 1440it [00:27, 53.19it/s, mean train loss=1.33]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.15it/s, mean loss=0.698, macro f1=0.68]\ntrain [epoch 2 batch 60]: : 1440it [00:26, 53.58it/s, mean train loss=0.482]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 112.03it/s, mean loss=0.452, macro f1=0.778]\ntrain [epoch 3 batch 60]: : 1440it [00:27, 53.29it/s, mean train loss=0.313]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 112.12it/s, mean loss=0.499, macro f1=0.791]\ntrain [epoch 4 batch 60]: : 1440it [00:26, 53.64it/s, mean train loss=0.256]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 112.06it/s, mean loss=0.279, macro f1=0.851]\ntrain [epoch 5 batch 60]: : 1440it [00:27, 53.13it/s, mean train loss=0.133]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 112.96it/s, mean loss=0.316, macro f1=0.842]\ntrain [epoch 6 batch 60]: : 1440it [00:26, 53.61it/s, mean train loss=0.102]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 114.01it/s, mean loss=0.291, macro f1=0.874]\n\n\nEarly stopping at epoch 6\n\n\ntest [batch 38]: : 912it [00:07, 120.98it/s, mean loss=0.24, macro f1=0.893]\n\n\nTrial 10 fold 2\n\n\ntrain [epoch 1 batch 60]: : 1440it [00:27, 53.08it/s, mean train loss=1.41]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 112.18it/s, mean loss=0.652, macro f1=0.703]\ntrain [epoch 2 batch 60]: : 1440it [00:26, 53.49it/s, mean train loss=0.534]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 112.75it/s, mean loss=0.333, macro f1=0.825]\ntrain [epoch 3 batch 60]: : 1440it [00:27, 53.21it/s, mean train loss=0.28]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.18it/s, mean loss=0.252, macro f1=0.913]\ntrain [epoch 4 batch 60]: : 1440it [00:26, 53.57it/s, mean train loss=0.145]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.08it/s, mean loss=0.187, macro f1=0.918]\ntrain [epoch 5 batch 60]: : 1440it [00:26, 53.46it/s, mean train loss=0.12]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 103.63it/s, mean loss=0.249, macro f1=0.905]\ntrain [epoch 6 batch 60]: : 1440it [00:27, 53.26it/s, mean train loss=0.0894]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.29it/s, mean loss=0.287, macro f1=0.878]\n\n\nEarly stopping at epoch 6\n\n\ntest [batch 38]: : 912it [00:07, 121.04it/s, mean loss=0.282, macro f1=0.9]\n\n\nTrial 10 fold 3\n\n\ntrain [epoch 1 batch 60]: : 1440it [00:26, 53.49it/s, mean train loss=1.36]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 109.36it/s, mean loss=0.725, macro f1=0.657]\ntrain [epoch 2 batch 60]: : 1440it [00:26, 53.38it/s, mean train loss=0.505]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.76it/s, mean loss=0.435, macro f1=0.768]\ntrain [epoch 3 batch 60]: : 1440it [00:26, 53.51it/s, mean train loss=0.312]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.48it/s, mean loss=0.482, macro f1=0.776]\ntrain [epoch 4 batch 60]: : 1440it [00:27, 52.99it/s, mean train loss=0.223]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 111.53it/s, mean loss=0.27, macro f1=0.86]\ntrain [epoch 5 batch 60]: : 1440it [00:26, 53.49it/s, mean train loss=0.165]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.27it/s, mean loss=0.3, macro f1=0.854]\ntrain [epoch 6 batch 60]: : 1440it [00:27, 53.20it/s, mean train loss=0.106]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 114.08it/s, mean loss=0.236, macro f1=0.912]\ntrain [epoch 7 batch 60]: : 1440it [00:26, 53.75it/s, mean train loss=0.0983]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.58it/s, mean loss=0.332, macro f1=0.834]\ntrain [epoch 8 batch 60]: : 1440it [00:27, 53.23it/s, mean train loss=0.0587]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.06it/s, mean loss=0.229, macro f1=0.904]\ntrain [epoch 9 batch 60]: : 1440it [00:26, 53.48it/s, mean train loss=0.0529]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.51it/s, mean loss=0.299, macro f1=0.854]\ntrain [epoch 10 batch 60]: : 1440it [00:27, 53.24it/s, mean train loss=0.0761]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 105.10it/s, mean loss=0.258, macro f1=0.889]\n\n\nEarly stopping at epoch 10\n\n\ntest [batch 38]: : 912it [00:07, 121.06it/s, mean loss=0.266, macro f1=0.895]\n[I 2024-10-19 21:42:15,827] Trial 10 finished with value: 0.8959995960548577 and parameters: {'batch_size': 24, 'lr': 0.00041926610680704515, 'weight_decay': 0.00020768834766933357, 'reduce_lr_factor': 0.09366763195250474, 'momentum': 0.9731649469005796, 'early_stop_patience': 2, 'jitter': False, 'horizontal_flip': False, 'rotation': False}. Best is trial 10 with value: 0.8959995960548577.\n\n\nTrial 11 params: {'batch_size': 24, 'lr': 0.0004494679962846208, 'weight_decay': 0.00030386197288080987, 'reduce_lr_factor': 0.09992933757507363, 'momentum': 0.9858812103194864, 'early_stop_patience': 2, 'jitter': False, 'horizontal_flip': False, 'rotation': False}\nTrial 11 fold 1\n\n\ntrain [epoch 1 batch 60]: : 1440it [00:26, 53.56it/s, mean train loss=1.48]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.25it/s, mean loss=0.85, macro f1=0.554]\ntrain [epoch 2 batch 60]: : 1440it [00:27, 53.13it/s, mean train loss=0.679]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 112.21it/s, mean loss=0.509, macro f1=0.738]\ntrain [epoch 3 batch 60]: : 1440it [00:26, 53.73it/s, mean train loss=0.378]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.07it/s, mean loss=0.456, macro f1=0.748]\ntrain [epoch 4 batch 60]: : 1440it [00:26, 53.65it/s, mean train loss=0.272]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 105.53it/s, mean loss=0.507, macro f1=0.816]\ntrain [epoch 5 batch 60]: : 1440it [00:26, 53.41it/s, mean train loss=0.277]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.49it/s, mean loss=0.368, macro f1=0.859]\ntrain [epoch 6 batch 60]: : 1440it [00:26, 53.65it/s, mean train loss=0.209]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.81it/s, mean loss=0.435, macro f1=0.83]\ntrain [epoch 7 batch 60]: : 1440it [00:27, 53.19it/s, mean train loss=0.153]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.27it/s, mean loss=0.229, macro f1=0.904]\ntrain [epoch 8 batch 60]: : 1440it [00:26, 53.83it/s, mean train loss=0.0816]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 114.07it/s, mean loss=0.445, macro f1=0.86]\ntrain [epoch 9 batch 60]: : 1440it [00:27, 53.30it/s, mean train loss=0.0898]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.63it/s, mean loss=0.4, macro f1=0.84]\n[I 2024-10-19 21:46:51,882] Trial 11 pruned. \n\n\nTrial 12 params: {'batch_size': 24, 'lr': 0.0007418366656250753, 'weight_decay': 0.0023652436805426938, 'reduce_lr_factor': 0.09810883725002961, 'momentum': 0.9482279084641699, 'early_stop_patience': 3, 'jitter': False, 'horizontal_flip': False, 'rotation': False}\nTrial 12 fold 1\n\n\ntrain [epoch 1 batch 60]: : 1440it [00:26, 53.69it/s, mean train loss=1.3]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 112.68it/s, mean loss=0.777, macro f1=0.664]\ntrain [epoch 2 batch 60]: : 1440it [00:27, 53.06it/s, mean train loss=0.474]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.52it/s, mean loss=0.46, macro f1=0.784]\ntrain [epoch 3 batch 60]: : 1440it [00:26, 53.57it/s, mean train loss=0.371]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 112.73it/s, mean loss=0.352, macro f1=0.818]\ntrain [epoch 4 batch 60]: : 1440it [00:26, 53.45it/s, mean train loss=0.238]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 103.10it/s, mean loss=0.247, macro f1=0.861]\ntrain [epoch 5 batch 60]: : 1440it [00:27, 53.32it/s, mean train loss=0.195]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 110.55it/s, mean loss=0.341, macro f1=0.872]\ntrain [epoch 6 batch 60]: : 1440it [00:26, 53.60it/s, mean train loss=0.112]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 106.21it/s, mean loss=0.357, macro f1=0.826]\ntrain [epoch 7 batch 60]: : 1440it [00:27, 53.07it/s, mean train loss=0.108]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 110.31it/s, mean loss=0.293, macro f1=0.869]\n\n\nEarly stopping at epoch 7\n\n\ntest [batch 38]: : 912it [00:07, 121.49it/s, mean loss=0.198, macro f1=0.914]\n\n\nTrial 12 fold 2\n\n\ntrain [epoch 1 batch 60]: : 1440it [00:26, 53.67it/s, mean train loss=1.39]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.13it/s, mean loss=0.715, macro f1=0.524]\ntrain [epoch 2 batch 60]: : 1440it [00:27, 53.21it/s, mean train loss=0.532]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.98it/s, mean loss=0.346, macro f1=0.818]\ntrain [epoch 3 batch 60]: : 1440it [00:26, 53.69it/s, mean train loss=0.311]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.53it/s, mean loss=0.342, macro f1=0.852]\ntrain [epoch 4 batch 60]: : 1440it [00:26, 53.37it/s, mean train loss=0.199]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.62it/s, mean loss=0.314, macro f1=0.869]\ntrain [epoch 5 batch 60]: : 1440it [00:26, 53.68it/s, mean train loss=0.162]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.10it/s, mean loss=0.308, macro f1=0.893]\ntrain [epoch 6 batch 60]: : 1440it [00:27, 53.06it/s, mean train loss=0.102]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.25it/s, mean loss=0.349, macro f1=0.853]\ntrain [epoch 7 batch 60]: : 1440it [00:26, 53.70it/s, mean train loss=0.115]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 108.52it/s, mean loss=0.219, macro f1=0.904]\ntrain [epoch 8 batch 60]: : 1440it [00:26, 53.36it/s, mean train loss=0.0609]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 103.65it/s, mean loss=0.294, macro f1=0.905]\ntrain [epoch 9 batch 60]: : 1440it [00:26, 53.59it/s, mean train loss=0.0488]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.37it/s, mean loss=0.212, macro f1=0.916]\ntrain [epoch 10 batch 60]: : 1440it [00:26, 53.63it/s, mean train loss=0.0274]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.34it/s, mean loss=0.266, macro f1=0.917]\ntrain [epoch 11 batch 60]: : 1440it [00:27, 53.04it/s, mean train loss=0.0496]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.75it/s, mean loss=0.193, macro f1=0.909]\ntrain [epoch 12 batch 60]: : 1440it [00:26, 53.56it/s, mean train loss=0.029]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.12it/s, mean loss=0.233, macro f1=0.911]\ntrain [epoch 13 batch 60]: : 1440it [00:27, 53.30it/s, mean train loss=0.0213]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.68it/s, mean loss=0.266, macro f1=0.919]\ntrain [epoch 14 batch 60]: : 1440it [00:26, 53.61it/s, mean train loss=0.0234]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.91it/s, mean loss=0.258, macro f1=0.916]\n\n\nEarly stopping at epoch 14\n\n\ntest [batch 38]: : 912it [00:07, 121.08it/s, mean loss=0.254, macro f1=0.898]\n\n\nTrial 12 fold 3\n\n\ntrain [epoch 1 batch 60]: : 1440it [00:26, 53.49it/s, mean train loss=1.5]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 109.48it/s, mean loss=0.97, macro f1=0.418]\ntrain [epoch 2 batch 60]: : 1440it [00:26, 53.83it/s, mean train loss=0.639]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.73it/s, mean loss=0.467, macro f1=0.762]\ntrain [epoch 3 batch 60]: : 1440it [00:26, 53.53it/s, mean train loss=0.256]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 114.00it/s, mean loss=0.309, macro f1=0.852]\ntrain [epoch 4 batch 60]: : 1440it [00:26, 53.65it/s, mean train loss=0.207]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 112.56it/s, mean loss=0.337, macro f1=0.839]\ntrain [epoch 5 batch 60]: : 1440it [00:26, 53.64it/s, mean train loss=0.169]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 104.44it/s, mean loss=0.32, macro f1=0.883]\ntrain [epoch 6 batch 60]: : 1440it [00:26, 53.47it/s, mean train loss=0.13]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.20it/s, mean loss=0.291, macro f1=0.869]\ntrain [epoch 7 batch 60]: : 1440it [00:26, 53.62it/s, mean train loss=0.0973]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.73it/s, mean loss=0.273, macro f1=0.887]\ntrain [epoch 8 batch 60]: : 1440it [00:27, 53.15it/s, mean train loss=0.126]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.83it/s, mean loss=0.322, macro f1=0.868]\ntrain [epoch 9 batch 60]: : 1440it [00:26, 53.60it/s, mean train loss=0.0528]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.61it/s, mean loss=0.357, macro f1=0.888]\ntrain [epoch 10 batch 60]: : 1440it [00:27, 53.07it/s, mean train loss=0.0528]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.99it/s, mean loss=0.449, macro f1=0.835]\n\n\nEarly stopping at epoch 10\n\n\ntest [batch 38]: : 912it [00:07, 121.53it/s, mean loss=0.333, macro f1=0.868]\n[I 2024-10-19 22:03:05,590] Trial 12 finished with value: 0.8935955507453679 and parameters: {'batch_size': 24, 'lr': 0.0007418366656250753, 'weight_decay': 0.0023652436805426938, 'reduce_lr_factor': 0.09810883725002961, 'momentum': 0.9482279084641699, 'early_stop_patience': 3, 'jitter': False, 'horizontal_flip': False, 'rotation': False}. Best is trial 10 with value: 0.8959995960548577.\n\n\nTrial 13 params: {'batch_size': 24, 'lr': 0.00025703928328789605, 'weight_decay': 0.002615828504402075, 'reduce_lr_factor': 0.08147142907323117, 'momentum': 0.9585454689948866, 'early_stop_patience': 2, 'jitter': False, 'horizontal_flip': False, 'rotation': False}\nTrial 13 fold 1\n\n\ntrain [epoch 1 batch 60]: : 1440it [00:26, 53.69it/s, mean train loss=1.51]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.65it/s, mean loss=0.763, macro f1=0.62]\ntrain [epoch 2 batch 60]: : 1440it [00:26, 54.14it/s, mean train loss=0.587]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 114.16it/s, mean loss=0.523, macro f1=0.751]\ntrain [epoch 3 batch 60]: : 1440it [00:26, 54.38it/s, mean train loss=0.341]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 114.06it/s, mean loss=0.297, macro f1=0.843]\ntrain [epoch 4 batch 60]: : 1440it [00:27, 53.32it/s, mean train loss=0.175]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 108.96it/s, mean loss=0.284, macro f1=0.878]\ntrain [epoch 5 batch 60]: : 1440it [00:26, 53.49it/s, mean train loss=0.112]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.54it/s, mean loss=0.263, macro f1=0.888]\ntrain [epoch 6 batch 60]: : 1440it [00:26, 53.59it/s, mean train loss=0.113]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 112.17it/s, mean loss=0.221, macro f1=0.897]\ntrain [epoch 7 batch 60]: : 1440it [00:26, 53.45it/s, mean train loss=0.0782]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.59it/s, mean loss=0.217, macro f1=0.901]\ntrain [epoch 8 batch 60]: : 1440it [00:26, 53.57it/s, mean train loss=0.0506]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.19it/s, mean loss=0.229, macro f1=0.917]\ntrain [epoch 9 batch 60]: : 1440it [00:27, 53.13it/s, mean train loss=0.0375]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.96it/s, mean loss=0.247, macro f1=0.909]\n\n\nEarly stopping at epoch 9\n\n\ntest [batch 38]: : 912it [00:07, 121.69it/s, mean loss=0.215, macro f1=0.919]\n\n\nTrial 13 fold 2\n\n\ntrain [epoch 1 batch 60]: : 1440it [00:26, 54.50it/s, mean train loss=1.57]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 114.01it/s, mean loss=0.841, macro f1=0.598]\ntrain [epoch 2 batch 60]: : 1440it [00:27, 53.12it/s, mean train loss=0.528]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 114.46it/s, mean loss=0.4, macro f1=0.786]\ntrain [epoch 3 batch 60]: : 1440it [00:26, 53.73it/s, mean train loss=0.292]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 114.31it/s, mean loss=0.315, macro f1=0.863]\ntrain [epoch 4 batch 60]: : 1440it [00:27, 53.22it/s, mean train loss=0.202]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 114.71it/s, mean loss=0.231, macro f1=0.884]\ntrain [epoch 5 batch 60]: : 1440it [00:26, 53.64it/s, mean train loss=0.157]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 114.29it/s, mean loss=0.262, macro f1=0.887]\ntrain [epoch 6 batch 60]: : 1440it [00:26, 53.50it/s, mean train loss=0.11]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 106.45it/s, mean loss=0.255, macro f1=0.883]\n\n\nEarly stopping at epoch 6\n\n\ntest [batch 38]: : 912it [00:07, 120.55it/s, mean loss=0.255, macro f1=0.887]\n\n\nTrial 13 fold 3\n\n\ntrain [epoch 1 batch 60]: : 1440it [00:26, 53.83it/s, mean train loss=1.67]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.99it/s, mean loss=1.11, macro f1=0.298]\ntrain [epoch 2 batch 60]: : 1440it [00:26, 53.74it/s, mean train loss=0.715]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 111.79it/s, mean loss=0.588, macro f1=0.719]\ntrain [epoch 3 batch 60]: : 1440it [00:26, 53.70it/s, mean train loss=0.396]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 114.60it/s, mean loss=0.407, macro f1=0.816]\ntrain [epoch 4 batch 60]: : 1440it [00:26, 53.80it/s, mean train loss=0.264]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 114.36it/s, mean loss=0.326, macro f1=0.838]\ntrain [epoch 5 batch 60]: : 1440it [00:26, 53.85it/s, mean train loss=0.17]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 114.08it/s, mean loss=0.367, macro f1=0.805]\ntrain [epoch 6 batch 60]: : 1440it [00:26, 53.74it/s, mean train loss=0.174]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 114.22it/s, mean loss=0.246, macro f1=0.876]\ntrain [epoch 7 batch 60]: : 1440it [00:27, 53.13it/s, mean train loss=0.162]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 114.25it/s, mean loss=0.314, macro f1=0.854]\ntrain [epoch 8 batch 60]: : 1440it [00:26, 53.66it/s, mean train loss=0.0956]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.12it/s, mean loss=0.251, macro f1=0.895]\n\n\nEarly stopping at epoch 8\n\n\ntest [batch 38]: : 912it [00:07, 121.24it/s, mean loss=0.252, macro f1=0.884]\n[I 2024-10-19 22:15:12,265] Trial 13 finished with value: 0.8967164091113543 and parameters: {'batch_size': 24, 'lr': 0.00025703928328789605, 'weight_decay': 0.002615828504402075, 'reduce_lr_factor': 0.08147142907323117, 'momentum': 0.9585454689948866, 'early_stop_patience': 2, 'jitter': False, 'horizontal_flip': False, 'rotation': False}. Best is trial 13 with value: 0.8967164091113543.\n\n\nTrial 14 params: {'batch_size': 24, 'lr': 0.00024174775803100938, 'weight_decay': 0.0035305699906768183, 'reduce_lr_factor': 0.08054162794179773, 'momentum': 0.9597674671303823, 'early_stop_patience': 2, 'jitter': False, 'horizontal_flip': False, 'rotation': False}\nTrial 14 fold 1\n\n\ntrain [epoch 1 batch 60]: : 1440it [00:26, 53.52it/s, mean train loss=1.58]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 114.20it/s, mean loss=0.932, macro f1=0.486]\ntrain [epoch 2 batch 60]: : 1440it [00:26, 53.67it/s, mean train loss=0.652]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.34it/s, mean loss=0.531, macro f1=0.713]\ntrain [epoch 3 batch 60]: : 1440it [00:26, 53.58it/s, mean train loss=0.339]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 107.34it/s, mean loss=0.43, macro f1=0.77]\ntrain [epoch 4 batch 60]: : 1440it [00:26, 53.71it/s, mean train loss=0.253]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.48it/s, mean loss=0.423, macro f1=0.828]\ntrain [epoch 5 batch 60]: : 1440it [00:26, 53.57it/s, mean train loss=0.25]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 108.88it/s, mean loss=0.302, macro f1=0.873]\ntrain [epoch 6 batch 60]: : 1440it [00:27, 53.17it/s, mean train loss=0.153]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 114.28it/s, mean loss=0.319, macro f1=0.84]\ntrain [epoch 7 batch 60]: : 1440it [00:26, 53.63it/s, mean train loss=0.146]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 114.08it/s, mean loss=0.314, macro f1=0.868]\n\n\nEarly stopping at epoch 7\n\n\ntest [batch 38]: : 912it [00:07, 122.50it/s, mean loss=0.254, macro f1=0.885]\n\n\nTrial 14 fold 2\n\n\ntrain [epoch 1 batch 60]: : 1440it [00:27, 53.27it/s, mean train loss=1.5]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.79it/s, mean loss=0.843, macro f1=0.444]\ntrain [epoch 2 batch 60]: : 1440it [00:26, 53.67it/s, mean train loss=0.637]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.80it/s, mean loss=0.443, macro f1=0.787]\ntrain [epoch 3 batch 60]: : 1440it [00:26, 53.35it/s, mean train loss=0.349]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 114.15it/s, mean loss=0.317, macro f1=0.864]\ntrain [epoch 4 batch 60]: : 1440it [00:26, 53.80it/s, mean train loss=0.244]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 114.10it/s, mean loss=0.251, macro f1=0.877]\ntrain [epoch 5 batch 60]: : 1440it [00:27, 52.93it/s, mean train loss=0.173]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 114.22it/s, mean loss=0.232, macro f1=0.881]\n[I 2024-10-19 22:21:29,022] Trial 14 pruned. \n\n\nTrial 15 params: {'batch_size': 24, 'lr': 0.00030194290448022913, 'weight_decay': 0.0028312494041207183, 'reduce_lr_factor': 0.06953388696278257, 'momentum': 0.9615861031772759, 'early_stop_patience': 2, 'jitter': False, 'horizontal_flip': False, 'rotation': False}\nTrial 15 fold 1\n\n\ntrain [epoch 1 batch 60]: : 1440it [00:26, 54.52it/s, mean train loss=1.57]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 116.28it/s, mean loss=1.05, macro f1=0.356]\ntrain [epoch 2 batch 60]: : 1440it [00:26, 54.24it/s, mean train loss=0.713]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 111.04it/s, mean loss=0.554, macro f1=0.701]\ntrain [epoch 3 batch 60]: : 1440it [00:26, 53.50it/s, mean train loss=0.382]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.71it/s, mean loss=0.388, macro f1=0.826]\ntrain [epoch 4 batch 60]: : 1440it [00:26, 53.61it/s, mean train loss=0.254]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.33it/s, mean loss=0.313, macro f1=0.863]\ntrain [epoch 5 batch 60]: : 1440it [00:27, 53.02it/s, mean train loss=0.13]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 112.40it/s, mean loss=0.292, macro f1=0.873]\ntrain [epoch 6 batch 60]: : 1440it [00:26, 53.63it/s, mean train loss=0.0776]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 112.51it/s, mean loss=0.401, macro f1=0.844]\ntrain [epoch 7 batch 60]: : 1440it [00:27, 53.10it/s, mean train loss=0.0907]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 112.79it/s, mean loss=0.306, macro f1=0.876]\n\n\nEarly stopping at epoch 7\n\n\ntest [batch 38]: : 912it [00:07, 122.54it/s, mean loss=0.284, macro f1=0.878]\n\n\nTrial 15 fold 2\n\n\ntrain [epoch 1 batch 60]: : 1440it [00:26, 53.82it/s, mean train loss=1.45]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.51it/s, mean loss=0.708, macro f1=0.638]\ntrain [epoch 2 batch 60]: : 1440it [00:27, 53.09it/s, mean train loss=0.588]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.81it/s, mean loss=0.395, macro f1=0.798]\ntrain [epoch 3 batch 60]: : 1440it [00:26, 53.73it/s, mean train loss=0.301]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.80it/s, mean loss=0.287, macro f1=0.867]\ntrain [epoch 4 batch 60]: : 1440it [00:26, 53.60it/s, mean train loss=0.157]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 106.94it/s, mean loss=0.249, macro f1=0.888]\ntrain [epoch 5 batch 60]: : 1440it [00:26, 53.53it/s, mean train loss=0.124]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.87it/s, mean loss=0.232, macro f1=0.9]\ntrain [epoch 6 batch 60]: : 1440it [00:26, 53.60it/s, mean train loss=0.103]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.82it/s, mean loss=0.262, macro f1=0.902]\ntrain [epoch 7 batch 60]: : 1440it [00:27, 53.16it/s, mean train loss=0.0531]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.96it/s, mean loss=0.296, macro f1=0.893]\n\n\nEarly stopping at epoch 7\n\n\ntest [batch 38]: : 912it [00:07, 121.91it/s, mean loss=0.294, macro f1=0.894]\n\n\nTrial 15 fold 3\n\n\ntrain [epoch 1 batch 60]: : 1440it [00:26, 53.57it/s, mean train loss=1.51]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.51it/s, mean loss=0.897, macro f1=0.538]\ntrain [epoch 2 batch 60]: : 1440it [00:27, 53.16it/s, mean train loss=0.554]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 114.02it/s, mean loss=0.559, macro f1=0.77]\ntrain [epoch 3 batch 60]: : 1440it [00:26, 53.72it/s, mean train loss=0.363]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.89it/s, mean loss=0.392, macro f1=0.789]\ntrain [epoch 4 batch 60]: : 1440it [00:27, 53.09it/s, mean train loss=0.271]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.97it/s, mean loss=0.374, macro f1=0.844]\ntrain [epoch 5 batch 60]: : 1440it [00:26, 53.66it/s, mean train loss=0.198]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.92it/s, mean loss=0.28, macro f1=0.833]\ntrain [epoch 6 batch 60]: : 1440it [00:27, 53.28it/s, mean train loss=0.123]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 106.14it/s, mean loss=0.237, macro f1=0.886]\ntrain [epoch 7 batch 60]: : 1440it [00:26, 53.52it/s, mean train loss=0.116]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.48it/s, mean loss=0.328, macro f1=0.868]\ntrain [epoch 8 batch 60]: : 1440it [00:26, 53.50it/s, mean train loss=0.0568]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 110.28it/s, mean loss=0.242, macro f1=0.893]\n\n\nEarly stopping at epoch 8\n\n\ntest [batch 38]: : 912it [00:08, 110.28it/s, mean loss=0.246, macro f1=0.9]\n[I 2024-10-19 22:33:07,675] Trial 15 finished with value: 0.8909237380561086 and parameters: {'batch_size': 24, 'lr': 0.00030194290448022913, 'weight_decay': 0.0028312494041207183, 'reduce_lr_factor': 0.06953388696278257, 'momentum': 0.9615861031772759, 'early_stop_patience': 2, 'jitter': False, 'horizontal_flip': False, 'rotation': False}. Best is trial 13 with value: 0.8967164091113543.\n\n\nTrial 16 params: {'batch_size': 24, 'lr': 0.0004819093288876784, 'weight_decay': 0.0016937287816857739, 'reduce_lr_factor': 0.04514422541734965, 'momentum': 0.9669468784630878, 'early_stop_patience': 3, 'jitter': False, 'horizontal_flip': False, 'rotation': False}\nTrial 16 fold 1\n\n\ntrain [epoch 1 batch 60]: : 1440it [00:26, 53.66it/s, mean train loss=1.26]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.84it/s, mean loss=0.724, macro f1=0.715]\ntrain [epoch 2 batch 60]: : 1440it [00:26, 53.70it/s, mean train loss=0.458]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.53it/s, mean loss=0.421, macro f1=0.805]\ntrain [epoch 3 batch 60]: : 1440it [00:27, 53.13it/s, mean train loss=0.247]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.76it/s, mean loss=0.377, macro f1=0.829]\ntrain [epoch 4 batch 60]: : 1440it [00:26, 53.79it/s, mean train loss=0.218]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.15it/s, mean loss=0.296, macro f1=0.826]\ntrain [epoch 5 batch 60]: : 1440it [00:27, 53.10it/s, mean train loss=0.116]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.74it/s, mean loss=0.334, macro f1=0.871]\ntrain [epoch 6 batch 60]: : 1440it [00:26, 53.60it/s, mean train loss=0.0912]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.65it/s, mean loss=0.241, macro f1=0.907]\ntrain [epoch 7 batch 60]: : 1440it [00:27, 53.11it/s, mean train loss=0.0676]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 110.20it/s, mean loss=0.215, macro f1=0.925]\ntrain [epoch 8 batch 60]: : 1440it [00:26, 53.63it/s, mean train loss=0.0709]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 112.95it/s, mean loss=0.273, macro f1=0.911]\ntrain [epoch 9 batch 60]: : 1440it [00:26, 53.71it/s, mean train loss=0.0472]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 108.82it/s, mean loss=0.264, macro f1=0.892]\ntrain [epoch 10 batch 60]: : 1440it [00:27, 53.25it/s, mean train loss=0.0275]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 112.75it/s, mean loss=0.235, macro f1=0.914]\n\n\nEarly stopping at epoch 10\n\n\ntest [batch 38]: : 912it [00:07, 122.33it/s, mean loss=0.2, macro f1=0.929]\n\n\nTrial 16 fold 2\n\n\ntrain [epoch 1 batch 60]: : 1440it [00:26, 53.77it/s, mean train loss=1.35]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 108.04it/s, mean loss=0.636, macro f1=0.719]\ntrain [epoch 2 batch 60]: : 1440it [00:27, 53.06it/s, mean train loss=0.466]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.40it/s, mean loss=0.333, macro f1=0.85]\ntrain [epoch 3 batch 60]: : 1440it [00:26, 53.64it/s, mean train loss=0.224]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.48it/s, mean loss=0.238, macro f1=0.894]\ntrain [epoch 4 batch 60]: : 1440it [00:27, 53.02it/s, mean train loss=0.151]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 114.07it/s, mean loss=0.278, macro f1=0.877]\ntrain [epoch 5 batch 60]: : 1440it [00:26, 53.68it/s, mean train loss=0.0919]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.54it/s, mean loss=0.224, macro f1=0.923]\ntrain [epoch 6 batch 60]: : 1440it [00:27, 52.97it/s, mean train loss=0.0647]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.93it/s, mean loss=0.247, macro f1=0.914]\ntrain [epoch 7 batch 60]: : 1440it [00:26, 53.63it/s, mean train loss=0.0508]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.67it/s, mean loss=0.257, macro f1=0.924]\ntrain [epoch 8 batch 60]: : 1440it [00:26, 53.51it/s, mean train loss=0.0339]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 110.29it/s, mean loss=0.391, macro f1=0.882]\n\n\nEarly stopping at epoch 8\n\n\ntest [batch 38]: : 912it [00:07, 117.56it/s, mean loss=0.331, macro f1=0.903]\n\n\nTrial 16 fold 3\n\n\ntrain [epoch 1 batch 60]: : 1440it [00:26, 53.52it/s, mean train loss=1.41]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.92it/s, mean loss=0.778, macro f1=0.657]\ntrain [epoch 2 batch 60]: : 1440it [00:26, 53.57it/s, mean train loss=0.554]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 110.46it/s, mean loss=0.426, macro f1=0.818]\ntrain [epoch 3 batch 60]: : 1440it [00:26, 53.36it/s, mean train loss=0.302]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.53it/s, mean loss=0.368, macro f1=0.809]\ntrain [epoch 4 batch 60]: : 1440it [00:26, 53.58it/s, mean train loss=0.231]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.89it/s, mean loss=0.383, macro f1=0.819]\ntrain [epoch 5 batch 60]: : 1440it [00:27, 53.10it/s, mean train loss=0.141]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.45it/s, mean loss=0.253, macro f1=0.87]\ntrain [epoch 6 batch 60]: : 1440it [00:26, 53.55it/s, mean train loss=0.124]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.39it/s, mean loss=0.241, macro f1=0.871]\ntrain [epoch 7 batch 60]: : 1440it [00:27, 53.19it/s, mean train loss=0.0649]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 111.22it/s, mean loss=0.258, macro f1=0.901]\ntrain [epoch 8 batch 60]: : 1440it [00:26, 53.54it/s, mean train loss=0.0685]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.85it/s, mean loss=0.211, macro f1=0.893]\ntrain [epoch 9 batch 60]: : 1440it [00:26, 53.61it/s, mean train loss=0.0727]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 111.26it/s, mean loss=0.241, macro f1=0.906]\ntrain [epoch 10 batch 60]: : 1440it [00:27, 53.15it/s, mean train loss=0.0456]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.72it/s, mean loss=0.207, macro f1=0.924]\ntrain [epoch 11 batch 60]: : 1440it [00:26, 53.55it/s, mean train loss=0.0389]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.69it/s, mean loss=0.295, macro f1=0.909]\ntrain [epoch 12 batch 60]: : 1440it [00:27, 53.26it/s, mean train loss=0.0496]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.39it/s, mean loss=0.302, macro f1=0.862]\ntrain [epoch 13 batch 60]: : 1440it [00:26, 53.60it/s, mean train loss=0.0379]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 112.54it/s, mean loss=0.244, macro f1=0.913]\n\n\nEarly stopping at epoch 13\n\n\ntest [batch 38]: : 912it [00:07, 122.34it/s, mean loss=0.239, macro f1=0.917]\n[I 2024-10-19 22:49:20,884] Trial 16 finished with value: 0.9163844390183554 and parameters: {'batch_size': 24, 'lr': 0.0004819093288876784, 'weight_decay': 0.0016937287816857739, 'reduce_lr_factor': 0.04514422541734965, 'momentum': 0.9669468784630878, 'early_stop_patience': 3, 'jitter': False, 'horizontal_flip': False, 'rotation': False}. Best is trial 16 with value: 0.9163844390183554.\n\n\nTrial 17 params: {'batch_size': 24, 'lr': 0.0001668437153729059, 'weight_decay': 0.0020084306972898095, 'reduce_lr_factor': 0.040683683734386994, 'momentum': 0.9236127660399246, 'early_stop_patience': 3, 'jitter': False, 'horizontal_flip': False, 'rotation': False}\nTrial 17 fold 1\n\n\ntrain [epoch 1 batch 60]: : 1440it [00:27, 53.13it/s, mean train loss=1.82]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.60it/s, mean loss=1.34, macro f1=0.177]\ntrain [epoch 2 batch 60]: : 1440it [00:26, 54.48it/s, mean train loss=1.06]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 112.23it/s, mean loss=0.815, macro f1=0.598]\ntrain [epoch 3 batch 60]: : 1440it [00:26, 54.27it/s, mean train loss=0.664]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 105.24it/s, mean loss=0.593, macro f1=0.752]\ntrain [epoch 4 batch 60]: : 1440it [00:26, 54.36it/s, mean train loss=0.503]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.35it/s, mean loss=0.555, macro f1=0.769]\ntrain [epoch 5 batch 60]: : 1440it [00:26, 54.34it/s, mean train loss=0.435]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.95it/s, mean loss=0.45, macro f1=0.803]\ntrain [epoch 6 batch 60]: : 1440it [00:27, 53.15it/s, mean train loss=0.335]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.46it/s, mean loss=0.467, macro f1=0.741]\n[I 2024-10-19 22:52:24,414] Trial 17 pruned. \n\n\nTrial 18 params: {'batch_size': 24, 'lr': 0.0005649250961480911, 'weight_decay': 0.0038202728999250303, 'reduce_lr_factor': 0.031075387954819105, 'momentum': 0.9451414712739943, 'early_stop_patience': 3, 'jitter': False, 'horizontal_flip': False, 'rotation': False}\nTrial 18 fold 1\n\n\ntrain [epoch 1 batch 60]: : 1440it [00:26, 54.32it/s, mean train loss=1.33]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 112.72it/s, mean loss=0.736, macro f1=0.618]\ntrain [epoch 2 batch 60]: : 1440it [00:26, 53.78it/s, mean train loss=0.463]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 114.01it/s, mean loss=0.456, macro f1=0.804]\ntrain [epoch 3 batch 60]: : 1440it [00:26, 53.66it/s, mean train loss=0.221]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.70it/s, mean loss=0.428, macro f1=0.79]\ntrain [epoch 4 batch 60]: : 1440it [00:26, 54.17it/s, mean train loss=0.24]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 104.79it/s, mean loss=0.373, macro f1=0.845]\ntrain [epoch 5 batch 60]: : 1440it [00:26, 54.19it/s, mean train loss=0.133]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 114.35it/s, mean loss=0.285, macro f1=0.833]\ntrain [epoch 6 batch 60]: : 1440it [00:26, 53.72it/s, mean train loss=0.108]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 110.13it/s, mean loss=0.3, macro f1=0.831]\n[I 2024-10-19 22:55:27,942] Trial 18 pruned. \n\n\nTrial 19 params: {'batch_size': 24, 'lr': 0.0004961347267509213, 'weight_decay': 0.0014024498900143132, 'reduce_lr_factor': 0.059636330343116425, 'momentum': 0.9676079987515462, 'early_stop_patience': 3, 'jitter': False, 'horizontal_flip': False, 'rotation': False}\nTrial 19 fold 1\n\n\ntrain [epoch 1 batch 60]: : 1440it [00:27, 52.77it/s, mean train loss=1.26]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.24it/s, mean loss=0.589, macro f1=0.717]\ntrain [epoch 2 batch 60]: : 1440it [00:27, 53.30it/s, mean train loss=0.368]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 111.14it/s, mean loss=0.381, macro f1=0.795]\ntrain [epoch 3 batch 60]: : 1440it [00:27, 52.99it/s, mean train loss=0.256]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 111.01it/s, mean loss=0.441, macro f1=0.828]\ntrain [epoch 4 batch 60]: : 1440it [00:26, 53.54it/s, mean train loss=0.133]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 106.53it/s, mean loss=0.291, macro f1=0.881]\ntrain [epoch 5 batch 60]: : 1440it [00:27, 53.24it/s, mean train loss=0.0917]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 104.57it/s, mean loss=0.246, macro f1=0.902]\ntrain [epoch 6 batch 60]: : 1440it [00:26, 53.48it/s, mean train loss=0.0766]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 112.71it/s, mean loss=0.249, macro f1=0.898]\ntrain [epoch 7 batch 60]: : 1440it [00:26, 53.58it/s, mean train loss=0.0468]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 112.10it/s, mean loss=0.331, macro f1=0.901]\ntrain [epoch 8 batch 60]: : 1440it [00:27, 53.05it/s, mean train loss=0.0608]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.42it/s, mean loss=0.267, macro f1=0.879]\n[I 2024-10-19 22:59:35,095] Trial 19 pruned. \n\n\nTrial 20 params: {'batch_size': 24, 'lr': 0.00035411316315068196, 'weight_decay': 0.0033040227384895787, 'reduce_lr_factor': 0.04748691266855614, 'momentum': 0.9131494575049925, 'early_stop_patience': 3, 'jitter': False, 'horizontal_flip': False, 'rotation': False}\nTrial 20 fold 1\n\n\ntrain [epoch 1 batch 60]: : 1440it [00:26, 53.66it/s, mean train loss=1.67]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.19it/s, mean loss=1.09, macro f1=0.343]\ntrain [epoch 2 batch 60]: : 1440it [00:27, 52.98it/s, mean train loss=0.858]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.60it/s, mean loss=0.65, macro f1=0.641]\ntrain [epoch 3 batch 60]: : 1440it [00:26, 53.60it/s, mean train loss=0.505]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.26it/s, mean loss=0.593, macro f1=0.607]\ntrain [epoch 4 batch 60]: : 1440it [00:27, 53.22it/s, mean train loss=0.338]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 105.05it/s, mean loss=0.336, macro f1=0.825]\ntrain [epoch 5 batch 60]: : 1440it [00:26, 53.62it/s, mean train loss=0.223]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.45it/s, mean loss=0.337, macro f1=0.838]\ntrain [epoch 6 batch 60]: : 1440it [00:26, 53.70it/s, mean train loss=0.154]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.92it/s, mean loss=0.266, macro f1=0.857]\ntrain [epoch 7 batch 60]: : 1440it [00:27, 52.86it/s, mean train loss=0.159]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.02it/s, mean loss=0.31, macro f1=0.849]\n[I 2024-10-19 23:03:10,819] Trial 20 pruned. \n\n\nTrial 21 params: {'batch_size': 24, 'lr': 0.0004336370725904691, 'weight_decay': 0.0004066475847682252, 'reduce_lr_factor': 0.08463519964480161, 'momentum': 0.9762195658502444, 'early_stop_patience': 2, 'jitter': False, 'horizontal_flip': False, 'rotation': False}\nTrial 21 fold 1\n\n\ntrain [epoch 1 batch 60]: : 1440it [00:26, 53.71it/s, mean train loss=1.47]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.58it/s, mean loss=0.778, macro f1=0.548]\ntrain [epoch 2 batch 60]: : 1440it [00:26, 53.95it/s, mean train loss=0.527]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.50it/s, mean loss=0.379, macro f1=0.817]\ntrain [epoch 3 batch 60]: : 1440it [00:26, 53.77it/s, mean train loss=0.318]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.52it/s, mean loss=0.335, macro f1=0.815]\ntrain [epoch 4 batch 60]: : 1440it [00:26, 53.98it/s, mean train loss=0.181]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 107.95it/s, mean loss=0.262, macro f1=0.883]\ntrain [epoch 5 batch 60]: : 1440it [00:26, 54.37it/s, mean train loss=0.183]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 112.85it/s, mean loss=0.267, macro f1=0.863]\ntrain [epoch 6 batch 60]: : 1440it [00:26, 54.35it/s, mean train loss=0.108]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 112.51it/s, mean loss=0.28, macro f1=0.879]\n\n\nEarly stopping at epoch 6\n\n\ntest [batch 38]: : 912it [00:08, 113.54it/s, mean loss=0.226, macro f1=0.9]\n\n\nTrial 21 fold 2\n\n\ntrain [epoch 1 batch 60]: : 1440it [00:26, 54.06it/s, mean train loss=1.48]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.00it/s, mean loss=0.77, macro f1=0.606]\ntrain [epoch 2 batch 60]: : 1440it [00:26, 53.78it/s, mean train loss=0.552]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 115.15it/s, mean loss=0.414, macro f1=0.805]\ntrain [epoch 3 batch 60]: : 1440it [00:26, 53.38it/s, mean train loss=0.281]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 114.14it/s, mean loss=0.404, macro f1=0.847]\ntrain [epoch 4 batch 60]: : 1440it [00:26, 54.42it/s, mean train loss=0.264]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 112.69it/s, mean loss=0.366, macro f1=0.862]\ntrain [epoch 5 batch 60]: : 1440it [00:26, 53.76it/s, mean train loss=0.118]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 111.75it/s, mean loss=0.332, macro f1=0.88]\n[I 2024-10-19 23:08:55,367] Trial 21 pruned. \n\n\nTrial 22 params: {'batch_size': 24, 'lr': 0.00020235892239526168, 'weight_decay': 0.0014620396847455305, 'reduce_lr_factor': 0.07059156074150905, 'momentum': 0.9479754424548245, 'early_stop_patience': 2, 'jitter': False, 'horizontal_flip': False, 'rotation': False}\nTrial 22 fold 1\n\n\ntrain [epoch 1 batch 60]: : 1440it [00:26, 53.67it/s, mean train loss=1.65]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 115.24it/s, mean loss=1.08, macro f1=0.426]\ntrain [epoch 2 batch 60]: : 1440it [00:26, 53.68it/s, mean train loss=0.784]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 109.39it/s, mean loss=0.585, macro f1=0.714]\ntrain [epoch 3 batch 60]: : 1440it [00:26, 53.90it/s, mean train loss=0.438]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.56it/s, mean loss=0.44, macro f1=0.795]\ntrain [epoch 4 batch 60]: : 1440it [00:26, 53.42it/s, mean train loss=0.282]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 114.37it/s, mean loss=0.357, macro f1=0.826]\ntrain [epoch 5 batch 60]: : 1440it [00:27, 52.72it/s, mean train loss=0.187]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 114.84it/s, mean loss=0.329, macro f1=0.854]\ntrain [epoch 6 batch 60]: : 1440it [00:26, 54.11it/s, mean train loss=0.169]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 112.83it/s, mean loss=0.339, macro f1=0.821]\ntrain [epoch 7 batch 60]: : 1440it [00:27, 53.19it/s, mean train loss=0.108]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 108.61it/s, mean loss=0.33, macro f1=0.856]\n[I 2024-10-19 23:12:30,487] Trial 22 pruned. \n\n\nTrial 23 params: {'batch_size': 24, 'lr': 0.0003998000959719629, 'weight_decay': 0.002486203660251997, 'reduce_lr_factor': 0.09072339594884746, 'momentum': 0.9750198623344721, 'early_stop_patience': 2, 'jitter': False, 'horizontal_flip': False, 'rotation': False}\nTrial 23 fold 1\n\n\ntrain [epoch 1 batch 60]: : 1440it [00:26, 53.64it/s, mean train loss=1.52]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 112.83it/s, mean loss=0.857, macro f1=0.588]\ntrain [epoch 2 batch 60]: : 1440it [00:26, 54.20it/s, mean train loss=0.574]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 107.73it/s, mean loss=0.463, macro f1=0.78]\ntrain [epoch 3 batch 60]: : 1440it [00:26, 53.88it/s, mean train loss=0.313]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 112.56it/s, mean loss=0.415, macro f1=0.81]\ntrain [epoch 4 batch 60]: : 1440it [00:26, 53.68it/s, mean train loss=0.208]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.20it/s, mean loss=0.258, macro f1=0.865]\ntrain [epoch 5 batch 60]: : 1440it [00:27, 52.81it/s, mean train loss=0.16]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 114.79it/s, mean loss=0.252, macro f1=0.858]\ntrain [epoch 6 batch 60]: : 1440it [00:26, 53.62it/s, mean train loss=0.134]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.26it/s, mean loss=0.243, macro f1=0.886]\ntrain [epoch 7 batch 60]: : 1440it [00:26, 54.31it/s, mean train loss=0.0773]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 105.07it/s, mean loss=0.255, macro f1=0.902]\ntrain [epoch 8 batch 60]: : 1440it [00:26, 53.65it/s, mean train loss=0.0419]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 111.02it/s, mean loss=0.228, macro f1=0.905]\n[I 2024-10-19 23:16:35,547] Trial 23 pruned. \n\n\nTrial 24 params: {'batch_size': 24, 'lr': 0.0004981905694438408, 'weight_decay': 0.0001860081403390097, 'reduce_lr_factor': 0.07923197031501877, 'momentum': 0.9554222873825267, 'early_stop_patience': 2, 'jitter': False, 'horizontal_flip': False, 'rotation': False}\nTrial 24 fold 1\n\n\ntrain [epoch 1 batch 60]: : 1440it [00:26, 53.89it/s, mean train loss=1.35]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 112.16it/s, mean loss=0.905, macro f1=0.632]\ntrain [epoch 2 batch 60]: : 1440it [00:26, 53.68it/s, mean train loss=0.572]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 111.72it/s, mean loss=0.599, macro f1=0.69]\ntrain [epoch 3 batch 60]: : 1440it [00:26, 54.52it/s, mean train loss=0.382]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 109.82it/s, mean loss=0.321, macro f1=0.841]\ntrain [epoch 4 batch 60]: : 1440it [00:26, 54.21it/s, mean train loss=0.229]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.89it/s, mean loss=0.315, macro f1=0.842]\ntrain [epoch 5 batch 60]: : 1440it [00:26, 54.47it/s, mean train loss=0.178]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.49it/s, mean loss=0.362, macro f1=0.85]\ntrain [epoch 6 batch 60]: : 1440it [00:26, 54.32it/s, mean train loss=0.0838]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 109.01it/s, mean loss=0.37, macro f1=0.849]\n\n\nEarly stopping at epoch 6\n\n\ntest [batch 38]: : 912it [00:08, 111.73it/s, mean loss=0.238, macro f1=0.9]\n\n\nTrial 24 fold 2\n\n\ntrain [epoch 1 batch 60]: : 1440it [00:26, 54.15it/s, mean train loss=1.39]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 112.44it/s, mean loss=0.633, macro f1=0.707]\ntrain [epoch 2 batch 60]: : 1440it [00:26, 54.41it/s, mean train loss=0.448]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 105.40it/s, mean loss=0.375, macro f1=0.808]\ntrain [epoch 3 batch 60]: : 1440it [00:26, 53.97it/s, mean train loss=0.26]\nvalidate [batch 15]: 100%|██████████| 360/360 [00:03&lt;00:00, 113.65it/s, mean loss=0.327, macro f1=0.826]\n[I 2024-10-19 23:21:19,223] Trial 24 pruned. \n\n\n\n\nvgg16_pretrained_study.best_params\n\n\n{'batch_size': 24,\n 'lr': 0.0004819093288876784,\n 'weight_decay': 0.0016937287816857739,\n 'reduce_lr_factor': 0.04514422541734965,\n 'momentum': 0.9669468784630878,\n 'early_stop_patience': 3,\n 'jitter': False,\n 'horizontal_flip': False,\n 'rotation': False}\n\n\n\nInterestingly, the pretrained VGG-16 model performed best without any data augmentation, like the MLP."
  },
  {
    "objectID": "notebooks/mlp-vgg16-fashion.html#training-and-evaluation",
    "href": "notebooks/mlp-vgg16-fashion.html#training-and-evaluation",
    "title": "A Comparison of Multilayer Perceptron and VGG-16 Models for Fashion Image Classification",
    "section": "Training and Evaluation",
    "text": "Training and Evaluation\nFinally, with the best set of hyperparameters determined for each model, we can perform the actual training and evaluation. I first define a helper function to display the evaluation results, including a classification report containing various metrics, a confusion matrix and a visualisation of some of the predictions.\n\ndef display_results(\n    results: EvaluationResults,\n    model: torch.nn.Module,\n    ds: torch.utils.data.Dataset,\n    transform: torchvision.transforms.v2.Transform,\n    device: str = DEVICE,\n):\n    print(results.classification_report_str)\n    print(f\"mean test loss: {results.mean_loss}\")\n    disp = sklearn.metrics.ConfusionMatrixDisplay(results.confusion_matrix, display_labels=target_le.classes_)\n    fig = plt.figure(figsize=(10, 10))\n    ax = fig.add_subplot()\n    disp.plot(ax=ax)\n    plt.show()\n\n    # Show predictions for some samples.\n    rows = 4\n    cols = 4\n    fig, axes = plt.subplots(rows, cols, figsize=(2 * cols, 2 * rows))\n    for idx in range(rows * cols):\n        img, label = ds[idx]\n        img_transformed = transform(img.detach().clone().to(device))\n\n        # Add an additional first dimension to simulate batching.\n        img_transformed = img_transformed[None, :, :, :]\n\n        label = target_le.inverse_transform([label])[0]\n\n        predicted_idx = model(img_transformed).argmax(dim=1).cpu()\n        predicted = target_le.inverse_transform(predicted_idx)[0]\n\n        ax = axes[idx // cols, idx % cols]\n        ax.imshow(img.permute(1, 2, 0))\n        ax.set_title(f\"Truth: {label}\\nPredicted: {predicted}\")\n    plt.tight_layout()\n    plt.show()\n\nFor keeping track of the training and validation losses using Tensorboard, we need to use PyTorch’s SummaryWriter. Training and validation losses are logged for each model using the train and validation loss hooks in the Trainer.\n\nwriter = SummaryWriter()\nwriter.add_custom_scalars({\n    \"Multilayer Perceptron\": {\n        \"Loss\": [\"Multiline\", [\"mlp/loss/train\", \"mlp/loss/validation\"]],\n    },\n    \"VGG16\": {\n        \"Loss\": [\"Multiline\", [\"vgg16/loss/train\", \"vgg16/loss/validation\"]],\n    },\n    \"VGG16 Pretrained\": {\n        \"Loss\": [\"Multiline\", [\"vgg16-pretrained/loss/train\", \"vgg16-pretrained/loss/validation\"]],\n    },\n})\n\nAs a quick recap, we use an undersampler to undersample the shoes and tees classes to mitigate the class imbalance problem. Training will also use early stopping based on the validation loss — if the validation loss stops improving after a certain number of epochs, the training will be stopped.\nEvaluation will use the following metrics / tools:\n\nAccuracy: The number of correct predictions out of the total number of predictions. Although accuracy provides a general indication of how well the model is performing, it can be misleading when there is a class imbalance (which is the case for our dataset).\nPrecision: The number of true positive predictions out of the total number of positive predictions. A high precision means the model is generally correct when it predicts a positive.\nRecall: The number of true positive predictions out of the total number of actual positive samples. A high recall means the model is able to identify most positive samples.\nF1 score: The harmonic mean of precision and recall. Can be interpreted as an average between precision and recall. F1 is especially useful when dealing with class imbalances since it combines both precision and recall.\nMacro average F1 score: The mean of the F1 scores for each class, useful for assessing overall model performance in multi-class classification tasks.\nConfusion matrix: A table that compares the actual target labels with the labels predicted by the model. From the confusion matrix, we can see what the model tends to misclassify samples as.\n\nNote: For each model, its architecture will be shown before the training and evaluation begin.\n\nMultilayer Perceptron\n\nmlp = mlp_from_params(mlp_study.best_params)\nprint(mlp)\n\nmlp_results = train_eval_generic_params(\n    model=mlp,\n    ds_train=ds_train,\n    ds_val=ds_val,\n    ds_test=ds_test,\n    sampler=undersampler,\n    params=mlp_study.best_params,\n    train_loss_hook=lambda loss, epoch: writer.add_scalar(\n        \"mlp/loss/train\", loss, epoch\n    ),\n    val_results_hook=lambda results, epoch: writer.add_scalar(\n        \"mlp/loss/validation\", results.mean_loss, epoch\n    ),\n)\n\n\nSequential(\n  (0): Flatten(start_dim=1, end_dim=-1)\n  (1): MultilayerPerceptron(\n    (layers): ModuleList(\n      (0): LazyLinear(in_features=0, out_features=256, bias=True)\n      (1): LazyBatchNorm1d(0, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU()\n      (3): LazyLinear(in_features=0, out_features=512, bias=True)\n      (4): LazyBatchNorm1d(0, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (5): ReLU()\n      (6): LazyLinear(in_features=0, out_features=256, bias=True)\n      (7): LazyBatchNorm1d(0, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (8): ReLU()\n      (9): LazyLinear(in_features=0, out_features=10, bias=True)\n    )\n  )\n)\n\n\ntrain [epoch 1 batch 85]: : 2720it [00:08, 337.37it/s, mean train loss=1.14]\nvalidate [batch 19]: : 608it [00:01, 305.52it/s, mean loss=0.953, macro f1=0.593]\ntrain [epoch 2 batch 85]: : 2720it [00:08, 329.19it/s, mean train loss=0.669]\nvalidate [batch 19]: : 608it [00:01, 307.83it/s, mean loss=0.743, macro f1=0.716]\ntrain [epoch 3 batch 85]: : 2720it [00:08, 339.25it/s, mean train loss=0.551]\nvalidate [batch 19]: : 608it [00:01, 307.82it/s, mean loss=0.652, macro f1=0.716]\ntrain [epoch 4 batch 85]: : 2720it [00:08, 337.62it/s, mean train loss=0.477]\nvalidate [batch 19]: : 608it [00:01, 305.81it/s, mean loss=0.672, macro f1=0.733]\ntrain [epoch 5 batch 85]: : 2720it [00:08, 328.63it/s, mean train loss=0.387]\nvalidate [batch 19]: : 608it [00:01, 310.40it/s, mean loss=0.662, macro f1=0.733]\n\n\nEarly stopping at epoch 5\n\n\ntest [batch 19]: : 608it [00:01, 308.16it/s, mean loss=0.573, macro f1=0.738]\n\n\n\n\ntorch.save(mlp.state_dict(), \"mlp_state.pt\")\n\n\ndisplay_results(\n    mlp_results,\n    mlp,\n    ds_test,\n    transforms_from_params(mlp_study.best_params)\n)\n\n\n              precision    recall  f1-score   support\n\n accessories       0.93      0.98      0.96        44\n     jackets       0.46      0.72      0.56        47\n       jeans       0.97      0.97      0.97        39\n    knitwear       0.24      0.27      0.25        41\n      shirts       0.52      0.45      0.48        49\n       shoes       0.99      0.98      0.99       148\n      shorts       0.92      0.97      0.94        34\n        tees       0.81      0.69      0.74       176\n\n    accuracy                           0.77       578\n   macro avg       0.73      0.75      0.74       578\nweighted avg       0.79      0.77      0.78       578\n\nmean test loss: 0.5728871100827267\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTrain loss vs. validation loss curve from Tensorboard (squared points are train loss; diamond points are validation loss):\n\n\n\ncurve-mlp.png\n\n\nThe train loss starts higher than the validation loss, though they both decrease. However, the validation loss seems to have a lower rate of decrease than the train loss — eventually, the train loss becomes lower than the validation loss. The MLP model seems to struggle to generalise to the validation set despite learning from the training data, which could either be a sign of overfitting or perhaps the model is too simple to adequately capture the underlying patterns. It may also be that the resizing of images to 64 by 64 removed too much information, so maybe more trials for hyperparameter tuning is needed.\nNonetheless, the performance metrics for the MLP model show strong results for some classes. For instance, categories like “shoes,” “accessories,” and “jeans” perform exceptionally well, with F1-scores of 99%, 96%, and 97% respectively. However, for classes like “knitwear” and “shirts”, the MLP model struggles, with F1-scores of 25% and 48% respectively. These results indicate both low precision and recall; i.e., the model often misclassifies these items and fails to identify them correctly. If we look at the confusion matrix, we see that the model appears to struggle with differentiating between knitwear, shirts and tees. For example, it misclassified 17 tees as jackets, 22 tees as knitwear and 14 tees as shirts. However, considering how similar these items can be, this is not unexpected, especially since MLPs do not account for the order of the pixels in the input images.\nThe accuracy of 77% indicates that the model’s overall performance is not bad. The macro average F1-score of 74% shows that the model’s performance across all classes is somewhat balanced. However, there is clearly still some room for improvement considering the misclassifications.\nFuture improvements should likely focus on improving the model’s ability to distinguish between knitwear, shirts and tees. Perhaps we could explore patch-based learning to allow the model to focus on the specific areas of the images (such as the sleeves).\n\n\nVGG-16\n\nvgg16 = VGG16(10)\nprint(vgg16)\n\nvgg16_results = train_eval_generic_params(\n    model=vgg16,\n    ds_train=ds_train,\n    ds_val=ds_val,\n    ds_test=ds_test,\n    sampler=undersampler,\n    params=vgg16_study.best_params,\n    train_loss_hook=lambda loss, epoch: writer.add_scalar(\n        \"vgg16/loss/train\", loss, epoch\n    ),\n    val_results_hook=lambda results, epoch: writer.add_scalar(\n        \"vgg16/loss/validation\", results.mean_loss, epoch\n    ),\n)\n\n\nVGG16(\n  (convs): Sequential(\n    (0): Sequential(\n      (0): LazyConv2d(0, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (1): LazyBatchNorm2d(0, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU(inplace=True)\n      (3): LazyConv2d(0, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (4): LazyBatchNorm2d(0, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (5): ReLU(inplace=True)\n      (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    )\n    (1): Sequential(\n      (0): LazyConv2d(0, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (1): LazyBatchNorm2d(0, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU(inplace=True)\n      (3): LazyConv2d(0, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (4): LazyBatchNorm2d(0, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (5): ReLU(inplace=True)\n      (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    )\n    (2): Sequential(\n      (0): LazyConv2d(0, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (1): LazyBatchNorm2d(0, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU(inplace=True)\n      (3): LazyConv2d(0, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (4): LazyBatchNorm2d(0, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (5): ReLU(inplace=True)\n      (6): LazyConv2d(0, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (7): LazyBatchNorm2d(0, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (8): ReLU(inplace=True)\n      (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    )\n    (3): Sequential(\n      (0): LazyConv2d(0, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (1): LazyBatchNorm2d(0, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU(inplace=True)\n      (3): LazyConv2d(0, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (4): LazyBatchNorm2d(0, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (5): ReLU(inplace=True)\n      (6): LazyConv2d(0, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (7): LazyBatchNorm2d(0, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (8): ReLU(inplace=True)\n      (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    )\n    (4): Sequential(\n      (0): LazyConv2d(0, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (1): LazyBatchNorm2d(0, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU(inplace=True)\n      (3): LazyConv2d(0, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (4): LazyBatchNorm2d(0, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (5): ReLU(inplace=True)\n      (6): LazyConv2d(0, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (7): LazyBatchNorm2d(0, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (8): ReLU(inplace=True)\n      (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    )\n  )\n  (avg_pool): Sequential(\n    (0): AdaptiveAvgPool2d(output_size=(7, 7))\n    (1): Flatten(start_dim=1, end_dim=-1)\n  )\n  (fcs): Sequential(\n    (0): LazyLinear(in_features=0, out_features=4096, bias=True)\n    (1): ReLU(inplace=True)\n    (2): Dropout(p=0.5, inplace=False)\n    (3): LazyLinear(in_features=0, out_features=4096, bias=True)\n    (4): ReLU(inplace=True)\n    (5): Dropout(p=0.5, inplace=False)\n    (6): LazyLinear(in_features=0, out_features=10, bias=True)\n  )\n)\n\n\ntrain [epoch 1 batch 113]: : 2712it [00:16, 159.73it/s, mean train loss=1.41]\nvalidate [batch 25]: : 600it [00:02, 255.52it/s, mean loss=0.925, macro f1=0.445]\ntrain [epoch 2 batch 113]: : 2712it [00:16, 161.91it/s, mean train loss=0.83]\nvalidate [batch 25]: : 600it [00:02, 236.30it/s, mean loss=0.595, macro f1=0.693]\ntrain [epoch 3 batch 113]: : 2712it [00:16, 162.11it/s, mean train loss=0.569]\nvalidate [batch 25]: : 600it [00:02, 257.69it/s, mean loss=0.574, macro f1=0.692]\ntrain [epoch 4 batch 113]: : 2712it [00:16, 160.68it/s, mean train loss=0.442]\nvalidate [batch 25]: : 600it [00:02, 250.62it/s, mean loss=0.326, macro f1=0.834]\ntrain [epoch 5 batch 113]: : 2712it [00:16, 161.85it/s, mean train loss=0.368]\nvalidate [batch 25]: : 600it [00:02, 255.54it/s, mean loss=0.405, macro f1=0.824]\ntrain [epoch 6 batch 113]: : 2712it [00:16, 161.04it/s, mean train loss=0.331]\nvalidate [batch 25]: : 600it [00:02, 258.74it/s, mean loss=0.41, macro f1=0.848]\n\n\nEarly stopping at epoch 6\n\n\ntest [batch 25]: : 600it [00:02, 240.97it/s, mean loss=0.529, macro f1=0.814]\n\n\n\n\ntorch.save(vgg16.state_dict(), \"vgg16_state.pt\")\n\n\ndisplay_results(\n    vgg16_results,\n    vgg16,\n    ds_test,\n    transforms_from_params(vgg16_study.best_params)\n)\n\n\n              precision    recall  f1-score   support\n\n accessories       0.91      0.98      0.95        44\n     jackets       0.68      0.96      0.80        47\n       jeans       0.97      1.00      0.99        39\n    knitwear       0.42      0.76      0.54        41\n      shirts       0.53      0.41      0.46        49\n       shoes       0.99      0.97      0.98       148\n      shorts       0.97      1.00      0.99        34\n        tees       0.94      0.72      0.82       176\n\n    accuracy                           0.83       578\n   macro avg       0.80      0.85      0.81       578\nweighted avg       0.86      0.83      0.84       578\n\nmean test loss: 0.5294404172897339\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTrain loss vs. validation loss curve from Tensorboard (squared points are train loss; diamond points are validation loss):\n\n\n\ncurve-vgg16.png\n\n\nThe curve for the VGG-16 model looks good — both train and validation losses decrease and eventually converge. There does not seem to be any sign of overfitting as the validation loss does not increase to become higher than the train loss.\nThe custom VGG-16 model achieved an overall accuracy of 83% and a macro average F1-score of 81%. This indicates generally balanced performance across classes. However, this is worse than I expected since the VGG-16 architecture is deep and should be able to handle fairly complex patterns. High-performing classes include “accessories”, “jeans”, “shoes”, and “shorts”, with F1-scores above 95%.\nThe model struggles with “knitwear” and “shirts,” showing F1-scores of 54% and 46% respectively. Looking at the confusion matrix, the model seems to have issues distinguishing between knitwear, shirts and tees — similar to the MLP model, but to a lesser extent. “Shirts”, in particular, has both low precision (53%) and recall (41%). Like the MLP, if we want to improve the model’s performance, we should focus on distinguishing between these classes.\n\n\nVGG-16 Pretrained\n\nvgg16_pretrained = make_vgg16_pretrained()\nprint(vgg16_pretrained)\n\nvgg16_pretrained_results = train_eval_generic_params(\n    model=vgg16_pretrained,\n    ds_train=ds_train,\n    ds_val=ds_val,\n    ds_test=ds_test,\n    sampler=undersampler,\n    params=vgg16_pretrained_study.best_params,\n    train_loss_hook=lambda loss, epoch: writer.add_scalar(\n        \"vgg16-pretrained/loss/train\", loss, epoch\n    ),\n    val_results_hook=lambda results, epoch: writer.add_scalar(\n        \"vgg16-pretrained/loss/validation\", results.mean_loss, epoch\n    ),\n    transforms_override=torchvision.models.VGG16_Weights.IMAGENET1K_V1.transforms(),\n)\n\n\nVGG(\n  (features): Sequential(\n    (0): LazyConv2d(0, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU(inplace=True)\n    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (3): ReLU(inplace=True)\n    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (6): ReLU(inplace=True)\n    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (8): ReLU(inplace=True)\n    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (11): ReLU(inplace=True)\n    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (13): ReLU(inplace=True)\n    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (15): ReLU(inplace=True)\n    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (18): ReLU(inplace=True)\n    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (20): ReLU(inplace=True)\n    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (22): ReLU(inplace=True)\n    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (25): ReLU(inplace=True)\n    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (27): ReLU(inplace=True)\n    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (29): ReLU(inplace=True)\n    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n  (classifier): Sequential(\n    (0): Linear(in_features=25088, out_features=4096, bias=True)\n    (1): ReLU(inplace=True)\n    (2): Dropout(p=0.5, inplace=False)\n    (3): Linear(in_features=4096, out_features=4096, bias=True)\n    (4): ReLU(inplace=True)\n    (5): Dropout(p=0.5, inplace=False)\n    (6): LazyLinear(in_features=0, out_features=10, bias=True)\n  )\n)\n\n\ntrain [epoch 1 batch 113]: : 2712it [00:50, 54.02it/s, mean train loss=1.1]\nvalidate [batch 25]: : 600it [00:04, 120.19it/s, mean loss=0.418, macro f1=0.84]\ntrain [epoch 2 batch 113]: : 2712it [00:50, 54.01it/s, mean train loss=0.299]\nvalidate [batch 25]: : 600it [00:04, 122.14it/s, mean loss=0.307, macro f1=0.898]\ntrain [epoch 3 batch 113]: : 2712it [00:51, 53.10it/s, mean train loss=0.193]\nvalidate [batch 25]: : 600it [00:04, 120.86it/s, mean loss=0.172, macro f1=0.911]\ntrain [epoch 4 batch 113]: : 2712it [00:50, 53.79it/s, mean train loss=0.116]\nvalidate [batch 25]: : 600it [00:04, 121.77it/s, mean loss=0.176, macro f1=0.92]\ntrain [epoch 5 batch 113]: : 2712it [00:51, 53.13it/s, mean train loss=0.0693]\nvalidate [batch 25]: : 600it [00:05, 118.23it/s, mean loss=0.167, macro f1=0.937]\ntrain [epoch 6 batch 113]: : 2712it [00:50, 53.73it/s, mean train loss=0.0641]\nvalidate [batch 25]: : 600it [00:04, 120.54it/s, mean loss=0.138, macro f1=0.927]\ntrain [epoch 7 batch 113]: : 2712it [00:50, 53.66it/s, mean train loss=0.0411]\nvalidate [batch 25]: : 600it [00:04, 121.00it/s, mean loss=0.153, macro f1=0.93]\ntrain [epoch 8 batch 113]: : 2712it [00:51, 53.03it/s, mean train loss=0.026]\nvalidate [batch 25]: : 600it [00:04, 120.74it/s, mean loss=0.244, macro f1=0.926]\ntrain [epoch 9 batch 113]: : 2712it [00:50, 53.63it/s, mean train loss=0.029]\nvalidate [batch 25]: : 600it [00:04, 120.19it/s, mean loss=0.19, macro f1=0.933]\n\n\nEarly stopping at epoch 9\n\n\ntest [batch 25]: : 600it [00:04, 120.08it/s, mean loss=0.203, macro f1=0.931]\n\n\n\n\ntorch.save(vgg16_pretrained.state_dict(), \"vgg16_pretrained_state.pt\")\n\n\ndisplay_results(\n    vgg16_pretrained_results,\n    vgg16_pretrained,\n    ds_test,\n    torchvision.models.VGG16_Weights.IMAGENET1K_V1.transforms(),\n)\n\n\n              precision    recall  f1-score   support\n\n accessories       1.00      0.98      0.99        44\n     jackets       0.90      0.91      0.91        47\n       jeans       1.00      1.00      1.00        39\n    knitwear       0.76      0.68      0.72        41\n      shirts       0.88      0.94      0.91        49\n       shoes       0.99      1.00      1.00       148\n      shorts       0.97      1.00      0.99        34\n        tees       0.95      0.94      0.95       176\n\n    accuracy                           0.95       578\n   macro avg       0.93      0.93      0.93       578\nweighted avg       0.95      0.95      0.95       578\n\nmean test loss: 0.20280604993458837\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTrain loss vs. validation loss curve from Tensorboard (squared points are train loss; diamond points are validation loss):\n\n\n\ncurve-vgg16-pretrained.png\n\n\nUnfortunately, the pretrained VGG-16 model seems to have slightly overfitted to the training data as the validation loss starts to increase slightly. However, the training was stopped before it overfitted further. Interestingly, the validation loss decreases at a much slower rate than that of the custom VGG-16 model. This is likely because the pretrained model has already learned common patterns from its previous training — indeed, the validation loss was already nearly 0.4 after the first epoch, unlike the custom VGG-16 model, whose validation loss was around 0.9 after the first epoch.\nThe fine-tuned pretrained VGG-16 model achieved an impressive overall accuracy of 95% and a macro average F1-score of 93%. Classes such as “accessories,” “jeans,” “shoes,” and “shorts” had near-perfect performance, with F1-scores around 99% or 100%. “Tees” and “jackets” also performed well, both achieving F1-scores above 90%, indicating strong precision and recall.\nHowever, “knitwear” was the lowest-performing category, with an F1-score of 72%. Looking at the confusion matrix, the model seems to sometimes misclassify knitwear as either jackets or tees. Similarly, the model sometimes misclassifies tees as knitwear. Despite this, the model shows excellent results overall with little variance in precision and recall between classes.\n\n\nTensorboard\nThe following cell runs Tensorboard with the train and validation loss data within the notebook. Note that this will not work on Kaggle (but it does work on Google Colab) — if you are using Kaggle, you will have to download the logs and run Tensorboard locally. For convenience, the plots have already been included in this notebook as images in the previous sections.\n\n%load_ext tensorboard\n%tensorboard --logdir runs"
  },
  {
    "objectID": "notebooks/mlp-vgg16-fashion.html#conclusion",
    "href": "notebooks/mlp-vgg16-fashion.html#conclusion",
    "title": "A Comparison of Multilayer Perceptron and VGG-16 Models for Fashion Image Classification",
    "section": "Conclusion",
    "text": "Conclusion\nThe pretrained VGG-16 model achieved the best performance out of the three models, with an overall accuracy of 95% and a macro average F1-score of 93%. The custom VGG-16 model had decent performance but falls behind, with an overall accuracy of 83% and a macro average F1-score of 81%. This result is expected, however, since training a model from scratch generally requires more extensive tuning and data to achieve optimal results than tuning a pre-trained model, which would already have learned common patterns from its previous training.\nLastly, the MLP model had an overall accuracy of 77% and a macro average F1-score of 74%. Since MLPs are unable to understand the spacial structure of images (they essentially ignore the order of pixels), the fact that the MLP performed the worst is not surprising.\nAll models seem to struggle with differentiating certain classes (to different extents), especially knitwear, shirts and tees. Considering how similar images in these classes can be, though, the confusion is not unexpected.\nFor future work, there is clearly room for improvement for the underperforming classes. Although hyperparameter tuning was done, there were still many hyperparameters left unexplored (e.g., use of dropout and regularisation, additional data augmentation transformations). Furthermore, only 25 trials were used for hyperparameter tuning — increasing the number of trials may lead to better hyperparameters that improve model performance. It may also be worth exploring cost-sentitive learning by assigning higher penalties to misclassifications, especially for the underperforming classes."
  }
]